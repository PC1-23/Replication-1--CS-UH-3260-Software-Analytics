repo,number,url,title,body,merged,merged_by,author,keywords,comments,commits
grpc/grpc-java,12587,https://github.com/grpc/grpc-java/pull/12587,opentelemetry: Add target attribute filter for metrics,"Introduce an optional Predicate<String> targetAttributeFilter to control how grpc.target is recorded in OpenTelemetry client metrics.

When a filter is provided, targets rejected by the predicate are normalized to ""other"" to reduce grpc.target metric cardinality, while accepted targets are recorded as-is. If no filter is set, existing behavior is preserved.

This change adds a new Builder API on GrpcOpenTelemetry to allow applications to configure the filter. Tests verify both the Builder wiring and the target normalization behavior.

This is an optional API; annotation (e.g., experimental) can be added per maintainer guidance.

Refs #12322
Related: gRFC A109 – Target Attribute Filter for OpenTelemetry Metrics
https://github.com/grpc/proposal/pull/528
",2026-01-09T14:16:41Z,kannanjgithub,becomeStar,"readability, readable","Thanks for looping me in. I'll keep an eye on the Android desugaring discussion.
In the meantime, I've updated the PR and gRFC A109 with a null-safety check for the filtering logic to make it more robust.
If it turns out that using Java 8's Predicate causes compatibility concerns, I'm happy to adjust the interface accordingly (e.g., using a small custom functional interface), based on what the maintainers think is best. | /gcbrun | /gcbrun","opentelemetry: Add optional targetAttributeFilter to module

Introduce an optional Predicate<String> targetAttributeFilter in the
OpenTelemetryMetricsModule to control how grpc.target is recorded in
metrics. Also add support in GrpcOpenTelemetry.Builder to allow users
to configure this filter when building the module. | opentelemetry: Add Javadoc for target attribute filter | opentelemetry: Simplify ClientCall initialization in tests | opentelemetry: Rerun flaky test | opentelemetry: Rerun flaky test | opentelemetry: Add null safety to target filtering logic | opentelemetry: Use internal interface for target filter

Replace Predicate with a package-private TargetFilter interface in
fields to ensure compatibility with Android API levels < 24. This
prevents potential runtime issues for Android users who do not use
the filtering feature.

Mark the API as experimental and link the tracking issue. | opentelemetry: Avoid method reference for better Android compatibility | opentelemetry: Add @IgnoreJRERequirement for Android CI compatibility | opentelemetry: Rerun ci test | opentelemetry: Replace anonymous classes with lambdas/method references for TargetFilter"
grpc/grpc-java,11220,https://github.com/grpc/grpc-java/pull/11220,binder: Use the builder pattern to replace BinderClientTransport's long ctor arg list,Centralizes defaults as well. This will make it so much easier to add new channel parameters.,2024-05-23T18:50:37Z,jdcormie,jdcormie,"readability, readable",,No more long argument lists! | commit | changes | Merge branch 'grpc:master' into builder-ctor-arg | changes | Merge branch 'builder-ctor-arg' of https://github.com/jdcormie/grpc-java into builder-ctor-arg | changes | copyright | stash offload executor at build() time instead
grpc/grpc-java,10610,https://github.com/grpc/grpc-java/pull/10610,Make Ring Hash LB a petiole policy,"Implementation of A61 for Ring Hash

Major changes to behavior:
-   Lazy create PickFirstLoadBalancers rather than lazily connect subchannels
-   When picking, look for first entry in ring not in TF rather than focus on first 2 entries in the ring after hash",2023-11-09T21:46:52Z,larry-safran,larry-safran,"readability, readable",We need to figure out what is going on with your rebase process. The rebase also caused weirdness with the commit message.,"Change Round Robin and WeightedRoundRobin into petiole policies | Responded to a number of the code review comments. | suppress bogus warning. | Address code review comments improving reuse and avoiding creating children just to immediately delete them. | Checkpoint LeastRequestLoadBALANCERTEST | Complete LeastRequestLoadBalancer | revert changes to PickFirst that were made to expose status that is no longer needed | Checkpoint | Complete fixing implementation and tests so that all tests pass as expected. | Update picker logic per A61 that it no longer pays attention to the first 2 elements, but rather takes the first ring element not in TF and uses that.

Also, added some comments, fixed a couple of bugs in MultiChildLoadBalancer and replaced SubchannelView with direct usage of RingHashChildLbState. | Fix style issues. | Cleanup after rebasing. | Go back to caching the connectivity state when the RingHash picker is created and respond to some code review comments. | Change disable to not do shutdown and then have acceptResolvedAddresses explicitly do the shutdowns after updating picker by default | Add tests to make sure that removed addresses have their subchannels shutdown in acceptResolvedAddresses. | Per code review, split the updating children from updating balancing state and removed child cleanup in acceptResolvedAddresses. | Per code review, split the updating balancing state from the removed child cleanup when processing acceptResolvedAddresses. | Address code review issues.  Mostly simplifications.  Changed to not try to start a connection in updateBalancingState when there aren't any active subchannels. | Complete comment | Complete comment | Change rules for reporting overall state so that if nothing is ready and there are 2+ TF then report TF.  1 TF + idle/connecting report connecting. | Address checkstyle violations | Fix rebase problems | android: Remove unneeded proguard rule

The methods other than forTarget() were no longer used after 493af030.
forTarget() was no longer used after cda0e9d9. | stub: Deprecate StreamObservers (#10654)

This class is of questionable utility and generally not used. | Do nothing change to try to get builds to pull in later master"
grpc/grpc-java,11886,https://github.com/grpc/grpc-java/pull/11886,xds: improve code readability of server FilterChain parsing,"- Improve code flow and variable names
- Reduce nesting
- Add comments between logical blocks
- Add comments explaining some xDS/gRPC nuances",2025-02-11T01:14:07Z,sergiitk,sergiitk,"readability, readable",,"xds: improve code readability of server FilterChain parsing

- Improve code flow and variable names
- Reduce nesting
- Add comments between logical blocks
- Add comments explaining some xDS/gRPC nuances | Address the feedback: variable names; generate fc name from idx"
grpc/grpc-java,11962,https://github.com/grpc/grpc-java/pull/11962,replace ExpectedException in grpc-api and grpc-core,Related to #7467,2025-03-21T07:30:25Z,kannanjgithub,panchenko,"readability, readable",,replace ExpectedException in grpc-api | replace ExpectedException in grpc-core | style | CR | CR | CR | CR | CR | CR | CR | Merge branch 'master' into ExpectedException-api-core | CR
grpc/grpc-java,11992,https://github.com/grpc/grpc-java/pull/11992,xds: float LRU cache across interceptors,,2025-04-17T01:56:40Z,shivaspeaks,shivaspeaks,"readability, readable, understandable",,Float LRU Cache across interceptors | respect new cache_size | respect new cache_size | respect new cache_size | respect new cache_size | respect new cache_size | respect new cache_size | Address comment | Address comment | address suggestions
grpc/grpc-java,11867,https://github.com/grpc/grpc-java/pull/11867,xds: Improve XdsNR's selectConfig() variable handling,"The variables from the do-while are no longer initialized to let the compiler verify that the loop sets each. Unnecessary comparisons to null are also removed and is more obvious as the variables are never set to null. Added a minor optimization of computing the RPCs path once instead of once for each route. The variable declarations were also sorted to match their initialization order.

This does fix an unlikely bug where if the old code could successfully matched a route but fail to retain the cluster, then when trying a second time if the route was _not_ matched it would re-use the prior route and thus infinite-loop failing to retain that same cluster.

CC @larry-safran ",2025-02-05T18:37:22Z,ejona86,ejona86,"readability, readable",,"xds: Improve XdsNR's selectConfig() variable handling

The variables from the do-while are no longer initialized to let the
compiler verify that the loop sets each. Unnecessary comparisons to null
are also removed and is more obvious as the variables are never set to
null. Added a minor optimization of computing the RPCs path once instead
of once for each route. The variable declarations were also sorted to
match their initialization order.

This does fix an unlikely bug where if the old code could successfully
matched a route but fail to retain the cluster, then when trying a
second time if the route was _not_ matched it would re-use the prior route
and thus infinite-loop failing to retain that same cluster. | Verify weight is not negative | Add comments, s/action/routeAction/ | Add newline after retainCluster loop"
grpc/grpc-java,11520,https://github.com/grpc/grpc-java/pull/11520,Change PickFirstLeafLoadBalancer to only have 1 subchannel at a time,"Hide behind environment variable GRPC_SERIALIZE_RETRIES

This is for testing with GMS core to see if they use the new PF with only 1 subchannel at a time trying to reconnect whether that eliminates their 6% data increase.",2024-10-03T00:03:47Z,larry-safran,larry-safran,"readability, readable",Ready for re-review,"Change PickFirstLeafLoadBalancer to only have 1 subchannel at a time if environment variable GRPC_SERIALIZE_RETRIES == true.

Cache serializingRetries value so that it doesn't have to look up the flag every time.

Clear the correct task when READY in processSubchannelState and move the logic to cancelScheduledTasks

Cleanup based on PR review

remove unneeded checks for shutdown. | Fix previously broken tests | Fix previously broken tests | Shutdown previous subchannel when run off end of index. | Provide option to disable subchannel retries to let PFLeafLB take control of retries. | InternalSubchannel internally goes to IDLE when sees TF when reconnect is disabled.
Remove an extra index.increment in LeafLB
Fix spelling, remove unneeded additions | Make errorprone happy | Make checkstyle happy | Fix the other spelling mistake | Cleanup | Remove unneeded optimization"
grpc/grpc-java,11661,https://github.com/grpc/grpc-java/pull/11661,xds: Add counter and gauge metrics ,"This PR implements xDS client defined in [A78](https://github.com/grpc/proposal/blob/master/A78-grpc-metrics-wrr-pf-xds.md#xdsclient).

Counters
- grpc.xds_client.server_failure
- grpc.xds_client.resource_updates_valid
- grpc.xds_client.resource_updates_invalid

Gauges
- grpc.xds_client.connected
- grpc.xds_client.resources

The `grpc.xds.authority` label is missing, and will be added in a later PR.",2024-11-26T00:47:32Z,DNVindhya,DNVindhya,"readability, readable, easier to read","@ejona86 and @larry-safran thanks for both of your approvals. I have addressed all your comments. If you don't have any new/pending comments can I go ahead and merge? | Once you have approvals from everyone, you can go ahead a merge.
…
On Mon, Nov 25, 2024 at 3:06 PM Vindhya Ningegowda ***@***.***> wrote:
 @ejona86 <https://github.com/ejona86> and @larry-safran
 <https://github.com/larry-safran> thanks for both of your approvals. I
 have addressed all your comments. If you don't have any new/pending
 comments can I go ahead and merge?

 —
 Reply to this email directly, view it on GitHub
 <#11661 (comment)>,
 or unsubscribe
 <https://github.com/notifications/unsubscribe-auth/AZQMCXTO7DVWXV7XYA4NHNT2COUQFAVCNFSM6AAAAABRBI57B6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDIOJZGIYTINBWGA>
 .
 You are receiving this because you were mentioned.Message ID:
 ***@***.***>","Rebase master | Rebase #2 | fix javadoc | Instead of storing MetricRecorder, pass it as is for XdsClient creation | Move interfaces for reporting gauge values to XdsClient and update unit tests | Updated `ServerConnectionCallback#reportServerConnectionGauge` method signature and addressed review comments. | Inject  value from XdsClientMetricReporterImpl and addressed review comments | Moved resource count metric reporting from XdsClientImpl to XdsClientMetricReporterImpl and addressed review comments | fix typo | address review comment for javadoc | Updated verifyResourceMetadataNacked method to verify ResourceMetadata.cached value | Return Future instead of SettableFuture for reportServerConnections() and addressed review comments | Added unit test to verify ResourceMetadata.isCached() set to false on Nacked resource"
grpc/grpc-java,10318,https://github.com/grpc/grpc-java/pull/10318,Bidi Blocking Stub,Created a BlockingClientCall class that does blocking streams for all 3 streaming types.,2024-12-21T00:16:17Z,larry-safran,larry-safran,"readability, readable, easier to read","FWIW, I do have a really hard time understanding what the state of the code was when I last reviewed, so that I can see the changes. | Sorry about making such a mess with the rebase.  I'll wait until you're ready to approve before trying to rebase again. | Sorry about making such a mess with the rebase. I'll wait until you're ready to approve before trying to rebase again.

It was less that you did the rebase and more that I can't tell where I should start reading. If you had squashed all the commits before the rebase and fixes were on top, I'd have actually been fine. Right now the first commit is ""Fix a bunch of things"" so I get lost immediately.
I see now that if I keep looking at commits, they make sense again, so the first commit just had the wrong message. But also the commits are repeated a second time. Looks like it was just a bad rebase. | I did do a squash before the rebase, that's why I called it ""Fix a bunch of
things"".  I see that adding ""Squashed"" would have been a good idea.
…
On Fri, Oct 18, 2024 at 3:09 PM Eric Anderson ***@***.***> wrote:
 Sorry about making such a mess with the rebase. I'll wait until you're
 ready to approve before trying to rebase again.

 It was less that you did the rebase and more that I can't tell where I
 should start reading. If you had squashed all the commits before the rebase
 and fixes were on top, I'd have actually been fine. Right now the first
 commit is ""Fix a bunch of things"" so I get lost immediately.

 I see now that if I keep looking at commits, they make sense again, so the
 first commit just had the wrong message. But also the commits are repeated
 a second time. Looks like it was just a bad rebase.

 —
 Reply to this email directly, view it on GitHub
 <#10318 (comment)>,
 or unsubscribe
 <https://github.com/notifications/unsubscribe-auth/AZQMCXQZKNRYNUBD3ZEHN2TZ4GBJPAVCNFSM6AAAAAAZWJRQKWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDIMRTGMYDONBRGY>
 .
 You are receiving this because you commented.Message ID:
 ***@***.***> | that's why I called it ""Fix a bunch of
things"".  I see that adding ""Squashed"" would have been a good idea.

A better name would have been ""Bidi Blocking Stub"" or similar, since it had all the implementation in it.","Fix a bunch of things | Restore things messed up by rebase | Fixed hang from not getting interrupted.
Added some 'final' to class definitions. | Generated service with blocking stream methods. | Reverse order of operations in BlockingClientCall.cancel to improve test reliability. | Revert ""Generated service with blocking stream methods.""

This reverts commit 3c63771a6de1e34e70015c86f5a644c19b762d6f. | Add ExperimentalApi to new blocking BlockingClientCall generated methods | Remove blocking from the creating a V2 ServerStreaming RPC which eliminates it throwing an exception. | android interop-testing builds | Switch from using jdk Predicate to the one in Guava | Switch method for Predicate from `test()` to `apply()`"
grpc/grpc-java,11575,https://github.com/grpc/grpc-java/pull/11575,core: SpiffeUtil API for extracting Spiffe URI and loading TrustBundles,"Additional API for SpiffeUtil:

- extract Spiffe URI from certificate chain
- load Spiffe Trust Bundle from filesystem [json spec](https://github.com/spiffe/spiffe/blob/main/standards/SPIFFE_Trust_Domain_and_Bundle.md) [JWK spec](https://github.com/spiffe/spiffe/blob/main/standards/X509-SVID.md#61-publishing-spiffe-bundle-elements)",2024-10-17T18:11:07Z,ejona86,erm-g,"readability, readable, clarity, easier to read",,"Enhance for JsonParser, API for loading trust bundle from file and extraction of SPIFFE uri from leaf certs | Style fix | Address PR comments | Change format of certs in x5c field | Style fix | Change skipping elements to invalidating the bundle in case of exceptions | Style fix | Address PR comments | Assert instead of checkArgument | Win test debug | Win test debug | Win test debug | Win test fix | Address PR comments | Change JsonParser to not allow duplicates | Style fix | Style fix fpr OrphanedFormatString | Address PR comments + extra test for class cast exceptions (wrong types in json) | Copy test files into temp dir | style fix | Change sequnce number json name to 'spiffe_sequence' + address PR comments | unused import"
grpc/grpc-java,11068,https://github.com/grpc/grpc-java/pull/11068,cronet : Update library + Move to StableAPIs in Cronet,"In this Pull Request
1. Cronet is updated to `119.6045.31` from `108.5359.79`, In which Experimental API's are deprecated and moved to Stable Interfaces.
2. grpc-cronet Moved Experimental API's to Stable Interfaces
3. Removed use of reflection as Experimental API's are no longer in use (which are introdced in #6111)

Minor Changes
1. Updated JavaDoc : Android Permission from [`MODIFY_NETWORK_ACCOUNTING` (deprecated)](https://android.googlesource.com/platform/frameworks/base/+/master/core/res/AndroidManifest.xml#6416) to `UPDATE_DEVICE_STATS`
2. Update to JAVA-8 API's to improve code readability
3. Moved from `Charset.forName(""UTF-8"")` to `StandardCharsets.UTF_8` Standard

Fixes #10396 (@sergiitk)",2024-04-22T16:54:49Z,temawi,Ashok-Varma,"readability, readable, understandable","The committers listed above are authorized under a signed CLA.✅ login: Ashok-Varma / name: Ashok Varma  (cb2e1af, 42ff5a4, e728136, ed457b5) | Hi @sergiitk / @ejona86  / @temawi,
I've updated changes in to different commits. It's open for review | Hi @sergiitk,
I was just wondering on Linux and Macos artifacts taking time (those are stalled for ~10 days), does it signify any problem in the builds ? or are we supposed to generate and add them ? | I was just wondering on Linux and Macos artifacts taking time (those are stalled for ~10 days), does it signify any problem in the builds ?

It's not a problem. We have to manually start those builds by adding that kokoro label. | This looks fine to me, but I'm not sure if enough Android devices have the stable API. We may need to fall-back to the experimental API to reach more devices. @temawi, would you be able to investigate that?

I agree to this, Adding little-bit of background :

ExperimentalAPI deprecation done on Oct 27, 2022
Any Custom Implementation of Cronet (if API is not yet updated), Needs to update the API +move the experimental implementations to CronetEngine.
All Googles cronet implementations (embedded, fallback, play-services) are compatible with new API. | This looks fine to me, but I'm not sure if enough Android devices have the stable API. We may need to fall-back to the experimental API to reach more devices. @temawi, would you be able to investigate that?

I agree to this, Adding little-bit of background :

ExperimentalAPI deprecation done on Oct 27, 2022
Any Custom Implementation of Cronet (if API is not yet updated), Needs to update the API +move the experimental implementations to CronetEngine.
All Googles cronet implementations (embedded, fallback, play-services) are compatible with new API.


I also checked with the Cronet team - this should be a safe change to make. | @temawi, given the commits are split out, we'll probably want to rebase this instead of squashing (ideally the commits would have cronet: prefixes to make scanning easier, but this is fine).

Updated commits with cronet: prefix | Thank you @temawi  and @ejona86  for the help on PR.
Any plans on next release date, What's the release cadence for gRPC ? | @Ashok-Varma, see the milestones. Minor releases are scheduled every six weeks.",cronet: Update Cronet to latest release + Move to Stable Cronet APIs. | cronet: @javadoc update android permission MODIFY_NETWORK_ACCOUNTING (deprecated) => UPDATE_DEVICE_STATS | cronet: Update to Java-8 API's and tighten the scopes | cronet: Update to StandardCharsets and assertNotNull API's
grpc/grpc-java,11216,https://github.com/grpc/grpc-java/pull/11216,util: Stabilize AdvancedTlsX509TrustManager,"This PR is a part of 'Stabilize Advanced TLS' effort.
Clean up, improve javadoc, de-experimentalize of AdvancedTlsX509TrustManager, add a unit test (e2e already exists).",2024-07-11T16:11:49Z,ejona86,erm-g,"readability, readable","API review meeting notes:

CERTIFICATE_ONLY_VERIFICATION and INSECURELY_SKIP_ALL_VERIFICATION

Should document that these are dangerous to us, unless also specifying your own cert validation. Yes, one has INSECURELY in its name, but there should be some javadoc.

INSECURELY_SKIP_ALL_VERIFICATION

Mention that any loaded trust certs will be ignored. Yes, that's what it says, but just ""those other methods that you use all the time stop doing anything"" is helpful to point out.

Add docs to say that after build, no trust certs are loaded. You need to call one of the update/usesystem methods.
Builder needs more docs. For example, to explain that setSslSocketAndEnginePeerVerifier is called in addition to verifying certs as specified by Verification.
Should make clear that it is normal for nothing in the builder needs to be called? That should be the common case. Maybe just improve the AdvancedTlsX509TrustManager javadoc with a code snippet?


The minimum refresh period of 1 minute is enforced.

""enforced"" could mean several different things, including ""causes an error."" Probably want to tweak that to be more clear. | API review meeting notes:

CERTIFICATE_ONLY_VERIFICATION and INSECURELY_SKIP_ALL_VERIFICATION

Should document that these are dangerous to us, unless also specifying your own cert validation. Yes, one has INSECURELY in its name, but there should be some javadoc.

INSECURELY_SKIP_ALL_VERIFICATION

Mention that any loaded trust certs will be ignored. Yes, that's what it says, but just ""those other methods that you use all the time stop doing anything"" is helpful to point out.

Add docs to say that after build, no trust certs are loaded. You need to call one of the update/usesystem methods.
Builder needs more docs. For example, to explain that setSslSocketAndEnginePeerVerifier is called in addition to verifying certs as specified by Verification.
Should make clear that it is normal for nothing in the builder needs to be called? That should be the common case. Maybe just improve the AdvancedTlsX509TrustManager javadoc with a code snippet?


The minimum refresh period of 1 minute is enforced.

""enforced"" could mean several different things, including ""causes an error."" Probably want to tweak that to be more clear.

Done - I also reworded few comments before 'bumping up' to javadoc level. PTAL | @ejona86 I applied the changes we discussed - PTAL",Stabilization of API and unit testing | unused import | restructure of javadoc | grammar fix | fixed <p> tags | fixed <p> tags | Adjust style to KeyManager | Adjust style to KeyManager | FakeClock migration | Delete TearDown | Delete unused imports | Improved javadoc | Improved javadoc | Improved javadoc | Improved javadoc | Improved javadoc | Changed log to be compatible with Android < 26 | Address PR comments | Delete unused imports | extra test for default delegateManager | extra test for default delegateManager | style fix | Address PR comment | unused import | Address PR comments | Merge branch 'master' into TrustManager | Javadoc formatting | Merge remote-tracking branch 'origin/TrustManager' into TrustManager
grpc/grpc-java,11113,https://github.com/grpc/grpc-java/pull/11113,s2a: Add gRPC S2A,"Add S2A Java client to gRPC Java.

Context: https://github.com/google/s2a-go/blob/main/README.md",2024-09-14T00:11:17Z,larry-safran,rmehta19,"readability, readable, easier to read","The committers listed above are authorized under a signed CLA.✅ login: rmehta19 / name: Riya Mehta  (739ee23, b35f145, 46691df, 2dd1c7e, 72630d8, 217a3e4, 44fe552, f94cc10, 08f8342, f47c560, 1d41d10, 42cd3e8, f96d395, 12586b1, 35084af, 7f26712, d1f413b, 655f0bd, da330cd, 50b7366, 0e059e4, a8cacb0, 3198eec, 3184cdc, 38b0a3a, b021a21, 752627a, 19583a7) | Thanks @rmehta19. Can you PTAL at the test failures? | @matthewstevenson88, I made 2 changes and it looks like 2/3 linux runs are passing. tests(11) is failing with an error in code that was not affected by this PR, so I don't think that failure is related.
The changes:


The build originally failed because the generated grpc code in this PR did not match the code generated by the codegen plugin. I couldn't easily compile the codegen myself(due to reasons explained in: #7690). So instead I used the latest published binary available on https://mvnrepository.com/artifact/io.grpc/protoc-gen-grpc-java (1.63) to generate S2AServiceGrpc.java (Created and used a separate simple maven project just to run the plugin and get grpc gen code). Then I changed line 8 & 9 of S2AServiceGrpc.java to match other generated code in this repo (ex: 
  
    
      grpc-java/alts/src/generated/main/grpc/io/grpc/alts/internal/HandshakerServiceGrpc.java
    
    
        Lines 8 to 9
      in
      fb9a108
    
  
  
    

        
          
           value = ""by gRPC proto compiler"", 
        

        
          
           comments = ""Source: grpc/gcp/handshaker.proto"") 
        
    
  

). This resolved the errors thrown by 
  
    
      grpc-java/.github/workflows/testing.yml
    
    
        Lines 65 to 66
      in
      3c867c9
    
  
  
    

        
          
           - name: Check for modified codegen 
        

        
          
             run: test -z ""$(git status --porcelain)"" || (git status && echo Error Working directory is not clean. Forget to commit generated files? && false) 
        
    
  




A failure in the tests was caused when IntegrationTest.java is in package io.grpc.s2a: GetAuthenticationMechanismsTest.java fails because the flag is not being set by JCommander (or perhaps it is getting overwritten). When I moved IntegrationTest.java into the same package as  GetAuthenticationMechanismsTest.java: io.grpc.s2a.handshaker, this error does not happen. | LGTM, thanks @rmehta19! When you have a chance, please remove ""draft"" to the PR title and can we say a bit more in the CL description including pointing to the S2A-Go README (similar to what we did in the proto PR)?
After that, can we get sign-off from @erm-g and @ejona86? | LGTM, thanks @rmehta19! When you have a chance, please remove ""draft"" to the PR title and can we say a bit more in the CL description including pointing to the S2A-Go README (similar to what we did in the proto PR)?
After that, can we get sign-off from @erm-g and @ejona86?

Done -- thank you for the review @matthewstevenson88! | To make sure I understand, s2a is the public API, and s2a/channel and s2a/handshaker are internal APIs. Are those internal APIs to be used anywhere, even within Google?

Correct s2a is the public api, s2a/handshaker and s2a/channel are internal APIs to only be used within the s2a package.
cc: @matthewstevenson88 | Thank you for reviewing @erm-g, @ejona86 ! I have addressed the comments in separate commits. PTAL when you get a chance, and provide any additional feedback. | @ejona86, PTAL when you get a chance, and leave any additional feedback. Thank you in advance!
@erm-g & @matthewstevenson88 have approved these changes. | On July 16th @ejona86 replied to an email requesting some major reworking in your implementation.  Has there been any progress on that?  Specifically:

No blocking on the event loop
Use Async, not the blocking stub
Change to a more standard way of using error classes
Have your tests specify reasonable deadlines

I've been waiting on reviewing until this rework was done.
From the email:
I'm not comfortable with that handlerAdded(). InterruptedException causes loud sirens; there must be no blocking on
the event loop. We were promised ALTS would stop blocking, but that was never implemented and it caused my team
lots of time on debugging and workarounds. Given you're able to use SSLHandler, I don't think there'd be any strong 
reason to allow blocking, even initially. I see the blocking stub, and that will probably need to be changed. 
IllegalArgumentException is just the wrong exception type; that means ""the programmer made an error"" and it should 
basically never be caught. But it also means an argument was wrong, and a KeyStoreException seems unrelated to the 
arguments. Mixing ""ChannelHandler next"" and ""negotiator.newHandler()"" also seems suspicious, as it seems you are 
delegating twice. | Thanks @larry-safran, the item's have been addressed | Thanks for the review @larry-safran ! Please let me know if there is anything else to address. | Thanks for the comments @ejona86; I'll address them in separate commits at #11534. Let me know if you prefer separate PRs for them, or any other way of organizing. | There's plenty of smaller/isolated items. Feel free to group those together as you wish, as they are easy to review. A few of the comments might turn into larger/plumbing changes. Some of those may be best to be separate. But overall, if you do lots of the ""easy"" stuff together that'd be fine to review/merge while you work on something more difficult. | @ejona86 , @larry-safran I have addressed all the comments on this PR in the following 3 PRs:

#11534
#11539
#11540

PTAL at these 3 and leave any comments.
I will address the comment to move things to internal package and combine the MTLS and non-MTLS apis in the following 2 PRs, once the above 3 are merged.

#11541
#11544

Besides these changes, please let me know anything else we can do to enable s2a to be included in a grpc-java release. Thank you!",add s2a java client. | update to use gRPC Authors with copyright. | S2AChannelPool returnChannel --> returnToPool name change. | add s2a to sync-protos script. | S2AGrpcChannelPool remove unnecessary state check. | update proto package to grpc.gcp.s2a. | ConnectionIsClosedException --> ConnectionClosedException. | identity() --> getIdentity(). | annotate S2AChannelCredentials.Builder with NotThreadSafe. | add values entered when generating certs. | remove JCommander dependency. | Committing to resolve merge conflicts when syncing. | Migrate away from deprecated functions. | Remove logging before errors thrown in S2AStub. | Build set of TLS versions from S2Av2's GetTlsConfigResp. | S2AStub uses withWaitForReady. | Don't block on SslContext creation in Java S2A client. | use javax.annotation.Nullable in S2AProtocolNegotiatorFactory. | getChannel() doesn't block. | Remove unnecessary local variable in  getAuthMechanism. | Invert if statement in ProtoUtil to improve readability. | Check localIdentity is null before setting it. | Check hostname not null or empty. | Change channelRead argument ctx to unused. | Remove unnecessary waitForReady() in IntegrationTest. | Push down the creation of Optional<S2AIdentity> until S2AProtocolNegotiator. | Wait for servers to be terminated in tearDown in IntegrationTest.java. | mark unused ctx in channelReadComplete.
grpc/grpc-java,10641,https://github.com/grpc/grpc-java/pull/10641,interop-testing: Improve ChannelAndServerBuilderTest readability,Add javadoc and code readability refactoring,2023-11-02T17:06:40Z,sergiitk,sergiitk,"readability, readable",Was a part of PR #10587 where it's no longer needed,interop-testing: Improve ChannelAndServerBuilderTest readability
grpc/grpc-java,10282,https://github.com/grpc/grpc-java/pull/10282,"services, xds, orca: LRS named metrics support","Design: 
* go/grpc-xds-lrs-named-metrics 
* go/lrs-named-metrics-design-review-slides
* Implements https://github.com/grpc/proposal/blob/master/A64-lrs-custom-metrics.md

",2023-07-11T20:29:32Z,YifeiZhuang,danielzhaotongliu,"readability, readable",PTAL @ejona86. Thanks. | cc @YifeiZhuang,"add namedMetrics map & API to CallMetricRecorder | add named metrics to ORCA server interceptor | new named metrics struct & ORCA tracer factory in ClusterImpl LB | added unit tests | added more tests | added comment for recordBackendLoadMetricStats() to perform increment/update or create entry atomically | BackendLoadMetricStats mutable, recordBackendLoadMetricStats(Map) and snapshot() synchronized | replace backend metrics map with new one in snapshot(), copy with unmodifiableMap() | updated ClusterImplLoadBalancerTest with float tolerance equality"
grpc/grpc-java,9856,https://github.com/grpc/grpc-java/pull/9856,googleapis: Allow user set c2p bootstrap config,"Instead of always overriding the bootstrap with a custom c2p config, now we allow user defined ones to also be used. This only applies when running in GCP with federation.",2023-01-24T23:57:11Z,temawi,temawi,"readability, readable","@apolcyn Could you please check that this meets the requirements?
@easwars Can you spot any discrepancies with the Go implementation?
@YifeiZhuang Can you please sanity check my approach of omitting the setting of the bootstrap override to allow xds client to pick up a user provided config?","googleapis: Allow user set c2p bootstrap config

Instead of always overriding the bootstrap with a custom c2p config, now
we allow user defined ones to also be used. This only applies when
running in GCP with federation. | Include zone and ipv6 in the condition

Also some refactoring for code clarity. | Put delegate starting back into a finally block"
grpc/grpc-java,8934,https://github.com/grpc/grpc-java/pull/8934,Static authorization server interceptor implementation,"1. Implements static authorization server interceptor with end to end tests.
2. Adds support for ""v3"" HeaderMatcher proto by handling new fields like string_match.",2022-12-21T23:30:43Z,ashithasantosh,ashithasantosh,"readability, readable","Was there a reason you didn't use RbacFilter directly?

I moved the rbac parsing logic to a common location in io.grpc.xds.internal.rbac.engine package due to following reasons

to avoid dependency on xds. Also RbacFilter is not public in io.grpc.xds; cannot be accessed from outside package
for code readability. the code seemed more readable to me if we split the rbac parsing code into a different class, instead of depending on RbacFilter implementation | Was there a reason you didn't use RbacFilter directly?

I moved the rbac parsing logic to a common location in io.grpc.xds.internal.rbac.engine package due to following reasons

to avoid dependency on xds. Also RbacFilter is not public in io.grpc.xds; cannot be accessed from outside package
for code readability. the code seemed more readable to me if we split the rbac parsing code into a different class, instead of depending on RbacFilter implementation


I think Eric is saying you only need to call rbacfilter.buildServerIntercetor() 
but yea we have to make RbacFilter accessible from auth | I think Eric is saying you only need to call rbacfilter.buildServerIntercetor()  but yea we have to make RbacFilter accessible from auth

In non-xDS authorization, we have upto two RBAC engines per Server Interceptor. And the grpc user can use the factory method to create the interceptor and attach it to server enabling grpc authorization.
gRPC Java authz API design
If we use rbac filter's method to generate server interceptor, then we would have upto two interceptors. This could complicate the API that we expose to the users or we could have two interceptors within an interceptor and keep the API unchanged. Also, when we introduce dynamic file reloading support, that is we reload the authorization policy periodically from file system, we would keep on creating new server interceptors as the policy changes, and this could complicate the design like we would have to consider the conditions where both interceptors aren't updated atomically, and we have authorization decisions being made with stale and new interceptor say. Plus code wise, I don't see any code duplication, just the creation and interception of interceptor, so I feel like we should keep xds and non-xds separate.
**EDIT: We can just swap the old with new internal static interceptor in file watcher to deal with synchronization issues. I still prefer keeping them separate as it simplifies code. | This would complicate the API that we expose to the users

No. Two vs 1 interceptor internally should have no bearing on the API we expose to users.
I suggest we do something like 
  
    
      grpc-java/xds/src/main/java/io/grpc/xds/XdsNameResolver.java
    
    
         Line 602
      in
      79f2562
    
  
  
    

        
          
           private static ClientInterceptor combineInterceptors(final List<ClientInterceptor> interceptors) { | This would complicate the API that we expose to the users

No. Two vs 1 interceptor internally should have no bearing on the API we expose to users.

Right! I meant the same thing above (quoting previous message ""or we could have two interceptors within an interceptor and keep the API unchanged"")
I updated the implementation based on the comment. Please take a look. Sorry it took so long. I was on a month long vacation and was busy catching up on some work. | @ejona86 Please take a look at the PR when you find time. Thank you:) | @ejona86 Friendly ping!:) | @ejona86 PTAL | @ejona86 PTAL | @YifeiZhuang, do you know how we were missing contains and stringMatcher? | @YifeiZhuang, do you know how we were missing contains and stringMatcher?

Oh, apparently they aren't used by this code, so that resolves some of my questions. I think they were just added later. @ashithasantosh simply noticed they were missing when comparing to C++, which makes sense. | @ejona86 Thank you for the review. PTAL I resolved all the comments.
@YifeiZhuang already approved the PR, should I wait on a final review?",Static authorization server interceptor implementation | Resolving comments | Remove RbacParser file | update error logs | checkstyle fixes | Merge remote-tracking branch 'upstream/master' into interceptor_impl | Merge remote-tracking branch 'upstream/master' into interceptor_impl | Add InternalRbacFilter | formatting | javadoc | format test file | resolving comments | Merge remote-tracking branch 'upstream/master' into interceptor_impl | minor formatting | Update comment
grpc/grpc-java,9145,https://github.com/grpc/grpc-java/pull/9145,Change the gRPC o11y logName when sending logs to GCP.,"Change the gRPC o11y logName when sending logs to GCP.
From: grpc
To: microservices.googelapis.com%2Fobservability%2Fgrpc",2022-06-08T20:57:46Z,sanjaypujare,fengli79,"readability, readable","Decoding a user provided string on web GUI could be pretty dangerous.
Also, by definition, log name projects/grpc-testing/logs/microservices.googleapis.com%2Fobservability%2Fgrpc is a string with URL encoded log id (microservices.googleapis.com/observability/grpc) in the middle.
The cloud logging explorer shows the log name instead of log id.
We verified this behavior by checking the log entries generated by audit log as well: | Decoding a user provided string on web GUI could be pretty dangerous.

You are talking about things like XSS. Indiscriminately decoding user input can be bad I agree - but they also do ""safe decoding"" when they are sure there is no security vulnerability. Decoding ""%2F"" to ""/"" in an html output can never be dangerous.
I have a basic question: if our (so called user provided) string of ""microservices.googleapis.com%2Fobservability%2Fgrpc"" is never rendered in a user friendly way (such as ""microservices.googleapis.com/observability/grpc"") to the user then why even bother with a name like that? Why not just use an _ or -  so  e.g. microservices.googleapis.com_observability_grpc ?
Also looking at the doc you cited https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry it also says
[LOG_ID] must be less than 512 characters long and can only include the following characters: upper and lower case alphanumeric characters, forward-slash, underscore, hyphen, and period.

I see some inconsistency here. They say it can contain forward-slash and I can't believe it means a percent-encoded forward-slash. | %2F is not danger doesn't mean other decoding is not danger too, like %3C. Also, this is cloud logging behavior instead of gRPC behavior. Cloud logging may apply other security protection mechanism, like validate the user generated string, etc, but eventually, it's their call about how to render the UGC.
I don't see inconsistency in cloud logging's definition and its GUI about log id and log name.
Please note (I posted in previous comments as well)

Log ID can contains forward slash /, and microservices.googleapis.com/observability/grpc is a valid Log ID.
Log Name is a string with URL encoded Log ID.
The cloud logging GUI shows the Log Name, not the Log ID.

Regarding to the microservices.googleapis.com_observability_grpc proposal, it's not a valid url any way (even worse for readability, also note that URL encode the log id implies its format should be a valid URL, or better to be), and we should be align with other googleapis, like audit logging. Having `microservices.googleapis.com/' prefix in our log id is important to distinguish from user generated log entries, they may not have a domain prefix and there's no clear way to distinguish them. | There is miscommunication. We should discuss offline to get this sorted out | A few updates after our offline meeting.

It's a must, to set URL encoded log id with in the log name. (I forwarded the bug to you via IM)
Project id was set via LoggingOptions, so, no need to set it in log name. (This needs to be followed up with cloud logging, as their document is probably not accurate, it reads like required in the log name).
Given that log id within the log name has to be URL encoded, showing a decoded log id when cloud logging GUI presents the log name is wrong, unless cloud logging change the definition of log name.

Please let me if there's other action items in your mind to unblock this PR. | @sanjaypujare, could you help to merge? | @sanjaypujare, could you help to merge?

Oops, sorry. Doing it now",Change the gRPC o11y logName when sending logs to GCP. | Format code. | Merge branch 'grpc:master' into master | Change the gRPC o11y logName when sending logs to GCP. | Reformat.
grpc/grpc-java,8767,https://github.com/grpc/grpc-java/pull/8767,xds: XdsNameResolver change to support RouteAction with RLS plugin,Implementation of the xDS Resolver section of the design http://go/grpc-rls-in-xds/view#heading=h.wkxepad0knu,2022-01-19T20:55:23Z,dapengzhang0,dapengzhang0,"readability, readable","Then we won't need separate maps for different cluster types in bunch of places, f.e. [generateServiceConfigWithLoadBalancingConfig()]

Looks like generateServiceConfigWithLoadBalancingConfig() doesn't actually need the two separate maps even now. It could just as easily iterate directly over clusterRefs. Looks like it didn't to avoid indentation/modifying the method much? I'd prefer it avoid the temporary maps here; it seems that'd be easier to understand.

Or in updateRoutes()

And this just avoids creating ClusterRefState within the Route loop. Seems like it'd be better to just generate a Map<String, ClusterRefState> within the Route loop, with very little duplicate code (just a ""check if there is already an entry"", and if so, bump reference count).
So I think this is more a ""chosen code flow"" and not a polymorphism issue.
But, Sergii, you mentioned the polymorphism would have a method to get the service config. Looking deeper, it would only need that method and an equals. Seems like ClusterRefState should just store the service config directly. That'd use a bit more memory, but there's not a ton of these objects. | Great, thanks @ejona86 for the feedback! @dapengzhang0 - I hope this makes sense and actionable for you. As I said, the code looks OK and I don't want to block this from getting merged. | Looks like generateServiceConfigWithLoadBalancingConfig() doesn't actually need the two separate maps even now. It could just as easily iterate directly over clusterRefs. Looks like it didn't to avoid indentation/modifying the method much?

Yes. I didn't to avoid having generateServiceConfigWithLoadBalancingConfig() take an argument of Map<String, ClusterRefState> clusterRefs, and ClusterRefState constains a lot of data that is unneeded for generateServiceConfigWithLoadBalancingConfig().
If generateServiceConfigWithLoadBalancingConfig() were not a static method, it would have no argument, and that would look much neater. However, this method has been static for testing purpose. | Thanks @sergiitk for blaming the code smell. Fixing it will make the code much neater. PTAL.",xds: XdsNameResolver change to support RouteAction with RLS plugin | Merge branch 'master' into xds-rls-resolver | use ImmutableMap.Builder | fit into a single line | fix code smell | rename methods | inline generateServiceConfigWithLoadBalancingConfig and use Immutable | call syncContext.throwIfNotInThisSynchronizationContext() | further improvement for readability
grpc/grpc-java,8588,https://github.com/grpc/grpc-java/pull/8588,Fix code & javadoc warnings in the binder package.,"Note: I didn't fix all javadoc warnings mentioned in #8585, since
they're not generated with a modern java version, and the fix feels
worse than the warning.

Specifically, {@link X.Y} generates a warning if only X is imported,
and {@link Z} generates a warning if Z is declared later in the class.

In particular, attempting to fix the first issue by importing X.Y results
in a code-readability warning suggesting I shouldn't do that.",2021-10-09T10:27:01Z,markb74,markb74,"readability, readable",,"Fix code & javadoc warnings in the binder package.

Note: I didn't fix all javadoc warnings mentioned in #8585, since
they're not generated with a modern java version, and the fix feels
worse than the warning.

Specifically, {@link X.Y} generates a warning if only X is imported,
and {@link Z} generates a warning if Z is declared later in the class.

In particular, attempting to fix the first issue by importing X.Y results
in a code-readability warning suggesting I shouldn't do that."
grpc/grpc-java,8818,https://github.com/grpc/grpc-java/pull/8818,kokoro: Pretty test results,"Previously, only Windows had the plumbing to rename test results for
the Kokoro result viewers to pretty-print.

macos.cfg was the only CI that lacked a corresponding .sh, which maked
unix.sh harder to reason about. Created macos.sh so that unix.sh is now
just a helper script and will not be called directly by Kokoro.",2022-01-12T23:44:01Z,ejona86,ejona86,"readability, readable, easier to read",CC @sanjaypujare,"kokoro: Pretty test results

Previously, only Windows had the plumbing to rename test results for
the Kokoro result viewers to pretty-print.

macos.cfg was the only CI that lacked a corresponding .sh, which maked
unix.sh harder to reason about. Created macos.sh so that unix.sh is now
just a helper script and will not be called directly by Kokoro. | Avoid ""gradle clean""

Still clean compiler since we do re-run the build multiple times with
varying platforms. Shouldn't be necessary, but ""just in case"" since I
want this commit to be low risk. | Avoid ""gradle clean"" on Windows, too

It has long been the case that the results were only shown on failure.
But since I'm cleaning up the other scripts, might as well fix the same
problem on Windows, too. | Allow spaces in file names"
grpc/grpc-java,8469,https://github.com/grpc/grpc-java/pull/8469,rls: migrate deprecated server/path to extraKeys,"The [`server` and `path` fields](https://github.com/grpc/grpc-java/blob/v1.40.1/rls/src/main/proto/grpc/lookup/v1/rls.proto#L25-L32) in `RouteLookupRequest` are deprecated. Instead, we will send the server/path information in side of [`key_map`](https://github.com/grpc/grpc-java/blob/v1.40.1/rls/src/main/proto/grpc/lookup/v1/rls.proto#L45).

The keys for the server, service and method in the `key_map` will be the _values_ of `host`, `service`, `method` fields respectively in [`extraKeys`](https://github.com/grpc/grpc-java/blob/v1.40.1/rls/src/main/proto/grpc/lookup/v1/rls_config.proto#L69) in RlsConfig.

We will also include all entries in the [`constantKey`](https://github.com/grpc/grpc-java/blob/v1.40.1/rls/src/main/proto/grpc/lookup/v1/rls_config.proto#L80) in RlsConfig into `RouteLookupRequest`.


Other changes:

- Add AutoValue library for ExtraKeys class, just like data classes used in grpc-xds. Will migrate other data classes to AutoValue as well.
- Not to keep `targetType` field in the route lookup request data class, because we always use ""grpc"" as targetType.",2021-09-08T04:32:34Z,dapengzhang0,dapengzhang0,"readability, readable",,rls: migrate deprecated server/path to extraKeys | address comments
grpc/grpc-java,7810,https://github.com/grpc/grpc-java/pull/7810,xds: refactor Bootstrapper,"Split Bootstrapper into Bootstrapper interface and BootstrapperImpl. Bootstrapper's implementation is getting complicated, the way how its code is organized gets bad for readability and testing. Also, having the singleton instance on Bootstrapper itself does not provide any benefit. The singleton `SharedXdsClientProvider` is already making use of a single Bootstrapper instance. We would still want Bootstrapper mockable as using a real instance in its consumer's tests is cumbersome. So we make Bootstrapper as an interface and implement a separate concrete implementation BootstrapperImpl. This is quite similar to NameResolver and DnsNameResolver.",2021-01-20T03:00:16Z,voidzcy,voidzcy,"readability, readable",,Separate Bootstapper into interface + implmentation. | Refactor tests for Bootstrapper implementation. | Fix usages for creating Bootstrapper instances. | Add test for using bootstrap filepath via system property. | Change fake bootstrap path to avoid confusion.
grpc/grpc-java,7944,https://github.com/grpc/grpc-java/pull/7944,xds: Simplify ClientXdsClientTestBase by reusing test resources,"In preparation for ADS parsing changes, I was reading through ClientXdsClientTestBase and ended up refactoring some pieces to make it easier to add [upcoming resource metadata capture](https://github.com/sergiitk/grpc-java/pull/12/commits/9cf1b78b173043aad865290f577042d76146d2b7).

- The same `Any` resources used in different places moved to private fields to make it easier to know when a different resource is used in tests on purpose
- Added some constants to make assertions for the same values easier to read/conceptualize
- Added helpers for single-resource `sendResponse()`, `verifyRequest()`, etc, to skip single-element collection so it's a bit easier to read
- Re-ordered arguments of  `sendResponse()`, `verifyRequest()` to make them consistent between each other, and upcoming similar verifyMetadata helpers

```java
protected abstract void verifyRequest(ResourceType type, List<String> resources, String versionInfo, String nonce, Node node);
protected abstract void sendResponse(ResourceType type, List<Any> resources, String versionInfo, String nonce);
```

I believe this should improve test readability.  
Before:
```java
call.sendResponse(""0"", listeners, ResourceType.LDS, ""0000"");
call.verifyRequest(NODE, ""0"", Collections.singletonList(LDS_RESOURCE), ResourceType.LDS, ""0000"");
```

After:
```java
call.sendResponse(LDS, listenerRds, VERSION_1, ""0000"");
call.verifyRequest(LDS, LDS_RESOURCE, VERSION_1, ""0000"", NODE);
```

",2021-03-08T17:14:22Z,sergiitk,sergiitk,"readability, readable, easier to read",,xds: Simplify ClientXdsClientTestBase by reusing test resources | address PR feedback
grpc/grpc-java,7375,https://github.com/grpc/grpc-java/pull/7375,"core, api, benchmarks: Random acts of garbage reduction",I noticed some opportunities to reduce allocations on hot paths,2021-01-21T18:45:32Z,ejona86,njhill,"readability, readable","Thanks @ejona86! I will address the comments when I get a chance but likely won't be until next week. | @njhill, that's more than fine. I'm sorry it took so very long for us to review.
And in case it wasn't clear, I'm fine with accepting the garbage reduction changes that probably don't reduce garbage. It's a PITA to check whether the JIT is actually doing what we hope and that still depends on what side of the bed the JIT woke up on. Since the changes don't harm readability, they seem fine. But I pointed them out since the value of those sorts of changes is questionable and we may not want to get out-of-hand with that class of change. | @ejona86 I think I've addressed all the comments, PTAL
I did also try to rebase this branch but it wouldn't let me push that:
!	refs/heads/garbage:refs/heads/garbage	[remote rejected] (refusing to allow an OAuth App to create or update workflow `.github/workflows/gradle-wrapper-validation.yml` without `workflow` scope | I did also try to rebase this branch but it wouldn't let me push that:

@njhill, that error doesn't look related to the force push. We have a github workflow now, so it looks like that error would apply every time you try to push to your repo with newer PRs. I don't know how you are authenticating, but maybe you need to go through that process again so it can request more more scopes. | @voidzcy, could you review? | CI failures need to be addressed (missing imports). | Thank you @njhill!","core, api, benchmarks: Random acts of garbage reduction

I noticed some opportunities to reduce allocations on hot paths | Address review comments from @ejona86 and @voidzcy | Fix missing imports in CronetClientTransport.java"
grpc/grpc-java,8228,https://github.com/grpc/grpc-java/pull/8228,xds: unify client and server handling HttpConnectionManager,"Enables parsing HttpConnectionManager filter for the server side TCP listener, with the same codepath for handling it on the client side. Major changes include:

- Remodeled `LdsUpdate` with HttpConnectionManager. Now `LdsUpdate` is an _oneof_ of `HttpConnectionManager` (for client side) or `Listener` (for server side). Each of `Listener`'s `FiliterChain` contains an HttpConnectionManager (required).
- Refactored code for validating and parsing the TCP `Listener` (for server side), put it into ClientXdsClient. The common part of validating/parsing HttpConnectionManager is reused/shared for client side.
- Included the `name` of FilterChain in the parsed form. As specified by the [API](https://github.com/grpc/grpc-java/blob/2cbc7fc3a5b98122cdd67bc881aefa3eef8e1b65/xds/third_party/envoy/src/main/proto/envoy/config/listener/v3/listener_components.proto#L253-L255), each FilterChain has a unique name. If the name is not provided by the control plane, a UUID is used. FilterChain names can be used for bookkeeping a set of FilterChain easily (e.g., used as map key).
- Added methods `isSupportedOnClients()` and `isSupportedOnServers()` to the `Filter` interface. Parsing the top-level HttpFilter requires knowing if the HttpFilter implementation is supported for the target usage (client-side or server-side). Note, parsing override HttpFilter configs does not need to know whether the config is used for an HttpFilter that is only supported for the client-side or server side.
- Added a new kind of Route: [Route with non-forwarding action](https://github.com/grpc/grpc-java/blob/c8ba60152958cb0f37d0e9a32ae406ce8d5f1ff0/xds/third_party/envoy/src/main/proto/envoy/config/route/v3/route_components.proto#L242). Updated the XdsNameResolver being able to handle Route with non-forwarding action: if such a Route is matched to an RPC, that RPC is failed. Note, it is possible that XdsNameResolver receives xDS updates with all Routes with non-forwarding action. That is, the service config will not reference any cluster. Such case can be handled by cluster_manager LB policy's LB config parser: the parser returns the error to Channel and the Channel will handle it as error service config.
",2021-06-18T18:57:37Z,voidzcy,voidzcy,"readability, readable, easier to read","Added support for Route with non-forwarding action and did more cleanup for tests. Also, the PR description is updated. PTAL. | Added a test case for covering parsing RBACPerRoute. CIs should pass after #8262 is merged.","Create HttpConnectionManager datatype to model HCM received as the API listener and network filter in TCP listener's filter chain. | Update LdsUpdate definition to use HttpConnection data model. | Update existing usages for LdsUpdate with data populated from HttpConnectionManager. | Move logics for parsing and validating TCP listener into ClientXdsClient. | Fix tests. | Merge branch 'master' of github.com:grpc/grpc-java into refactor/use_unified_lds_message_handler_for_client_and_server | Revert excessive changes. | Listener address port is optional for server side TCP listener. | Minor polishment. | Update misleading variable name. | Eliminate redundant static method qualifier. | Unify the handling of data inside HttpConnectionManager for both client and server side. This adds the requirement for server side HttpConnectionManager to have either RDS or inlined RouteConfiguration. | Merge branch 'master' of github.com:grpc/grpc-java into refactor/use_unified_lds_message_handler_for_client_and_server | Simplify proto getter. | Remove inapplicable TODO. | Change static factory method name from withXXX() to forXXX(). | Delete unncessary addresses in test data. | Include FilterChain's name in the parsed form. | Fix tests, use some random FilterChain names as it does not matter for existing tests. | Include FilterChain's name in the parsed form, if no name is provided, generate an UUID. | Add Route with non_forwarding_action | Parse top-level HttpFilters based on client/server usages. | Merge branch 'master' of github.com:grpc/grpc-java into refactor/use_unified_lds_message_handler_for_client_and_server | Fix parsing top-level HttpFilter with parsing override HttpFilter config method. | Include RBAC filter parsing in test. | Minor style issue. | Manage RDS resources referenced by HttpConnectionManagers in LDS responses for both client-side API listener and server-side TCP listener. | Fix XdsNameResolver handling Route without cluster, also add test covering cluster_manager LB policy config parser is able to handle config with empty childPolicy. | Add a test case for covering parsing RBACPerRoute. | Merge branch 'master' of github.com:grpc/grpc-java into refactor/use_unified_lds_message_handler_for_client_and_server | Minor style polish and code simplication. | Fix bug in parsing HttpFilter config for checking if the filter implementation is supported for client/server. Also implement the Router filter for server side's usage. | Delete redundant interface method for indicating if the filter implementation can be used for client/server. | Nack LDS response if HttpConnectionManager contains xff_num_trusted_hops. | Simplify boolean zen."
grpc/grpc-java,12353,https://github.com/grpc/grpc-java/pull/12353,Fix misleading exception in AdvancedTlsX509TrustManager when cert file is missing,"fixes : https://github.com/grpc/grpc-java/issues/10221
Adds a regression test using reflection on the private readAndUpdate() method to verify that a missing file properly results in a FileNotFoundException.",2025-09-15T12:34:10Z,kannanjgithub,Sangamesh1997,readable,"@Sangamesh1997, please follow our commit message convention (see ""Follow typical Git commit message structure"" bullet of CONTRIBUTING.md). The ""fixes"" should have been part of the commit message and really even the ""java.security.GeneralSecurityException: Files were unmodified..."" error should have been included. Otherwise it isn't clear what ""misleading exception"" you are talking about.",Util: AdvancedTlsX509TrustManager code changes for handling fine not found | Util: AdvancedTlsX509TrustManager code changes for handling fine not found | Util: AdvancedTlsX509TrustManager review comments addressed | Merge branch 'grpc:master' into Issue_Fixed_10221
grpc/grpc-java,12327,https://github.com/grpc/grpc-java/pull/12327,android: fix network change handling on API levels < 24,"Fixes : https://github.com/grpc/grpc-java/issues/12313

In case if VPN toggled on<->off `wasConnected` returns true and `delegate.enterIdle()` is skipped.


PS: Not using `android.net.ConnectivityManager.NetworkCallback#onCapabilitiesChanged` for API 24+ can also be an issue if capabilities change and user needs to receive updates.",2025-09-18T19:51:25Z,ejona86,dmytroreutov,readable,The committers listed above are authorized under a signed CLA.✅ login: dmytroreutov / name: dmytroreutov  (fb0abb3) | Removed this block from AndroidChannelBuilderTest because it stopped making sense after changes in AndroidChannelBuilder. | Thank you!,android: fix network change handling on API levels < 24
grpc/grpc-java,11863,https://github.com/grpc/grpc-java/pull/11863,xds: Reuse filter interceptors across RPCs,"This moves the interceptor creation from the ConfigSelector to the resource update handling.

The code structure changes will make adding support for filter lifecycles (for RLQS) a bit easier. The filter lifecycles will allow filters to share state across interceptors, and constructing all the interceptors on a single thread will mean filters wouldn't need to be thread-safe (but their interceptors would be thread-safe).

CC @larry-safran (It's a bit unclear who best to review; Sergii dug into this some which triggered me to make the PR. If you're interested, take a look.)",2025-01-30T20:43:51Z,ejona86,ejona86,"readable, clarity, easier to read",,"xds: Reuse filter interceptors across RPCs

This moves the interceptor creation from the ConfigSelector to the
resource update handling.

The code structure changes will make adding support for filter
lifecycles (for RLQS) a bit easier. The filter lifecycles will allow
filters to share state across interceptors, and constructing all the
interceptors on a single thread will mean filters wouldn't need to be
thread-safe (but their interceptors would be thread-safe). | Add Nullable to RouteAction in RouteData constructor"
grpc/grpc-java,11658,https://github.com/grpc/grpc-java/pull/11658,interop-testing: Add concurrency condition to the soak test using existing blocking api ,"The goal of this PR is to increase the test coverage of the C2P E2E load test by improving the rpc_soak and channel_soak tests to support concurrency.

**rpc_soak:**
The client performs many large_unary RPCs in sequence over the same channel. The test can run in either a concurrent or non-concurrent mode, depending on the number of threads specified (soak_num_threads):
  - Non-Concurrent Mode: When soak_num_threads = 1, all RPCs are performed sequentially on a single thread.
  - Concurrent Mode: When soak_num_threads > 1, the client uses multiple threads to distribute the workload. Each thread performs a portion of the total soak_iterations, executing its own set of RPCs concurrently.

**channel_soak:**
Similar to rpc_soak, but this time each RPC is performed on a new channel. The channel is created just before each RPC and is destroyed just after. Note on Concurrent Execution and Channel Creation: In a concurrent execution setting (i.e., when soak_num_threads > 1), each thread performs a portion of the total soak_iterations and creates and destroys its own channel for each RPC iteration.
- createNewChannel Function: In channel_soak, the createNewChannel function is used by each thread to create a new channel before every RPC. This function ensures that each RPC has a separate channel, preventing race conditions by isolating channels between threads. It shuts down the previous channel (if any) and creates a new one for each iteration, ensuring accurate latency measurement per RPC.

- Thread-specific logs will include the thread_id, helping to track performance across threads, especially when each thread is managing its own channel lifecycle.

PTAL @apolcyn 
@DNVindhya ",2024-11-18T19:57:02Z,sergiitk,zbilun,readable,"The committers listed above are authorized under a signed CLA.✅ login: zbilun  (7a9f3a9, 3d8c555, 885e109, 96f779e, 512ad84, e3232e6, 58e2ebd, 902c15a, fedbd64, 84dac62, 92498b4, bd7df8a, 2bd91dd, 52c4fe2, b83b33f, 56883b5, b2f0968, 5f55a8c) | In the TestServiceClient.java,  case RPC_SOAK and case CHANNEL_SOAK, I have added the try catch here due to the IDE's error about the unhandled InterruptedException in the lambda. | During our last meeting, you mentioned that we don't need to modify the code in XdsFederationTestClient.java. However, in the PR, we've made changes to the performSoakTest which is called in the XdsFederationTestClient  I think we may need to update the calling code in XdsFederationTestClient.java. Any thoughts on this? | During our last meeting, you mentioned that we don't need to modify the code in XdsFederationTestClient.java. However, in the PR, we've made changes to the performSoakTest which is called in the XdsFederationTestClient I think we may need to update the calling code in XdsFederationTestClient.java. Any thoughts on this?

Oh in that case we should just apply the same change to XdsFederationTestClient as we did for TestServiceClient: instead of passing a boolean to reset the channel or not, pass the appropriate lambda","Add concurrency condition to the soak test using exisiting blocking api | Modify the influenced files | Address code review comments from Alex: improve soak test logic by using soak_num_threads Flag | Address code review comments from Alex: modify totalFailures handling, channel creation logic, and refactor thread body for performSoakTest | Removed useless file | Modify the channel implementation for rpc_soak test. | Refactor soak test to use function callback for channel management and simplify thread result aggregation | Refactor performSoakTest and related functions to propagate InterruptedException. Update the ThreadResults data type. | Rename executeSoakTestInThread function and update affected code in XdsFederationTestClient.java to align with changes in performSoakTest | Update the lambda function exception handling. | Refactor maybeCreateNewChannel to handle Exception and remove numThread flag for XdsFederationTestClient | Remove unnecessary imports and redundant try/catch blocks in AbstractInteropTest and XdsFederationTestClient | Replace maybeCreateNewChannel with createNewChannel and simplify lambdas for RPC_SOAK and CHANNEL_SOAK | fix typo | fix typo | fix exception error | fix style | fix style"
grpc/grpc-java,11638,https://github.com/grpc/grpc-java/pull/11638,xds: Implement GcpAuthenticationFilter,"This is a step towards [gRFC A83](https://github.com/grpc/proposal/blob/master/A83-xds-gcp-authn-filter.md). 
When enabled for a listener, GcpAuthenticationFilter fetches JWT tokens from GCE Metadata Server for the cluster audience, creates Google Cloud CallCredentials using the token and attaches them to outgoing RPCs.",2024-11-06T11:09:00Z,shivaspeaks,shivaspeaks,readable,"@ejona86 I see there's one more mistake in this PR,
GcpAuthenticationFilter.java and LruCache.java both are in package io.grpc.xds.internal which is not what I intended to do. Probably it slipped in track-pad while refactoring.
I wanted to put GcpAuthenticationFilter.java in package io.grpc.xds where all other Filters are residing. And LruCache.java in package io.grpc.xds.internal.
Makes sense? What do you think? | I suggest putting both in the same package (io.grpc.xds) and both being package-private. LruCache is small and only used by the Filter, having it close to the filter is helpful.
(Personally, I'd generally put LruCache as a nested class of the Filter. Yes, you can think it can be reused, but reusable code is reusable when it is reused. Until then ""reusable"" is a lie and can be harmful if you make the wrong thing complex to make some code generic. We can always move the code out of the class. Even in this case, the long of maxSize ""pollutes"" the LruCache with random Filter logic.) | (Although I'll also mention that in cases like this where it doesn't matter, it's mostly up to the author of the code to decide these sorts of things.) | we no longer need to return-an-error/NAK if the value is too large; we'd only NAK on zero. Other cases we just clamp to our maximum.

Alright, so we do check for <=0 values in our parseFilterConfig() and in LRUCache we do clamp to the minimum of our max or provided size. I think we're good with this implementation. But I want to know more about the downsides of limiting the size to our maximum.
FYI, the highlighted message in this comment is from grpc/proposal#438 (comment) | Well, <0 for a uint is ""very large"", so those should get clamped instead of fail. You can use UnsignedLongs from Guava to help do the comparisons.
There's no real downside to our limit. A thousand would be a heck of a lot and a million is outrageous. A billion is well beyond the realm of legitimate.",xds: Implement GcpAuthenticationFilter | Refactored LruCache | cacheSize now compared with Guava's UnsignedLongs | Added GcpAuthenticationFilterTest | updated envoy api version and some refactoring | Envoy proto sync to 2024-11-01 | Envoy proto sync to 2024-11-01 | Added required deps in BUILD.bazel | Refactoring | Some more refactoring
grpc/grpc-java,11437,https://github.com/grpc/grpc-java/pull/11437,core: Don't reuse channels in PickFirstLeafLB test,"PickFirstLeafLB uses channel reference equality to see if it has re-created subchannels. The tests reusing channels breaks the expected invariant.

----

I've got a cleanup to PF to use SubchannelData more, but the change triggers bugs in the tests. So this fixes the test first, and will pass both before and after the PF cleanup.",2024-08-02T18:40:02Z,ejona86,ejona86,readable,"Checkstyle seems to be unreasonable in that it doesn't allow underscores in
variable names.
…
On Fri, Aug 2, 2024 at 10:04 AM Eric Anderson ***@***.***> wrote:
 @ejona86 <https://github.com/ejona86> requested your review on: #11437
 <#11437> core: Don't reuse channels
 in PickFirstLeafLB test.

 —
 Reply to this email directly, view it on GitHub
 <#11437 (comment)>, or
 unsubscribe
 <https://github.com/notifications/unsubscribe-auth/AZQMCXU6AB7UTDWKFKEHFBTZPO33PAVCNFSM6AAAAABL44WIFKVHI2DSMVQWIX3LMV45UABCJFZXG5LFIV3GK3TUJZXXI2LGNFRWC5DJN5XDWMJTG42DINRRG43TMNQ>
 .
 You are receiving this because your review was requested.Message ID:
 ***@***.***> | Yeah, that is technically the style. I thought it missed it. I'll figure out some text to put between the numbers instead (I don't hate myself enough to change 1_2 to 12 😄). Maybe ""inst"", ""instance 2"" is somewhat readable.","core: Don't reuse channels in PickFirstLeafLB test

PickFirstLeafLB uses channel reference equality to see if it has
re-created subchannels. The tests reusing channels breaks the expected
invariant. | Use n2 for ""number 2"" instead of _2 in subchannel names"
grpc/grpc-java,11294,https://github.com/grpc/grpc-java/pull/11294,netty:Fix Netty composite buffer merging to be compatible with Netty 4.1.111,"Change the logic for identifying changes to the read index when merging into a netty composite buffer so that it works with both older versions of netty and netty 4.1.111

Fixes #11284 ",2024-06-20T22:55:07Z,larry-safran,larry-safran,readable,"@ejona86 @larry-safran I see you added ""backport"" and ""release blocker"" tags. Could you please share in what versions do you plan to backport this fix and what is ETA for new releases? | I plan to backport to 1.65 (not yet released), 1.64 & 1.63
…
On Fri, Jun 21, 2024 at 10:29 AM Idel Pivnitskiy ***@***.***> wrote:
 @ejona86 <https://github.com/ejona86> @larry-safran
 <https://github.com/larry-safran> I see you added ""backport"" and ""release
 blocker"" tags. Could you please share in what versions do you plan to
 backport this fix and what is ETA for new releases?

 —
 Reply to this email directly, view it on GitHub
 <#11294 (comment)>,
 or unsubscribe
 <https://github.com/notifications/unsubscribe-auth/AZQMCXTKL2CKOXBT3DDJCULZIRPHTAVCNFSM6AAAAABJLFX722VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCOBTGE2TONRWG4>
 .
 You are receiving this because you were mentioned.Message ID:
 ***@***.***> | @larry-safran what is ETA for 1.64.1 to be available in Maven Central? | I'm actively working on the patch release so it should be available this
week.
…
On Tue, Jun 25, 2024 at 10:00 AM Idel Pivnitskiy ***@***.***> wrote:
 @larry-safran <https://github.com/larry-safran> what is ETA for 1.64.1 to
 be available in Maven Central?

 —
 Reply to this email directly, view it on GitHub
 <#11294 (comment)>,
 or unsubscribe
 <https://github.com/notifications/unsubscribe-auth/AZQMCXQ3ZZT6TEEESZUU7GTZJGO2HAVCNFSM6AAAAABJLFX722VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCOBZGQ3TCNJXGM>
 .
 You are receiving this because you were mentioned.Message ID:
 ***@***.***>","Change the logic for identifying changes to the read index when merging into a netty composite buffer so that it works with both older versions of netty and netty | Change the logic for identifying changes to the read index when merging into a netty composite buffer so that it works with both older versions of netty and netty | Also use the old readerIndex, not just arrayOffset, to set the new readerIndex | Fix debugging leftover | Fix mistaken file update | Utilize netty-buffer version number to choose approach to setting readerIndex during merges. | Improve version comparison to handle versions prior to 4.1.100 | Change offset calculation logic to be based on the difference between internal and external buffers' writeIndex values. | Use the same approach for all netty versions. | Use addComposite instead of addFlattenedComposite and not append to components that are composites. | Use addComposite instead of addFlattenedComposite and not append to components that are composites. | Add Javadoc | Add Javadoc | Respond to review comments. | Replace most addFlattenedComponents with addComponent."
grpc/grpc-java,9574,https://github.com/grpc/grpc-java/pull/9574,Use the `artifact` macro for loading maven deps,"The recommended way to load dependencies from `rules_jvm_external` is to make use of the `@maven` workspace, and the most readable way of doing that is to use the `artifact` macro provides.

This removes the need to generate the ""compat"" namespaces, which `rules_jvm_external` provided for backwards compatibility with older releases. This change also sets things up for supporting `bzlmod`: this requires all workspaces accessed by a library to be named ""up front"" in the `MODULE.bazel` file. This way, the only repo that needs to be exported is `@maven`, rather than the current huge list.

Finally, this PR introduces a lock file for `rules_jvm_external` which improves local build times by avoiding the need to do a local resolution. In order to avoid the common failure case of ""add a dep, forget to regenerate the lock file"", the `fail_if_repin_required` attribute is set: builds will fail if the deps have been updated but the lock file hasn't been.",2024-03-28T21:33:32Z,ejona86,AutomatedTester,readable,"The committers listed above are authorized under a signed CLA.✅ login: AutomatedTester / name: David Burns  (579c97a)✅ login: ejona86 / name: Eric Anderson  (4944d5b, ac40960, 46eb721, 52cccdb, 8d10eca) | We've been purposefully avoiding using @maven directly since using it would require our users to use maven_install and they needed time to migrate. It's probably been long enough. Let me double-check the state of things. | I'm +1 for this PR. Thank you for double-checking @ejona86!
I'm one of the maintainers of rules_jvm_external, and did the work to make it play nicely with bzlmod. I've not implemented the backwards compat libraries there because the user experience with them is so poor --- essentially, someone using them will need to list the transformed name of every single first-order dependency in their MODULE.bazel. For grpc-java, that's a pretty long list, which will need to be maintained in lock-step with any new imports added. The alternative (of using @maven) is significantly less work and (more importantly) a lot less error-prone.
Until grpc-java is modularised (#9559) anyone who transitively depends on grpc-java in their modularised builds would need to maintain this list themselves, and it would require support in rules_jvm_external that I'm not keen on putting there. | TL;DR: it's fine not to use the lock file.
There's no need to use it, but it makes builds more reproducible. Builds may also be faster, since they can skip the dependency resolution step: I'm not sure how much of a difference that makes for you, and Bazel should be caching the results until the bazel server shuts down. | Hmm... I don't know how #9559 will impact this (or everything, really) | it would be great if we could merge this, I have a local change that supports bzlmod for this repo but it either depends on this or breaks compat w/ the non-bzlmod setup","Use the `artifact` macro for loading maven deps

The recommended way to load dependencies from `rules_jvm_external`
is to make use of the `@maven` workspace, and the most readable
way of doing that is to use the `artifact` macro provides.

This removes the need to generate the ""compat"" namespaces, which
`rules_jvm_external` provided for backwards compatibility with
older releases. This change also sets things up for supporting
`bzlmod`: this requires all workspaces accessed by a library to
be named ""up front"" in the `MODULE.bazel` file. This way, the
only repo that needs to be exported is `@maven`, rather than the
current huge list.

Finally, this PR introduces a lock file for `rules_jvm_external`
which improves local build times by avoiding the need to do a
local resolution. In order to avoid the common failure case of
""add a dep, forget to regenerate the lock file"", the
`fail_if_repin_required` attribute is set: builds will fail if
the deps have been updated but the lock file hasn't been. | Merge branch 'master' into HEAD

Turned back off compat repositories to swapping new targets to
artifact() in a less-noisy commit.

 Conflicts:
	BUILD.bazel
	WORKSPACE
	api/BUILD.bazel
	core/BUILD.bazel
	netty/BUILD.bazel
	okhttp/BUILD.bazel
	services/BUILD.bazel
	testing/BUILD.bazel
	xds/BUILD.bazel | Convert more to artifact(); remove compat_repositories() | Revert change to use maven_install_json | Remove pinned_maven_install() | Fix merge conflicts, remove unnecessary formatting changes | Another merge tweak"
grpc/grpc-java,10912,https://github.com/grpc/grpc-java/pull/10912,Move xds classes for Stubby to xds.client package,"To allow the Stubby team to use XdsClient, moved the basic set of classes to a new package io.grpc.xds.client.  Removed from those classes references to grpc specific things.  Created some interfaces and some grpc spefic extensions of those classes.",2024-02-27T00:41:17Z,larry-safran,larry-safran,readable,"CC @anicr7, we'll probably want to wait for this one as well.","checkpoint 1 of moving bootstrap | Move bootstrap, XdsClient and load reporting to xds.client package. | Fix checkstyle | checkpoint | Respond to review comments. | Responded to more review comments | address review comments | Respond to review comments. | force update | duh"
grpc/grpc-java,10633,https://github.com/grpc/grpc-java/pull/10633,Handle slow security policies without blocking gRPC threads.,"- Introduce PendingAuthListener to handle a ListenableFuture<Status>, progressing the gRPC through each stage in sequence once the future completes and is OK.
- Move unit tests away from `checkAuthorizationForService` and into `checkAuthorizationForServiceAsync` since that should be the only method called in production now.
- Some tests in `ServerSecurityPolicyTest` had their expectations updated; they previously called synchornous APIs that transformed failed `ListenableFuture<Status>` into one or another status. Now, we call the sync API, so those transformations do not happen anymore, thus the test needs to deal with failed futures directly.
- I couldn't figure out if this PR needs extra tests. AFAICT `BinderSecurityTest` should already cover the new codepaths, but please let me know otherwise.

This should be the last PR to address #10566.",2023-12-06T10:13:29Z,markb74,mateusazis,"readable, easier to read",,"Handle slow security policies without blocking gRPC threads.

- Introduce PendingAuthListener to handle a ListenableFuture<Status>, progressing the gRPC through each stage in sequence once the future completes and is OK.
- Move unit tests away from `checkAuthorizationForService` and into `checkAuthorizationForServiceAsync` since that should be the only method called in production now.
- Some tests in `ServerSecurityPolicyTest` had their expectations updated; they previously called synchornous APIs that transformed failed `ListenableFuture<Status>` into one or another status. Now, we call the sync API, so those transformations do not happen anymore, thus the test needs to deal with failed futures directly.
- I couldn't figure out if this PR needs extra tests. AFAICT `BinderSecurityTest` should already cover the new codepaths, but please let me know otherwise.

This should be the last PR to address #10566. | - Short-circuit immediately resolved auth futures.
- Drop sequential executor from PendingAuthListener. | fix indentation | Address jdcormie's comments | - Fix indentation
- Factory method for PendingAuthListener.
- Fix leak of executor when PendingAuthListener encountered an exception while starting the call.
- Catching RuntimeException. | Exception -> RuntimeException | - Refactor test helpers.
- Comment + TODO for multiple concurrent auth checks. | Move ListenableFuture handling logic to BinderTransportSecurity | Move the lifecycle management of the executor to the BinderServer. | - revert formatting changes in ServerSecurityPolicyTest
- define BinderTransportSecurity.ShutdownListener interface
- take a non-close executor closeable in BinderServer's constructor | extra lint and style fixes"
grpc/grpc-java,10596,https://github.com/grpc/grpc-java/pull/10596,Allow async/slow implementations of authorization checks for Binder gRPCs.,"Introduce `AsyncSecurityPolicy`, exposing a method returns a `ListenableFuture<Status>` that callers can implement to perform slower auth checks (like network calls, disk I/O etc.) without necessarily blocking the gRPC calling thread.

Partially addresses: https://github.com/grpc/grpc-java/issues/10566",2023-10-20T20:53:13Z,ejona86,mateusazis,readable,"The committers listed above are authorized under a signed CLA.✅ login: mateusazis / name: Mateus Azis  (9a94b59, 2293136, 0ee47e0, aec48f6, 05c35b1, f84ca5e, ea519e4, 3d558f5, acbcfa6, 5ba3c33) | @jdcormie & @ejona86 - Since this is making a change to how we enforce security, can I ask you to give this a once over as well? It looks good to me.","Allow async/slow implementations of authorization checks for Binder gRPCs.

Introduce `AsyncSecurityPolicy`, exposing a method returns a `ListenableFuture<Status>` that callers can implement to perform slower auth checks (like network calls, disk I/O etc.) without necessarily blocking the gRPC calling thread.

Partially addresses: https://github.com/grpc/grpc-java/issues/10566 | Address code review comments.

- Fixed some indentation issues.
- Updated Javadocs.
- Added more tests for cancelled and interrupted futures.
- Re-interrupted the current thread when handling interrupted exception. | Move out AsyncSecurityPolicy to another commit | Address code review feedbacks:

- duplicate imports
- Javadoc fixes
- removing @ExperimentalAPI annotation | Javadoc improvements. | Added exception handling, javadoc to checkAuthorizationForServiceAsync. | Correctly handle thread interruption. | Improve/fix Javadoc about checkAuthorizationForServiceAsync. | Empty commit to trigger a build | Remove @ExperimentalApi from internal package; update Javadoc linkswq."
grpc/grpc-java,9710,https://github.com/grpc/grpc-java/pull/9710,test:Report the values that were compared rather than the underlying durat…,"Eliminate usage of BigInteger as its usage could lead to flakiness due to potential class loading issues and it is completely unnecessary.

Report the values that were compared rather than the underlying durations that generated those values and continue to change over time.

CallOptionsTest.withDeadlineAfter frequently fails even though the reported values seem to be within the specified delta.  Having the reported values differ from what is actually compared is confusing.

Fixes #6757 ",2022-11-22T01:29:37Z,larry-safran,larry-safran,readable,"It was previously reporting in seconds which I think really is the most
readable.  Since the BigInteger values were in nanoseconds, I wanted to
continue displaying seconds.
…
On Mon, Nov 21, 2022 at 3:47 PM Sergii Tkachenko ***@***.***> wrote:
 ***@***.**** commented on this pull request.
 ------------------------------

 In context/src/test/java/io/grpc/testing/DeadlineSubject.java
 <#9710 (comment)>:

 > @@ -65,9 +66,9 @@ public void of(Deadline expected) {
          BigInteger deltaNanos = BigInteger.valueOf(timeUnit.toNanos(delta));
          if (actualTimeRemaining.subtract(expectedTimeRemaining).abs().compareTo(deltaNanos) > 0) {
            failWithoutActual(
 -              fact(""expected"", expected),
 -              fact(""but was"", actual),
 -              fact(""outside tolerance in ns"", deltaNanos));
 +              fact(""expected"", expectedTimeRemaining.doubleValue() / SECONDS.toNanos(1)),

 Why delete by one nanosecond? Would TimeUnit.convert(expectedTimeRemaining.doubleValue(),
 TimeUnit.NANOSECONDS) work?

 —
 Reply to this email directly, view it on GitHub
 <#9710 (review)>,
 or unsubscribe
 <https://github.com/notifications/unsubscribe-auth/AZQMCXQQG7URSWQEJKLVZ3TWJQCZ3ANCNFSM6AAAAAASHFSBIQ>
 .
 You are receiving this because you authored the thread.Message ID:
 ***@***.***>",Report the values that were compared rather than the underlying durations that generated those values. | Eliminate BigInteger to eliminate flakiness from possible class loading overhead.
grpc/grpc-java,9688,https://github.com/grpc/grpc-java/pull/9688,compiler: Generate interfaces for services to implement,"When generating Java code from an rpc proto, not only generate the ImplBase and stub methods, but also generate an interface named **AsyncService** containing the async service methods and have the \<service\>**ImplBase** classes implement it in addition to BindableService.

All methods for the interfaces are defined with defaults.  
The current logic for the methods in \<service\>**ImplBase** will be moved to the default methods of**AsyncService** and the methods will be removed from \<service\>**ImplBase**.

The bindService logic will be moved into the main <service>Grpc class in a method with the signature:
`public static final io.grpc.ServerServiceDefinition bindService(<service>Async serviceImpl)
`
fixes #9320 
",2023-02-15T18:33:45Z,larry-safran,larry-safran,readable,We do not want the client stubs to implement the interfaces; they are final today to prevent mocking. | @ejona86 Ready for rereview. | @ejona86 ping,"Proposed changes to introduce interfaces in generated code. | Eliminate all added imports from generated file.
Rename TestService to TestServiceAsync
Change TestServiceAsync default implementations to be copies of TestServiceImplBase's
Change exception type thrown by default implementations of Blocking and Future interaces to UnsupportedOperationException | compiler: Checkpoint | compiler: Checkpoint | compiler: Checkpoint | compiler: Checkpoint | Use fully qualified class names, even for the current package. | Update generator logic to add in the interfaces and match what was hand done for TestServiceGrpc.java. | Fix some syntax errors. | Fix some syntax errors. | fix syntax errors | Fix some syntax errors. | Final tweaks. | Final tweaks | update golden files to match output with interfaces | Update all generated service files (*Grpc.java) | Add the generated java files from subprojects that aren't built by
default. | Remove interfaces from stubs (includes not creating BLOCKING and FUTURE client interfaces). | regenerate service files with interface only for server | update pom files to allow java 1.8 | Respond to reviewer comments:
  * Add a bindService(<service>Async) method
  * Remove methods from BaseImpl that were defaulted in Interface
  * Fix indentations. | Fix generation of bind method | more generated service files | android interop testing generated files | Update interface name from <service>Async to AsyncService | Updated generated files to match change in interface name | Respond to code review | Fixed indenting, so regenerated all files | Fixed indenting, so regenerated all files | Updated generated files to match change in interface name | Add @Experimental flag and regenerate files | Add a test case using a class that implements the interface directly instead of using ImplBase. | Respond to code review. | Fix syntax error | fix syntax errors | regenerate android-interop-testing service files | Address code review comments | Regenerated with period at end of method descriptions | Remove ExperimentalApi annotation from generated files | Merge branch 'master' into service-interfaces | squash! Remove ExperimentalApi annotation | missed generated file | Move testing with interface to the main interop test class instead of alts and revert the ignore of venv."
grpc/grpc-java,9944,https://github.com/grpc/grpc-java/pull/9944,census: Trace annotation for reporting inbound message sizes,"This PR uses [OpenCensus Annotation](https://www.javadoc.io/static/io.opencensus/opencensus-api/0.31.0/io/opencensus/trace/Annotation.html) to report message size [bytes] for inbound/received messages in traces.

`addMessageEvent` API which is currently used expects both uncompressed and compressed message (optional) sizes to be reported at the same. Since decompression for messages happens at a later point in time, reporting compressed message as is and reporting uncompressed size as `-1` renders the size as _0 bytes received_ in cloud tracing front end.

As a workaround, we add _two annotations for each received message_:
* For compressed message size
* For uncompressed message size (when it is available)

This PR also removes `addMessageEvents` a flag introduced in PR #9485 to temporarily suppress message events for gcp-observability 

CC @ejona86 @sanjaypujare ",2023-03-11T00:19:21Z,ejona86,DNVindhya,readable,,use annotation instead of message events for reporting received message sizes(compressed and uncompressed) | addressed comments
grpc/grpc-java,9902,https://github.com/grpc/grpc-java/pull/9902,"services,orca: update backend metrics support to allow for server-wide metrics recording (per-call and OOB)","Design doc: go/grpc-backend-metrics-update

This PR includes:

- Add range validation checks for setters in `MetricRecorder` and `CallMetricRecorder` (for consistency with the `OrcaLoadReport` proto and ""push down"" validation checks as early as possible)
- `MetricRecorder` is passed to constructor for `OrcaMetricReportingServerInterceptor` (for per-call load reporting) and if the `MetricRecorder` is set, then merges with metrics from `CallMetricRecorder` which takes a higher precedence.
- update/add unit tests such that it reflects the range validation checks",2023-04-10T18:45:05Z,ejona86,danielzhaotongliu,readable,"The committers listed above are authorized under a signed CLA.✅ login: danielzhaotongliu / name: Daniel Liu  (755a50e, d6f1167, 28d4d8d, edcb0e0) | @YifeiZhuang could you also adding another kokoro:run label after your review since there was a previous kokoro build failure that I fixed in the latest commit. I followed go/grpc-local-testing#kokoro-ephemeral-one-off-builds using my fork, branch and the job as grpc/java/master/presubmit/bazel.cfg and but it gave error: Pool 'grpc-ubuntu16' does not exist for cluster 'UNKNOWN_CLUSTER'. | go/grpc-local-testing#kokoro-ephemeral-one-off-builds

That's C-centric documentation. I don't know what it does. I don't know if you have permission, but if you notice a Kokoro build flake you may be able to re-trigger via:

Click ""Details""
Click ""Invocation Details""
Follow ""FUSION_URL""
Look on the left for the run that has a blue box around it, which denotes the run you are currently looking at. (Dunno if you have permissions here, but I'd wager you do:) Click the ""rebuild"" button in the top-right of that's run's small box

To try against your own repo, you can do the same flow (choose a random Details from a random PR), but instead of ""rebuild"":

Create a PR in your repo, just to make the tool happy
Click ""Trigger Build"" at the top-right
Set ""Owner"" to be your github username
Set PR number
Click Trigger | Assuming ""don't break the existing API"" is fixed, this is fine.
@YifeiZhuang, do you think the changes to the example and interop-testing won't break anything (""the example doesn't work like it claims"" or ""interop-testing now fails"")?

it does look like the orca-oob intergration test is failing.
zivy-macbookpro:grpc-java-v1 zivy$ ./run-test-server.sh --use_tls=false
Server started on port 8080

zivy-macbookpro:grpc-java-v1 zivy$ ./run-test-client.sh --use_tls=false --test_case=orca_oob --service_config_json='{""loadBalancingConfig"":[{""test_backend_metrics_load_balancer"":{}}]}'
Running test orca_oob
Exception in thread ""main"" expected to be less than: 5
but was                 : 5
	at io.grpc.testing.integration.AbstractInteropTest.testOrcaOob(AbstractInteropTest.java:1830)
	at io.grpc.testing.integration.TestServiceClient.runTest(TestServiceClient.java:499)
	at io.grpc.testing.integration.TestServiceClient.run(TestServiceClient.java:274)
	at io.grpc.testing.integration.TestServiceClient.main(TestServiceClient.java:70) | Applied change from #9902 (review) to AbstractInteropTest.java in latest commit.","add range validation & merge metrics from MetricRecorder and CallMetricRecorder in per-call reporting | add/modify unit tests for OrcaServiceImpl & CallMetricRecorder | add/modify unit tests for OrcaMetricReportingServerInterceptor | fix indentation | add tests for MetricRecorder and merging of metrics | make MetricReporter non-static and volatile in OrcaMetricReportingServerInterceptor & update tests | MetricRecorder is final & passed into ctor, update to DCL lazy init singleton for ORCA server interceptor | Remove unused method in ORCA server interceptor | merge all metrics & optimize to 1 Builder.build(), remove file formatting, addressed other review comments | remove unnecessary code styling | add default create() method to replace old getInstance() method | remove unnecessary null check in OrcaMetricReportingServerInterceptor | create() allows null & interceptor handles null | fix kokoro build failure: add MetricRecorderHelper.java dep to services/BUILD.bazel | keep getInstance(), add Javadoc for create(MetricRecorder), update util for AbstractInteropTest"
grpc/grpc-java,9622,https://github.com/grpc/grpc-java/pull/9622,gcp-observability: updated config to public preview config,"This PR implements config changes defined for public preview. 
With new config schema, logging filters are matched on text-order basis compared to precedence basis and takes separate filters for client and server RPC events.

b/245415575

CC @sanjaypujare  @ejona86 ",2022-10-18T21:23:54Z,sanjaypujare,DNVindhya,readable,"...are matched on iterative basis...

Might be better to say it uses ""text order"" to match since ""iterative"" could be confusing | Addressed comments. PTAL | Thanks for the fix. The integration tests all passed.",updated observability config to public preview config | added logic for detecting project id from environment; addressed comments(1) | updated logic for empty config | fixed config test | updated trace config variable name to | added unit test to use mock GcpLogSink; addressed comments(2) | added unit test to use mock GcpLogSink; addressed comments(2) | resolved conflicts from merging logging proto changes
grpc/grpc-java,8393,https://github.com/grpc/grpc-java/pull/8393,core: fix RetriableStream edge case bug introduced in #8386,"While adding regression tests to #8386, I found a bug in an edge case: while retry attempt is draining the last buffered entry, if it is in the mean time committed and then we cancel the call, the stream will never be cancelled. See the regression test case `commitAndCancelWhileDraining()`.

Although we can add regression test that reproduces any bug we found, we can not really unit-test race condition, so I try to make the concurrency control logic simple and readably, but it's still hard to prove the correctness, nay...",2021-08-07T01:32:55Z,dapengzhang0,dapengzhang0,readable,,core: fix RetriableStream edge case bug introduced in #8386
grpc/grpc-java,8711,https://github.com/grpc/grpc-java/pull/8711,core: have JsonUtil support parsing String value as number,"As documented in https://developers.google.com/protocol-buffers/docs/proto3#json,
the canonical proto-to-json converter converts int64 (Java long) values to string values in Json rather than Json numbers (Java Double). Conversely, either Json string value or number value are accepted to be converted to int64 proto value.

To better support service configs defined by protobuf messages, support parsing String values as numbers in `JsonUtil`.
",2021-11-19T18:12:40Z,dapengzhang0,dapengzhang0,"readable, easier to read",cc @ejona86,core: have JsonUtil support parsing String value as number | add more tests | replace meaningless keys | add test for exponent
grpc/grpc-java,8169,https://github.com/grpc/grpc-java/pull/8169,xds: add null reference checks in SslContextProviderSupplier,Add null checks in SslContextProviderSupplier constructor and `close` (in case `updateSslContext` was never called),2021-05-12T17:27:44Z,sanjaypujare,sanjaypujare,readable,,xds: add null reference checks in SslContextProviderSupplier | address review comment
grpc/grpc-java,8326,https://github.com/grpc/grpc-java/pull/8326,xds: add hint of fault injection to injected failures,,2021-07-15T02:37:25Z,dapengzhang0,dapengzhang0,readable,,xds: add hint of fault injection to injected failures | more readable
grpc/grpc-java,8102,https://github.com/grpc/grpc-java/pull/8102,"api, core: support zero copy into protobuf","A variant of #7330, taking the alternative codepath in protobuf.

Major changes of this PR: 
   - Added a `HasByteBuffer` API that allows the marshaller to access the backing ByteBuffers directly (cherry-picked from #7330).
   - Added a `Detachable` API that allows the application to take over the ownership of underlying buffers and close them later.

The new approach is to let the application implement a marshaller that passes ByteBuffers as immutable to protobuf:
   - Wrap ByteBuffers to ByteString with [UnsafeByteOperations](https://github.com/protocolbuffers/protobuf/blob/bc45f92262a8e0e6e1ab7302c72a380eb0346f8e/java/core/src/main/java/com/google/protobuf/UnsafeByteOperations.java#L97)
   - [Concatenate](https://github.com/protocolbuffers/protobuf/blob/bc45f92262a8e0e6e1ab7302c72a380eb0346f8e/java/core/src/main/java/com/google/protobuf/ByteString.java#L588) them as a RopeByteString
   - [Turn it to CodedInputStream](https://github.com/protocolbuffers/protobuf/blob/bc45f92262a8e0e6e1ab7302c72a380eb0346f8e/java/core/src/main/java/com/google/protobuf/RopeByteString.java#L605)

This requires buffers to be alive all the time through the proto messages used by the application (compared to the normal codepath that makes a copy of the bytes when parsed to proto messages and recycle buffers right after). The application is responsible for managing the lifetime of underlying buffers. An example marshaller that enables zero-copy codepath can be something like the following:

```java
  class ZeroCopyMarshaller<T extends Message> implements PrototypeMarshaller<T> {
    private IdentityHashMap<T, InputStream> unclosedStreams = new IdentityHashMap<>();
    private final Parser<T> parser;

    ZeroCopyMarshaller(T defaultInstance) {
      parser = (Parser<T>) defaultInstance.getParserForType();
    }

    // ... ...

    @Override
    public T parse(InputStream stream) {
      CodedInputStream cis = null;
      boolean streamDetached = false;
      if (stream instanceof Detachable) {
        stream = ((Detachable) stream).detach();
        streamDetached = true;
      }
      try {
        if (stream instanceof KnownLength) {
          if (stream instanceof HasByteBuffer
              && ((HasByteBuffer) stream).byteBufferSupported()) {  // fastest path, zero copy
            List<ByteString> byteStrings = new ArrayList<>();
            stream.mark(stream.available());  // retain bytes in ByteBuffers so that calling skip() won't throw bytes away
            while (stream.available() != 0) {
              ByteBuffer buffer = ((HasByteBuffer) stream).getByteBuffer();
              byteStrings.add(UnsafeByteOperations.unsafeWrap(buffer));
              stream.skip(buffer.remaining());
            }
            stream.reset();
            cis = ByteString.copyFrom(byteStrings).newCodedInput();
          } else {
            // slightly slower path, copy into a byte array
            // see ProtoLiteUtils#MessageMarshaller
            // ...
          }
        }
      } catch (IOException e) {
        throw new RuntimeException(e);
      }
      if (cis == null) {  // slowest path, via InputStream read
        cis = CodedInputStream.newInstance(stream);
      }

      // ... ...

      T message;
      try {
        message = parseFrom(cis);
      } catch (InvalidProtocolBufferException ipbe) {
        throw Status.INTERNAL.withDescription(""Invalid protobuf byte sequence"").withCause(ipbe).asRuntimeException();
      }
      if (streamDetached) {
        unclosedStreams.put(message, stream);
      }
      return message;
    }

    private T parseFrom(CodedInputStream stream) throws InvalidProtocolBufferException {
      T message = parser.parseFrom(stream);
      try {
        stream.checkLastTagWas(0);
        return message;
      } catch (InvalidProtocolBufferException e) {
        e.setUnfinishedMessage(message);
        throw e;
      }
    }

    // Application calls this method when no longer need the message.
    public void releaseMessage(T message) {
      InputStream stream = unclosedStreams.get(message);
      if (stream != null) {
        try {
          stream.close();
        } catch (IOException e) {
          throw new RuntimeException(e);
        }
        unclosedStreams.remove(message);
      }
    }
  }
```
",2021-05-14T21:45:03Z,voidzcy,voidzcy,"readable, easier to read",@ejona86 Updated. PTAL and let me know if anything I am misunderstanding. Thanks you! | Updated descriptions in this PR and ExperimentalApi links in the code. Please let me know if any doc needs improvement. Thanks for the review! | Thank you for the PR! My early benchmark shows that this can save ~20% of cpu time when used in the GCS benchmark client. (link),"Add mark&reset methods and canUseByteBuffer&getByteBuffer methods to the interface. | Default implementations. | Wire new methods for forwarder | Support mark&reset and retrieving content via ByteBuffer for netty. | Implementation for composite. | Define interface for accessing readable content via ByteBuffers. | Implement mark&reset for simple readable buffers. | Use HasByteBuffer interface for accesing input stream's backing ByteBuffer. | Eliminate the length argument for retrieving the ByteBuffer. | Do no require netty buffer to be direct from API's perspective. | Use Deque operations to avoid unncessary moves. | Make a list of ByteBuffers up-front instead of a running iterator. | Add getByteBufferSupported method for HasByteBuffer so that it can be used for okhttp as well. | It's not necessary to implement getByteBuffer for ByteReadbaleBufferWrapper. | Add test coverage for mark&reset and getByteBuffer for generic ByteBuffer. | Add test coverage for netty's special get NIO bytebuffer operation. | Skip test for operations not supported by okhttp. | Add test coverage for BufferInputStream with getByteBuffer operation. | Add test using a known-length input stream with getByteBuffer operation for protobuf parse. | Modify test method name. | Add test coverage for mark&reset and getByteBuffer for CompositeReadableByteBuffer. | Add getByteBuffer support for ByteReadableBufferWrapper. | Only pull ByteBuffers when message is large. | Run ByteBuffer codepath only in Java 9+. | Slight improvement for avoiding array creation if not necessary. | Merge branch 'master' of github.com:grpc/grpc-java into impl/zero_copy_into_protobuf_2 | Change ReadableBuffer#canUseByteBuffer to hasByteBuffer. | Removed unnecessary reset. | Simplify checking runtime java version. | Add ExperimentalApi annotation. | Rename ReadableBuffer#hasByteBuffer to getByteBufferSupported. | Merge branch 'master' of github.com:grpc/grpc-java into impl/zero_copy_into_protobuf_3 | Revert changes for MessageMarshaller. | Add Retainable interface that allows taking over resource ownership and prevent being closed too early. | Make BufferInputStream implements Retainable. Its close() method becomes a no-op if the underlying resource is retained. | Remove no longer needed constructors. | Change return type to be more specific. | Restore optimizations for avoid allocating new buffer wrappers. | Optimize by allocating rewindable buffer deque lazily. | Change to Detachable API, which makes the original InputStream behaves like being exhausted after being detached. | Change naming for getByteBufferSupported(). | Eliminate generics, use Object and casts instead. | Return Detachable instead of Object. | Hook BufferInputStream's markSupported() with underlying buffers. | Update Detachable interface definition, make it more specific to InputStream. | Replace the internal buffer of BufferInputStream with an empty buffer after being detached. Further operations on the detached BufferInputStream just delegate to the empty buffer. | Add ExperimentalApi link. | Update Javadoc for Detachable."
grpc/grpc-java,7841,https://github.com/grpc/grpc-java/pull/7841,xds: parse HttpFault filter from LDS/RDS response,"Parse [HTTPFault](https://github.com/envoyproxy/envoy/blob/18db4c90e3295fb2c39bfc7b2ce641cfd6c3fbed/api/envoy/extensions/filters/http/fault/v3/fault.proto#L57) from 4 possible places:

- HttpFilters in HttpConnectionManager in LDS response
  - `EnvoyProtoData.HttpFault` will be a field of `LdsUpdate`
- TypedPerFilterConfigMap in VirtualHost
  - `EnvoyProtoData.HttpFault` will be a field of `EnvoyProtoData.VirtualHost`
- TypedPerFilterConfigMap in Route
  - `EnvoyProtoData.HttpFault` will be a field of `EnvoyProtoData.Route`
- TypedPerFilterConfigMap in ClusterWeight
  - `EnvoyProtoData.HttpFault` will be a field of `EnvoyProtoData.ClusterWeight`

Design doc: https://github.com/grpc/proposal/pull/201",2021-01-28T04:45:19Z,dapengzhang0,dapengzhang0,"readable, understandable, easier to read",,xds: parse HttpFault filter from LDS/RDS response | add tests | add missing fields maxActiveFaults etc | use Status.fromCodeValue | refactor decodeFaultFilterConfig | use explicit boolean for headerDelay and headerAbort | parse regardless of feature flag
grpc/grpc-java,7674,https://github.com/grpc/grpc-java/pull/7674,Move multiple-port ServerImpl to NettyServer,"This is the first step towards #6641 , a deadlock issue at ServerImpl.start().
It makes ServerImpl to have a single transport server, and this single transport server (NettyServer) will bind to all listening addresses during bootstrap, as supported by Netty 4.x.",2021-01-05T21:24:17Z,YifeiZhuang,YifeiZhuang,"readable, easier to read","The committers are authorized under a signed CLA.✅  Yifei Zhuang (744ffde) | @YifeiZhuang The way you merge your PR with master head is not correct because the diff does not only show your change. The right way should be git pull https://github.com/grpc/grpc-java.git master.  I suggest do the following to correct it:

git revert [last commits of other authors]
git pull https://github.com/grpc/grpc-java.git master
[If there are any merge conflicts, resolve them manually, then git add -u then git commit]
git push <your_repo> <your_branch>","Move multiple port transport servers to NettyServer | execute bind in bossExecutor | Revert ""netty: create adaptive cumulator"" (#7669)

This reverts commit 729175c783b3d565d6aee21b456bf494f3d6c530. | xds: fix the new server API for ServerXdsClient (#7666) | api,core: interceptor-based config selector (#7610)

 Interceptor-based config selector will be needed for fault injection.

Add `interceptor` field to `InternalConfigSelector.Result`. Keep `callOptions` and `committedCallback` fields for the moment, because it needs a refactoring to migrate the existing xds config selector implementation to the new API. | xds: fix text in the readme and the comment about the --secure flag (#7676) | interop-testing: update proto generated service files (#7682) | Fix nettyServer get listen addresses when not bound, bind exception cause, and serverImpl shutdown | Revert ""interop-testing: update proto generated service files (#7682)""

This reverts commit be578f49ef6fa5b78256a0ce127da9a2572519cd. | Revert ""xds: fix text in the readme and the comment about the --secure flag (#7676)""

This reverts commit e1afcc4889d2e26ae5bef3eec81f39a51349e55f. | Revert ""api,core: interceptor-based config selector (#7610)""

This reverts commit 67bef1457e9e7d629b42fdf1fecf34d443f1cc9b. | Revert ""xds: fix the new server API for ServerXdsClient (#7666)""

This reverts commit ac92d19e6e895fa1a0dcd8054292d4f871cfd205. | Revert ""Revert ""netty: create adaptive cumulator"" (#7669)""

This reverts commit e8042444c97ec89294d442273036858952b476c3. | Merge branch 'master' of https://github.com/grpc/grpc-java | Merge branch 'master' of https://github.com/grpc/grpc-java | bind multiple partial failure | use single transportServer in ServerImpl instead of a list | Merge branch 'master' of https://github.com/grpc/grpc-java | fix close channel group on paritial failures | fix remove channel group channel close listener | fix indentation, add listenSocketStatsList null check | fix listenSocketStatsList read thread-unsafe, remove removing from channelz in shutdown | fix thread unsafe listenSocketStatsList | fix thread unsafe listenSocketsList | fix thread unsafe listenSocketStatsList | fix thread unsafe listenSocketStatsList"
grpc/grpc-java,12618,https://github.com/grpc/grpc-java/pull/12618,s2a: Improve S2AStubTest clarity and performance,"Refactors S2AStubTest to address issues in #12581:
- Rename `send_receiveOkStatus` to test successful responses using FakeWriter
- Add `send_withUnavailableService_throwsDeadlineExceeded` with 1-second deadline for timeout testing

- as is

<img width=""484"" height=""351"" alt=""image2"" src=""https://github.com/user-attachments/assets/24320f43-32fe-4305-9daf-29d2570ecaf1"" />

- to be
<img width=""596"" height=""404"" alt=""image"" src=""https://github.com/user-attachments/assets/6f8b9c4c-ca6c-46ae-8e99-d405a504308e"" />

Reduces test execution time from 20 seconds to 1 second while clarifying test intent.",2026-02-05T18:19:37Z,kannanjgithub,Gyuhyeok99,clarity,The committers listed above are authorized under a signed CLA.✅ login: Gyuhyeok99 / name: 황규혁 (bd2125b) | /gcbrun,"s2a: Improve S2AStubTest clarity and performance

Rename send_receiveOkStatus to test successful response handling using FakeWriter instead of timing out with a real connection.

Add send_withUnavailableService_throwsDeadlineExceeded test with a 1-second deadline to verify timeout behavior without the 20-second wait."
grpc/grpc-java,12607,https://github.com/grpc/grpc-java/pull/12607,core: clarify dns javadoc/test about trailing path segments,,2026-01-13T21:40:04Z,jdcormie,jdcormie,clarity,"Summary of Changes
Hello @jdcormie, I'm Gemini Code Assist1! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed!
This pull request focuses on enhancing the clarity and future maintainability of the DNS name resolution component. It refines the documentation to precisely define the expected URI structure for DNS targets and marks an existing test with a TODO to address a known behavior regarding trailing path segments, signaling a potential area for future enforcement.
Highlights

Javadoc Clarification: The Javadoc for DnsNameResolverProvider has been updated to explicitly state that the target URI must contain ""exactly one path segment"" for DNS resolution, providing clearer guidance on URI formatting.
Test Annotation: A TODO comment has been added to the newNameResolver_toleratesTrailingPathSegments test, highlighting that trailing path segments should ideally be forbidden and noting that the test currently demonstrates consistent behavior across different newNameResolver() overloads despite this.

🧠 New Feature in Public Preview: You can now enable Memory to help Gemini Code Assist learn from your team's feedback. This makes future code reviews more consistent and personalized to your project's style. Click here to enable Memory in your admin console.

Using Gemini Code Assist

The full guide for Gemini Code Assist can be found on our documentation page, here are some quick tips.
Invoking Gemini
You can request assistance from Gemini at any point by creating a comment using either /gemini <command> or @gemini-code-assist <command>. Below is a summary of the supported commands on the current page.



Feature
Command
Description




Code Review
/gemini review
Performs a code review for the current pull request in its current state.


Pull Request Summary
/gemini summary
Provides a summary of the current pull request in its current state.


Comment
@gemini-code-assist
Responds in comments when explicitly tagged, both in pull request comments and review comments.


Help
/gemini help
Displays a list of available commands.



Customization
To customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a .gemini/ folder in the base of the repository. Detailed instructions can be found here.
Limitations & Feedback
Gemini Code Assist may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with 👍 and 👎 on @gemini-code-assist comments. If you're interested in giving your feedback about your experience with Gemini Code Assist for Github and other Google products, sign up here.
You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the Gemini Code Assist IDE Extension.

Footnotes


Review the Generative AI Prohibited Use Policy, and learn how to configure Gemini Code Assist in GitHub here. Gemini can make mistakes, so double check it and use code with caution. ↩ | (We're working on getting Gemini Code Assist to settle down. I'm quite disappointed, honestly; it wrote far more than the PR itself, so was obviously a net negative with its defaults and simple PRs.)

Haha yeah it does seem quite chatty. I've had great results with the critique integration though so I'm optimistic you'll turn it net positive in github. | I'm not holding my breath. I saw the value in the critique one. Even if it was never happy, it pointed out things that could be improved and was terse. I'm going to let others (in other repos) invest the time to get this one useful.",core: clarify dns javadoc/test about trailing path segments
grpc/grpc-java,12203,https://github.com/grpc/grpc-java/pull/12203,xds: ORCA to LRS propagation changes,"Implements gRFC A85 (https://github.com/grpc/proposal/pull/454).
",2025-10-13T07:57:31Z,shivaspeaks,shivaspeaks,clarity,,ORCA to LRS propagation changes | added BackendMetricPropagationTest and addressed comments | added BackendMetricPropagationTest and addressed comments | format | add test | merge from master | use env variable in recordBackendLoadMetricStats | address comments | Merge branch 'master' into lrs-custom-metric-changes | merge from master and refactoring | merge from master | resolve merge conflicts
grpc/grpc-java,12377,https://github.com/grpc/grpc-java/pull/12377,A68 random subsetting (part 1),"implementing gRFC A65 [grpc/proposal/pull/423](https://github.com/grpc/proposal/pull/423)

This change contains:
1. ~~Relocation of `XxHash64` library, so it can be shared between `util` and `xds` projects. Proposed source directory is: `third-party/zero-allocation-hashing`.~~ Usage of `murmur3_128` hashing algorithm from Guava library.  
2. Implementation of `RandomSubsettingLoadBalancer` and `RandomSubsettingLoadBalancerProvider` classes and integration into the `util` project.
3. ~~Implementation of xDS conversion function for `random_subsetting` LB policy and as well new envoy proto message.~~ Since envoy extensions does not support `random_subsetting` LB policy yet, xDS related changes will be introduced later. Envoy PR [here](https://github.com/envoyproxy/envoy/pull/41758).",2025-11-05T21:37:54Z,ejona86,Zgoda91,clarity,"@ejona86 - PR updated | CC @s-matyukevich, @joybestourous | @ejona86 - PR updated. | @ejona86 - pushed changes. However, I do have two questions, which I have posted as answers to your comments. | /gcbrun","Moved `XxHash64` library to enable shared use across projects | Implemented random_subsetting LB policy | Implemented random_subsetting converter for xDS | Optimized seed lifetime | Optimized memory allocations for arrays | Changed visibility of LB to package-private | Revert ""Moved `XxHash64` library to enable shared use across projects""

This reverts commit 8d349df5d3108b4e13f35a7508ca7b94c331b8e4. | Changed hashing function to `murmur3_128` | Removed leftover dependency from `BUILD.bazel` | Changed hash code comparator | Improved test assertion to get better failure message | Excluded new LB from javadoc | Improved `subsetSize` parsing | Adjusted expectation for flaky test | Changed way of storing hashes | Improved config builder | Updated xDS converting function | Updated status for subset size parsing failure | Added deterministic seed setup for LB tests | Reverted xDS changes | Reused existing constructor | Fixed status codes | Suffix name with _experimental | Suffix name with _experimental in tests, too"
grpc/grpc-java,12259,https://github.com/grpc/grpc-java/pull/12259,stub: use the closedTrailers in StatusException,"Fixes #12256 

cc: @benjaminp ",2025-08-06T06:54:34Z,shivaspeaks,shivaspeaks,clarity,,stub: use the closedTrailers in StatusException | use atomic reference to hold both status and metadata | check not null for status in CloseState
grpc/grpc-java,12091,https://github.com/grpc/grpc-java/pull/12091,Rename PSM interop fallback test suite to light,"This is part of a cross-repository change to generalize the fallback test suite to support other tests, and to change the name for clarity. See also https://github.com/grpc/psm-interop/pull/179.",2025-05-22T22:43:26Z,ejona86,murgatroid99,clarity,"Doing this atomically makes it seem like we hate ourselves. But whatever. cl/762145914 looks to be the Kokoro rename.
Windows build is messed up because cl/761987289 was merged but not #11961
CC @kannanjgithub",Rename PSM interop fallback test suite to light
grpc/grpc-java,12186,https://github.com/grpc/grpc-java/pull/12186,A75 Aggregate cluster fixes,,2025-07-29T12:36:39Z,kannanjgithub,kannanjgithub,clarity,,"Add Docker fiels for xds example server and client. | Merge branch 'grpc:master' into master | Merge branch 'grpc:master' into master | Merge branch 'grpc:master' into master | Merge branch 'grpc:master' into master | Merge branch 'grpc:master' into master | in-progress changes. | in-progress changes. | in-progress changes. | in-progress changes. | in-progress changes. | in-progress changes. | Revert ""Add Docker fiels for xds example server and client.""

This reverts commit ed5072e1481c2f355e4f8b2e3eab92f1a7a7b7cd. | Changes | whitespace changes nightmares | whitespace changes nightmares | whitespace changes nightmares | Replace clusters array with single cluster in ClusterResolverLB | Review comments, excepting the child cluster type change handling. | nit | Handle cluster type changes by using Graceful switch load balancer as a delegate in CdsLoadbalancer2 | Fix test | Review comments. | Revert indendation changes done by IDE in unchanged lines. | A map for ClusterState is not required in ClusterResolverLoadBalancer which now holds only a single cluster's state. | Reinstantiate delegate after shutdown. | Merge branch 'master' into a75-fixes2 | Fix problem after merge. | Fix problem after merge. | Fix style errors."
grpc/grpc-java,12050,https://github.com/grpc/grpc-java/pull/12050,census: expose client interceptors,Helps resolve https://b.corp.google.com/issues/391196954,2025-07-01T15:14:28Z,AgraVator,AgraVator,clarity,,census: expose client interceptors | census: expose server tracer factories | census: use no arg methods for ClientInterceptors and StreamFactories | census: introduces builder pattern for stats and tracing | census: introduces builder pattern for stats and tracing
grpc/grpc-java,11045,https://github.com/grpc/grpc-java/pull/11045,core: Improve clarity of RpcProgress meanings,,2024-03-27T23:11:26Z,ejona86,ejona86,clarity,,core: Improve clarity of RpcProgress meanings
grpc/grpc-java,11627,https://github.com/grpc/grpc-java/pull/11627,xds: Spiffe Trust Bundle Support,"Enables verification of SPIFFE based identities using SPIFFE trust bundles.
[gRFC](https://github.com/markdroth/proposal/blob/spiffe/A87-mtls-spiffe-support.md)",2024-11-08T05:03:15Z,ejona86,erm-g,clarity,,Initial commit | Failing tests fix | Failing tests fix | Simple happy path | Test updates for FileWatcher | Test enhancements for FileWatcher Spiffe | Test enhancements for XdsX509TrustManager | Delegates refactor | Factory tests | checkClientTrusted test | mTls | style fix | style fix | style fix | style fix | debugging delay | Add initial loading | full mtls with spiffe | Merge branch 'master' into spiffeE2E | exception tests mtls with spiffe | Merge remote-tracking branch 'origin/spiffeE2E' into spiffeE2E | typo during merge | merge fix - adjust default params | init of FileWatcherCertificateProvider includes initial load of the certs | CertificateProvider includes init() method to synchronously load the deps | Address PR comments | style fix | acceptedIssuers fix | Address PR comments | Address PR comments | Style fix | Style fix | Address PR comments | typof fix | get rid of io in netty event loop | style fix | Address PR comments | Updated README | Make `ca_certificate_file` optional if `spiffe_trust_bundle_map_file` is present | Style fix | Test fix | Introduce GRPC_EXPERIMENTAL_SPIFFE_TRUST_BUNDLE_MAP flag | Style fix | Style fix | Style fix | Workaround for XdsSecurityClientServerTest | Style fix | Address PR comments | Style fix | Style fix
grpc/grpc-java,10804,https://github.com/grpc/grpc-java/pull/10804,xds: (minor) address Java Code Clarity recommendations in CSDS,"Redoes https://github.com/grpc/grpc-java/pull/8028
Ref cl/365582959",2024-01-08T20:30:47Z,sergiitk,sergiitk,clarity,,"xds: (minor) address Java Code Clarity recommendations in CSDS

Redoes https://github.com/grpc/grpc-java/pull/8028
Ref cl/365582959 | Address feedback and checkstyle"
grpc/grpc-java,11098,https://github.com/grpc/grpc-java/pull/11098,"minor: remove the unnecessary final,static",sele expansion,2024-04-18T22:26:01Z,ejona86,laglangyue,clarity,"The committers listed above are authorized under a signed CLA.✅ login: laglangyue / name: Laglangyue  (c5abab9, 46ea88e, dfdff9d) | Thanks for your review.
Do you think we need just one PR to clean up the diamond for our repository additional?
@ejona86 @sergiitk | @laglangyue, updating examples in a single PR is fine, and can make them feel more modern. But as I said before, I don't think we want to make such changes everywhere because that is a purely stylistic change.","minor: remove the unnecessary final,static | revert RouteGuideServer | revert diamond"
grpc/grpc-java,10674,https://github.com/grpc/grpc-java/pull/10674,xDS: implement ADS stream flow control mechanism,"fix https://github.com/grpc/grpc/issues/34099


@ejona86  early feedback about the overall shape would be great, tests needs to be cleaned up.

",2023-12-16T19:17:03Z,YifeiZhuang,YifeiZhuang,clarity,,"add flow control | sync context | remove sync context in resolver | remove sync context in cds | Revert ""sync context""

This reverts commit b1f62e57274b71bbadae2c50a5aad97fc476dd4f. | pass in sync context | fix style | fix eric's 1 coment | local update | fix and test | executor, big 1 | fix | add test | minor fix"
grpc/grpc-java,10623,https://github.com/grpc/grpc-java/pull/10623,dual stack: generic health checking,"go/grpc-java-generic-health-check


This change has health checking consumer (new pick first) to install a listener through and health checking producer (outlier detection and client health checking) producing health checks. Health notification chain is built reusing the previous connectivity state chain.


#### New Pick First
Pickfirst installs the health listener, but is capable of detecting when no health checking producer is installed in the system, i.e. when the pick first is top level LB policy, not petiole. In that case, it sets health status to be READY so that health system is no-op.


cc. @ejona86 @larry-safran ",2023-12-12T01:49:56Z,YifeiZhuang,YifeiZhuang,clarity,Is there a design doc for this? | I only merged the health producer part of the change so far. The new pick first change is coming soon.,"initial | Merge remote-tracking branch 'upstream/master' into health-experiment | fix client health | pick first leaf policy test | Merge branch 'master' of https://github.com/grpc/grpc-java into health-experiment | add env variable, health checking pick first lb provider in multiChildLbPolicy | use a helper to simplify | fix env variable, fix test client health check, use statelistener as health listener
add health producer ut | outlier test | fix pickfirst | Merge branch 'master' of https://github.com/grpc/grpc-java into health-experiment | minor fixes | root health producer listener auto notification, health producer key is boolean, parameterized test | kiss | new pick first with health check | Merge remote-tracking branch 'upstream/master' into health-experiment | minor fix style | fix format | Merge remote-tracking branch 'upstream/master' into health-experiment | fix | fix | Merge branch 'master' of https://github.com/grpc/grpc-java into health-experiment | style change | style change"
grpc/grpc-java,10481,https://github.com/grpc/grpc-java/pull/10481,Mark MultiChildLoadBalancer as Internal. ,"Cannot move to the intern package because of its use of classes in the util package.
There is precedence for using the @Internal annotation as it is being done by outlier detection and round robin.  ",2023-08-16T00:33:23Z,larry-safran,larry-safran,clarity,"Cannot move to the intern package because of its use of classes in the util package.

Yeah, ForwardingLoadBalancerHelper should really be in io.grpc.

There is precedence for using the @internal annotation as it is being done by outlier detection and round robin.

They are essentially doing it wrong as well. The only @Internal things should have internal in their name, or be Providers. We do that for clarity, and also because we have Blaze globs internally that prevent users from using our internal APIs without permission (because people kept using them, and then surprised when it broke because the person that did it isn't on the team any more).
RoundRobinLoadBalancer is public as a hack to share code with WRR, which had to be implemented very quickly. There should be some code sharing, but we haven't gotten that nice yet (and this base class might actually replace what was done there).
OutlierDetectionLoadBalancer looks to be public because the config is used from xds. That should just use json and the Provider.",Mark MultiChildLoadBalancer as Internal.  Cannot move to the internal package because of its use of classes in the util package. | Exclude MultiChildLoadBalancer from javadoc generation. | Fix javadoc creation. | Fix javadoc creation. | Revert interop-testing no-javadoc configuration
grpc/grpc-java,9626,https://github.com/grpc/grpc-java/pull/9626,core: fix RejectedExecutionException in  Retriable Stream,"fix https://github.com/grpc/grpc-java/issues/9547

### revision 1
Add big negative integer to pending stream count when cancelled. The count is used to delay closing master listener until streams fully drained.
Increment pending stream count before creating one. The count is also used to indicate callExecutor is safe to be used. New stream not created if big negative number was added, i.e. stream cancelled. New stream is created if not cancelled, callExecutor is safe to be used, because cancel will be delayed.


### revision 2
1.  incrementing pending stream count only when cancel is not called, otherwise no op. This is to prevent the count never goes to `Integer.MIN_VALUE`.
2. create new streams (retry, hedging) is moved to the main thread, before callExecutor calls drain.
3. minor refactor the masterListener.close() scenario. 
",2022-11-22T21:04:07Z,YifeiZhuang,YifeiZhuang,clarity,"@ejona86 , do you want to approve? So that we can try import next monday. global tap seems fine.",core: fix race condition cancel and drain in retriable stream | Merge branch 'master' of https://github.com/grpc/grpc-java into inflight-substream-counter | count stream within createSubStream | fix
grpc/grpc-java,9873,https://github.com/grpc/grpc-java/pull/9873,xds: add weighted round robin LB policy support,implementation design doc: [go/grpc-java-wrr](http://goto.google.com/grpc-java-wrr),2023-02-27T18:34:52Z,YifeiZhuang,YifeiZhuang,clarity,"Can you rebase this on master because #9875 is merged? I would have looked at just the commits after that other PR, but there were so many; in the future you could squash commits that were just used during development before creating the PR.","refactor round robin LB | rename abstract* | refactor round robin LB | rename abstract* | temp: add weightedroundrobinimpl
1. to merge orca api remove listener
2. to merge metric report api rqs
3. to settle the package | add weighted round robin picker and scheduler | comments, and add afterSubchannelUpdate | format | move abstraction to composition | remove listener | add update timer to LB , not in picker
RoundRobinLB picker issue, e.g.
size = 5, index = 4
pick1: i = index = 5
pick2: i = index = 6
pick1 : oldi = 5, i = 0, index = 0
pick2: oldi = 6, i = 1, index not updated = 0.
pick1 return subchanel[0], pick2 return subchannel[1]
next time, it still return subchannel[1], it gets picked more often; | use original round robin wl | add subchannel listener | add test | Merge branch 'master' of https://github.com/grpc/grpc-java into wrr-impl | add more tests | add provider test | fix current picker | remove virtual time, change comment | fix avg weight | Merge branch 'master' of https://github.com/grpc/grpc-java into wrr-impl | Merge branch 'master' of https://github.com/grpc/grpc-java into wrr-impl | add env variable | Merge branch 'master' of https://github.com/grpc/grpc-java into wrr-impl | parse wrr proto | add test scheduler | fix comments, timer, volatile, etc | bazel checksum, and use ticker | infTime = nanoTime() + MAX_VALUE | minor fix"
grpc/grpc-java,9025,https://github.com/grpc/grpc-java/pull/9025,xds: Improve code clarity by removing Unnecessary fully qualified names and using Immutable interface types.,"1. Unnecessary fully qualified names
Currently in XdsCredentialsRegistry, the child classes are referred by
their fully qualified names i.e.
'io.grpc.xds.internal.GoogleDefaultXdsCredentialsProvider' instead of
importing GoogleDefaultXdsCredentialsProvider and just using
GoogleDefaultXdsCredentialsProvider.class.

2. Use immutable interfaces instead of the generic collection interface
   i.e. ImmutableMap instead of just Map in the unit tests.

These improvements are related to changes made in #8924.

@ejona86 ",2022-04-04T14:19:54Z,ejona86,anicr7,clarity,,"xds: Improve code clarity by removing Unnecessary fully qualified names
and using Immutable interface types.

1. Unnecessary fully qualified names
Currently in XdsCredentialsRegistry, the child classes are referred by
their fully qualified names i.e.
'io.grpc.xds.internal.GoogleDefaultXdsCredentialsProvider' instead of
importing GoogleDefaultXdsCredentialsProvider and just using
GoogleDefaultXdsCredentialsProvider.class.

2. Use immutable interfaces instead of the generic collection interface
   i.e. ImmutableMap instead of just Map.

These improvements are related to #8924. | xds: Revert the change to return ImmutableMap in XdsCredentialsRegistry"
grpc/grpc-java,8695,https://github.com/grpc/grpc-java/pull/8695,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity,This change will also need to be backported to all branches containing xds-k8s.,2021-11-13T01:40:07Z,sanjaypujare,sanjaypujare,clarity,,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity
grpc/grpc-java,8702,https://github.com/grpc/grpc-java/pull/8702,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (backport 1.42.x) (#8695),,2021-11-15T18:30:01Z,sanjaypujare,sanjaypujare,clarity,,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (#8695)
grpc/grpc-java,8703,https://github.com/grpc/grpc-java/pull/8703,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (backport 1.35.x) (#8695),,2021-11-15T18:29:43Z,sanjaypujare,sanjaypujare,clarity,,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (#8695)
grpc/grpc-java,8704,https://github.com/grpc/grpc-java/pull/8704,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (backport 1.34.x) (#8695),,2021-11-15T18:29:21Z,sanjaypujare,sanjaypujare,clarity,,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (#8695)
grpc/grpc-java,8701,https://github.com/grpc/grpc-java/pull/8701,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (backport 1.41.x) (#8695),,2021-11-15T18:30:48Z,sanjaypujare,sanjaypujare,clarity,,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (#8695)
grpc/grpc-java,8700,https://github.com/grpc/grpc-java/pull/8700,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (backport 1.40.x) (#8695),,2021-11-15T18:31:04Z,sanjaypujare,sanjaypujare,clarity,,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (#8695)
grpc/grpc-java,8698,https://github.com/grpc/grpc-java/pull/8698,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (backport 1.38.x) (#8695),,2021-11-15T18:31:36Z,sanjaypujare,sanjaypujare,clarity,,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (#8695)
grpc/grpc-java,8699,https://github.com/grpc/grpc-java/pull/8699,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (backport 1.39.x) (#8695),,2021-11-15T18:31:23Z,sanjaypujare,sanjaypujare,clarity,,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (#8695)
grpc/grpc-java,8697,https://github.com/grpc/grpc-java/pull/8697,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (backport 1.37.x) (#8695),,2021-11-15T18:31:51Z,sanjaypujare,sanjaypujare,clarity,,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (#8695)
grpc/grpc-java,8696,https://github.com/grpc/grpc-java/pull/8696,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (backport 1.36.x) (#8695),,2021-11-15T18:32:05Z,sanjaypujare,sanjaypujare,clarity,,buildscripts: rename xds-k8s to psm-security as part of tech-debt cleanup and name clarity (#8695)
grpc/grpc-java,8715,https://github.com/grpc/grpc-java/pull/8715,xds: add end-2-end test with java control plane,"Moving https://github.com/grpc/grpc-java/pull/8618 to xds package, because it seems a better destination.
Previously reverted due to wrongly using shaded dependency that breaks google3: https://github.com/grpc/grpc-java/pull/8656
cc. @dapengzhang0 

Latest structure:
added a java control plane for xds tests end-to-end.
The FakeControlPlaneService manages full sets of xds resources. Use `setXdsConfig()` method to update the latest xds configurations; the method can be called anytime and multiple times dynamically. The fake control plane allows multiple clients connecting, delivers xds responses(for the data resources, or ACK/NACK) for the xds client requests.
The `FakeControlPlaneXdsIntegrationTest` only has one pingPong test case now. Other test case can be added in a similar way.


",2022-02-25T21:22:03Z,YifeiZhuang,YifeiZhuang,clarity,,"xds: add xds end to end interop test | Merge branch 'master' of https://github.com/grpc/grpc-java into xds_e2e_test | refactor | Merge branch 'master' of https://github.com/grpc/grpc-java into xds_e2e_test | fix | cleanup | find available port | refactor | Merge branch 'master' of https://github.com/grpc/grpc-java into xds_e2e_test | fix | Merge branch 'master' of https://github.com/grpc/grpc-java into xds_e2e_test | Merge branch 'master' of https://github.com/grpc/grpc-java into xds_e2e_test | fix | use xdsServerBuilder(0,...) | Merge branch 'master' of https://github.com/grpc/grpc-java into xds_e2e_test | Merge branch 'master' of https://github.com/grpc/grpc-java into xds_e2e_test | use 127.0.0.1 for local server | use wildcard domain in rds, get server listener socket address for eds | Merge branch 'master' of https://github.com/grpc/grpc-java into xds_e2e_test | fix"
grpc/grpc-java,8681,https://github.com/grpc/grpc-java/pull/8681,netty: Add system property to disable Connection header check,"A user has a proxy that is sending ""Connection: close"", which is against
the HTTP/2 spec, but will take time to fix.

Fixes #8674

CC @augi, @YifeiZhuang 

I'll backport this to v1.42.x. We're anticipating having a 1.42.1 release soon.",2021-11-09T17:26:19Z,ejona86,ejona86,clarity,,"netty: Add system property to disable Connection header check

A user has a proxy that is sending ""Connection: close"", which is against
the HTTP/2 spec, but will take time to fix.

Fixes #8674"
grpc/grpc-java,7877,https://github.com/grpc/grpc-java/pull/7877,Document that xds uses grpc-netty-shaded,"For clarity, also extract grpc-netty-shaded to a separate paragraph.

Closes: #7869.",2021-05-12T02:19:40Z,sanjaypujare,lepistone,clarity,"The committers are authorized under a signed CLA.✅  Leonardo Pistone (bfa4e7a) | @lepistone could you address the comments so we can merge the PR? Thanks | I removed the additional paragraph so I am now only adding a paragraph. This should address both comments.
Sorry it took so long and thanks for reviewing!",Document that xds uses grpc-netty-shaded
grpc/grpc-java,7803,https://github.com/grpc/grpc-java/pull/7803,api: Reword retry javadoc to make clear service config is source of configuration,"Multiple users have tried things like
`mcb.enableRetry().maxRetryAttempts(3)` and been confused when no
retries were performed. Providing a reference to the gRFC and
`defaultServiceConfig()` should greatly increase the clarity of how to
use the method.",2021-01-13T19:36:09Z,ejona86,ejona86,clarity,,"api: Reword retry javadoc to make clear service config is source of configuration

Multiple users have tried things like
`mcb.enableRetry().maxRetryAttempts(3)` and been confused when no
retries were performed. Providing a reference to the gRFC and
`defaultServiceConfig()` should greatly increase the clarity of how to
use the method."
grpc/grpc-java,11859,https://github.com/grpc/grpc-java/pull/11859,xds: Allow FaultFilter's interceptor to be reused across RPCs,"This is the only usage of PickSubchannelArgs when creating a filter's ClientInterceptor, and a follow-up commit will remove the argument and actually reuse the interceptors. Other filter's interceptors can already be reused.

There doesn't seem to be any significant loss of legibility by making FaultFilter a more ordinary interceptor, but the change does cause the ForwardingClientCall to be present when faultDelay is configured, independent of whether the fault delay ends up being triggered.

Reusing interceptors will move more state management out of the RPC path which will be more relevant with RLQS.

CC @sergiitk 

-----

The diff is sorta hard to follow, so it may be easier to see the shape of the code before and compare to it after. I don't believe there to be any behavior changes other than what I mentioned with ForwardingClientCall being present in extra cases, although the change is obviously not trivial. The only code I wrote was around `checkFault` and setting `delegate` after the `DeadlineInsightForwardingCall` creation; everything else should be moves.",2025-01-29T22:21:53Z,ejona86,ejona86,legibility,"The test is a good point. I didn't consider it at the time because this was split out from another PR which will remove the argument that provides the headers/metadata today. I agree it makes sense with this split out. But since that second part is coming now-ish and make it impossible to work the old way, a test won't do much.","xds: Allow FaultFilter's interceptor to be reused

This is the only usage of PickSubchannelArgs when creating a filter's
ClientInterceptor, and a follow-up commit will remove the argument and
actually reuse the interceptors. Other filter's interceptors can
already be reused.

There doesn't seem to be any significant loss of legibility by making
FaultFilter a more ordinary interceptor, but the change does cause the
ForwardingClientCall to be present when faultDelay is configured,
independent of whether the fault delay ends up being triggered.

Reusing interceptors will move more state management out of the RPC path
which will be more relevant with RLQS."
grpc/grpc-java,10354,https://github.com/grpc/grpc-java/pull/10354,"new pick first policy, architectural change","The [dual-stack support project](https://github.com/markdroth/proposal/blob/dualstack/A61-IPv4-IPv6-dualstack-backends.md?rgh-link-date=2023-06-29T18%3A36%3A24Z) will add Happy Eyeballs and change pick first to be a universal leaf LB policy. However, it details some architectural differences between C-core and Java/Go for the subchannel and pick first policy.

In Java and Go, pick first logic is implemented within the subchannel itself instead of inside the load balancer unlike C-core.

We will take this opportunity to bring gRPC to a more uniform architecture across implementations and write a new pick first policy. This is important so that different implementations do not continue to diverge as more features are implemented. 

This change will include creating a PickFirstLeafLoadBalancer which will contain the pick first logic, as well as redesigning some components such as backoffs and address updates. This will set us up nicely to implement Happy Eyeballs and use pick first as a universal leaf policy.

The new InternalSubchannel will be migrated later on once it is time to deprecate the current InternalSubchannel.",2023-08-14T22:23:40Z,tonyjongyoonan,tonyjongyoonan,"legibility, easier to read","This is getting close! I only had a quick glimpse of the test and I didn't find the test case about the state machine after READY. Once after READY and that connection is down, we will do the iteration again by creating new subchannels. Also sticky TF should be explicitly tested if not already covered.

Excited to finalize! Both of the cases you mentioned should be tested.

state machine after READY

We test this inside: recreate_shutdown_subchannel() as well as the way it interacts with address updates: updateAddresses_intersecting_idle(),  updateAddresses_disjoint_idle().

sticky TF should be explicitly tested

We test this here: success_from_transient_failure() as well as all the update address cases where ""transient_failure"" is in the name. Let me know if there's anything I missed. | cc. @larry-safran","static stride scheduler algorithm/class (currently unused)

fixed check issues | Merge pull request #2 from tonyjongyoonan/static-stride-scheduler-draft-1

static stride scheduler algorithm/class (currently unused) | Revert ""static stride scheduler algorithm/class (currently unused)"" | Merge pull request #3 from tonyjongyoonan/revert-2-static-stride-scheduler-draft-1

Revert ""static stride scheduler algorithm/class (currently unused)"" | internal subchannel experimental test | internal subchannel experimental | pick first load balancer experimental | pick first load balancer experimental test | adding logic | migrated pickfirstexperimentaltest | migrated pickfirstloadbalancerexperimental | migrated internalsubchannelexperimental | migrated internalsubchannelexperimentaltest | fixed update address logic, added simple tests for successful connections | fixed updateAddresses(), added basic disjoint test case | updateAddresses intersecting logic + test cases | backoff logic | fixed internal subchannel + test cases to reflect new architecture | fixed internal subchannel test cases | fully fixed and tested update address/connection logic | fixed small todos | test cases verify correct subchannel picked | added sticky transient failure logic, fixed mock backoff logic | channel creation fixed | new way of representing/storing subchannels | fixed and tested internal subchannel | update logic | set comparison logic is fixed | fixed and added most update test cases | added update address connection test cases | update addresses fully tested | fixed comments | fixed refresh name resolution and feedback | fixed imports | fixed style, completed update test cases | removed unused method | local build doesn't match github style checker | fixed iterate logic | fix test case | optimizations + removing unnecessary component | fixed shutdown logic | removed TF to IDLE, covered edge cases | feedback fix + optimizations + checking for TF with index | style fix | added tf --> idle | fixed concurrent modification | simplified request connection logic | update addresses properly enters transient failure | fixed feedback + optimizations | unnecessary import | style fix | test syntax fix | address update connecting out of backoff behavior | state map update properly | Delete InternalSubchannelExperimental.java | Delete InternalSubchannelExperimentalTest.java | fixed feedback, simplified address fetch logic | remove unused ds | new class to represent subchannel data | ready logic + tightened request connection logic | remove unnecessary imports | fixed feedback + cut verbose testing | unused state listeners | unit testing requestConnection() | removed unnecessary test"
grpc/grpc-java,12360,https://github.com/grpc/grpc-java/pull/12360,11246 :: Unexpected error when server expands a compressed message to learn it is too large ,Fixes : #11246 ,2025-11-03T09:19:04Z,kannanjgithub,vimanikag,easier to read,"It might be easier to test with a complete flow, #12428",11246 :: Unexpected error when server expands a compressed message to learn it is too large | 11246 :: Unexpected error when server expands a compressed message to learn it is too large | 11246 :: Unexpected error when server expands a compressed message to learn it is too large | 11246:: addressing the review comments. | 11246:: added the junit's for checked exception (StatusException) | 11246:: addressing the review comments | 11246:: addressing the review comments | 11246:: added the serialVersionUID or new static class to fix the PR check failures | 11246:: addressed the review comments excluding junit's. | 11246:: added the junit's for the latest changes. | 11246:: Updated the JUnit tests based on the recent review comments. | 11246:: added the JUnit tests based on the recent review comments. | 11246:: Addressing the latest review comments | 11246:: Addressing the latest review comments | 11246:: Addressed the review comments
grpc/grpc-java,11936,https://github.com/grpc/grpc-java/pull/11936,xds: Support filter state retention,"This PR adds support filter state retention in Java. The mechanism will be similar to the one described in [A83](https://github.com/grpc/proposal/blob/master/A83-xds-gcp-authn-filter.md#filter-call-credentials-cache) for C-core, and will serve the same purpose. However, the implementation details are very different due to the different nature of xDS HTTP filter support in C-core and Java.

### Filter instance lifecycle

#### xDS gRPC clients

New filter instances are created per combination of:

1.  `XdsNameResolver` instance,
2.  Filter name+typeUrl as configured in `HttpConnectionManager` (HCM) http_filters.

Existing client-side filter instances are shutdown:
-   A single filter instance -- when an LDS update contains HCM that is missing filter configuration for name+typeUrl combination of this instance.
-   All filter instances -- when watched LDS resource is missing from an LDS update.
-   All filter instances -- on name resolver shutdown.

#### xDS-enabled gRPC servers

New filter instances are created per combination of:
1. Server instance,
2. `FilterChain` name,
3. Filter name+typeUrl as configured in `FilterChain`'s HCM.http_filters.

Filter instances of Default Filter Chain is tracked separately per:
1. Server instance,
2. Filter name+typeUrl in default_filter_chain's HCM.http_filters.

Existing server-side filter instances are shutdown:
-   A single filter instance -- when an LDS update contains `FilterChain` with HCM.http_filters that is missing configuration for filter name+typeUrl.
-   All filter instances associated with the `FilterChain` -- when an LDS update no longer contains `FilterChain`'s name.
-   All filter instances -- when watched LDS resource is missing from an LDS update.
-   All filter instances -- on server shutdown.

### Related
- Part 1: #11883",2025-03-06T18:32:08Z,sergiitk,sergiitk,easier to read,"FYI @shivaspeaks @larry-safran | The committers listed above are authorized under a signed CLA.✅ login: sergiitk / name: Sergii Tkachenko  (837de81, ce2edde, c08d918, 6ad5f15, 9fda971, cffc7de) | /easycla
Apparently ""EasyCLA is broken at the moment; they are working on it"" | /easycla | /easycla | /easycla | For posterity, semi-related refactoring work and fixes:

#11886
#11930","xds: Support filter state retention

Introduces filter instance lifecycle.

**xDS gRPC clients**

New filter instances are created per combination of:
1. `XdsNameResolver` instance,
2. Filter name+typeUrl in HttpConnectionManager (HCM) http_filters.

Existing client-side filter instances are shutdown:
- A single a filter instance is shutdown when an LDS update contains
  HCM that is missing filter configuration for name+typeUrl
  combination of this instance.
- All filter instances when watched LDS resource is missing from
  an LDS update.
- All filter instances name resolver shutdown.

**xDS-enabled gRPC servers**

New filter instances are created per combination of:
1. Server instance,
2. FilterChain name,
3. Filter name+typeUrl in FilterChain's HCM.http_filters.

Filter instances of Default Filter Chain is tracked separately per:
1. Server instance,
2. Filter name+typeUrl in default_filter_chain's HCM.http_filters.

Existing server-side filter instances are shutdown:
- A single a filter instance is shutdown when an LDS update contains
  FilterChain with HCM.http_filters that is missing configuration for
  filter name+typeUrl.
- All filter instances associated with the FilterChain when an LDS
  update no longer contains FilterChain's name.
- All filter instances when watched LDS resource is missing from
  an LDS update.
- All filter instances on server shutdown. | Address feedback - round 1 | address feedback - round 2 | force easycla check | Merge branch 'master' into xds-filter-state | resolve conflicts with master"
grpc/grpc-java,11741,https://github.com/grpc/grpc-java/pull/11741,xds: Parsing xDS Cluster Metadata,Add Support for Audience Metadata Parsing in xDS Cluster Resource,2025-01-07T04:33:13Z,shivaspeaks,shivaspeaks,easier to read,"Note: I will add unit tests in the next commit after finalizing on main code. Please react on this comment (👍) if this looks good, and then I will proceed with unit tests. | @danielzhaotongliu",Parsing xDS Cluster Metadata | Moving AudienceMetadataParser to gcp auth filter and better error handling | Unit tests and review point fixed | Review points | Added ProtobufJsonConverterTest | fixing minor comments
grpc/grpc-java,11724,https://github.com/grpc/grpc-java/pull/11724,netty: Per-rpc call option authority verification against peer cert subject names,,2025-02-24T14:58:11Z,kannanjgithub,kannanjgithub,easier to read,"I don't know what this error is about:
java/netty/src/main/java/io/grpc/netty/ProtocolNegotiators.java:621:18: 'public' modifier out of order with the JLS suggestions. [ModifierOrder] | synchronized public boolean mayBeVerifyAuthority should have ""public"" first: public synchronized boolean mayBeVerifyAuthority.
JLS == Java Language Specification
The relevant part of the style guide:
https://google.github.io/styleguide/javaguide.html#s4.8.7-modifiers
But checkstyle also links to some useful things:
https://checkstyle.sourceforge.io/checks/modifier/modifierorder.html | Sending what I have. It seems dealing with lack of X509ExtendedTrustManager on android is going to be annoying.

I have done this change in the source files. Can we bypass the animal sniffer check for the tests, because in ProtocolNegotiatorsTest I'm mocking X509ExtendedTrustManager and I can't do this with reflecttion because java.lang.Class cannot be mocked. | Can we bypass the animal sniffer check for the tests

Yeah, that's not a problem. We don't run the netty/okhttp unit tests on Android. Only in interop-testing/src/main (it's tests don't run on Android either) do you need to be more careful, like with AbstractInteropTest. | Can we bypass the animal sniffer check for the tests

Yeah, that's not a problem. We don't run the netty/okhttp unit tests on Android. Only in interop-testing/src/main (it's tests don't run on Android either) do you need to be more careful, like with AbstractInteropTest.

How to disable animal sniffer for tests? I see there is an excludeDependencies but the supported artifact patterns don't have groupId:artifactId::scope that we can use to exclude the test scope. | Can we bypass the animal sniffer check for the tests

Yeah, that's not a problem. We don't run the netty/okhttp unit tests on Android. Only in interop-testing/src/main (it's tests don't run on Android either) do you need to be more careful, like with AbstractInteropTest.

How to disable animal sniffer for tests? I see there is an excludeDependencies but the supported artifact patterns don't have groupId:artifactId::scope that we can use to exclude the test scope.

Can you help with with this? | Use @org.codehaus.mojo.animal_sniffer.IgnoreJRERequirement on a class or method. | Use @org.codehaus.mojo.animal_sniffer.IgnoreJRERequirement on a class or method.

The usage of X509ExtendedTrustManager in the ProtocolNegotiatorsTest was for testing the cache but now after I moved the cache to NettyClientTransport, it is no longer required because the cache is not testable, since it doesn't use mocks for protocol negotiator. | The usage of X509ExtendedTrustManager in the ProtocolNegotiatorsTest was for testing the cache but now after I moved the cache to NettyClientTransport, it is no longer required because the cache is not testable, since it doesn't use mocks for protocol negotiator.

You can easily use a forwarding protocol negotiator (or mock+delegatesTo()) by injecting it on one of the newTransport(result.negotiator.newNegotiator()) lines. With the mock, you could then easily verify how many times verifyAuthority() was called.
(mock+delegatesTo() is actually really nice in general.) | The usage of X509ExtendedTrustManager in the ProtocolNegotiatorsTest was for testing the cache but now after I moved the cache to NettyClientTransport, it is no longer required because the cache is not testable, since it doesn't use mocks for protocol negotiator.

You can easily use a forwarding protocol negotiator (or mock+delegatesTo()) by injecting it on one of the newTransport(result.negotiator.newNegotiator()) lines. With the mock, you could then easily verify how many times verifyAuthority() was called.
(mock+delegatesTo() is actually really nice in general.)

Done. | I'm now only doing the authority verification if the transport authority is different from the one in the headers.



I do think we should set the attribute in most of the built-in negotiators, but ALTS and S2A would probably be without it for now.



As a subsequent work I assume.
After the changes to disallow authority override without having a verifier in the attributes, the FakeControlPlaneIntegrationTest which doesn't use Tls but plain text transport is failing. Is it ok to introduce a system property to allow authority override without a verifier and set it from the integration test?","In-progress changes. | In-progress changes that used ExtendedSSLSession. | Authority verify in Netty transport. | In-progress changes for Authority verify in okhttp transport. | Netty authority verify against peer host in server cert. | unit test. | in-progress changes. | nit changes and drop unintended changes. | nit changes and drop unintended changes. | Cache the peer verification result. | Move extraction of X509ExtendedTrustManager to utils. | commit | Fixes. | Fixes. | Fixes. | Fixes. | nit | Changes. | In-progress review comments. | Address review comments. | revert unintended | revert unintended | Address review comments. | Fix warning. | Address review comments. | Address review comments. | In progress changes. | Remove duplicate definitions of createTrustManager. | in-progress changes. | in-progress changes. | in-progress changes. | in-progress changes. | Merge branch 'grpc:master' into authorityverifyokhttp | Unit tests and using HostnameVerifier in per-rpc. | Merge remote-tracking branch 'origin/authorityverifyokhttp' into authorityverifyokhttp | Review comments. | Revert unintended changes. | Revert unintended changes. | Move NoopSslSession to io.grpc.internal under grpc-core project. | Address review comments. | Address review comments. | Added flag with default false for the per-rpc authority check. | Added flag with default false for the per-rpc authority check and removed setting sslEndpointAlgorithm. | Update README etc to reference 1.69.0 | :Revert ""Update README etc to reference 1.69.0""

This reverts commit f5b3614c6e3e7e3d7494988bd5be00619a4bbf53. | Fix style. | Fix style. | Merge branch 'master' into authoritychecktls | Fix merge conflicts. | Review comments and use reflection for X509ExtendedTrustManager. | Include failed method name in the tls verification failed log message. | Merge branch 'master' into authorityverifyokhttp | Use reflection to access X509ExtendedTrustManager. | Merge remote-tracking branch 'origin/master' into authorityverifyokhttp | Merge remote-tracking branch 'origin/authorityverifyokhttp' into authorityverifyokhttp | Address review comments. | Address review comments. | Address review comments. | Remove the code handling for impossible exception. | Update comment for NoSuchMethodError. | Some changes based on similar comments in the authority check for Netty PR. | Animal sniffer suppress | Merge branch 'master' into authoritychecktls | Ignore animal sniffer errors via annotation in tests. | In-progress changes to move authority verification to the ChannelHandler instead of the Protocol negotiator. | Save sslEngine and use it later after the handshake is complete, to set the attribute for the hostname verifier. | temp testing changes | Changes. | Merge branch 'authoritychecktls-fork' into authoritychecktls

# Conflicts:
#	examples/example-tls/src/main/java/io/grpc/examples/helloworldtls/HelloWorldClientTls.java | Merge branch 'grpc:master' into authoritychecktls | Merge remote-tracking branch 'origin/authorityverifyokhttp' into authoritychecktls

# Conflicts:
#	core/src/main/java/io/grpc/internal/CertificateUtils.java
#	okhttp/src/main/java/io/grpc/okhttp/OkHttpChannelBuilder.java | Merge remote-tracking branch 'origin/authoritychecktls' into authoritychecktls | Revert ""Merge remote-tracking branch 'origin/authorityverifyokhttp' into authoritychecktls""

This reverts commit 8576a4efdbe2aa2ac080c7d3e30b8327e9183376, reversing
changes made to da19b2886ac3b793b8deb7d0f9059b99f4292b73. | Fix style | Revert unintended changes. | Revert unintended changes. | Revert unintended changes. | Review comments. | Review comments | Fix review comments | Fix test and some mistakes."
grpc/grpc-java,11254,https://github.com/grpc/grpc-java/pull/11254,Xds fallback,Implementation of https://github.com/grpc/proposal/blob/master/A71-xds-fallback.md in Java,2024-12-09T23:42:27Z,larry-safran,larry-safran,easier to read,"Fallback is ready for review again. | Ugh. Github sent the review before I was done with it. Looking over the sent comments, I think they make sense alone/aren't wrong. But I'm still working on the review. | The `FailingXdsTransport` seems like the cleanest approach, though I think
we need a `FailingXdsStreamingCall` to go with it.
…
On Wed, Nov 27, 2024 at 2:43 PM Eric Anderson ***@***.***> wrote:
 ***@***.**** commented on this pull request.
 ------------------------------

 In xds/src/main/java/io/grpc/xds/client/XdsClientImpl.java
 <#11254 (comment)>:

 > +      cleanUpResourceTimers(cpcClosed);
 +
 +      if (status.isOk()) {
 +        return; // Not considered an error
 +      }
 +
 +      Collection<String> authoritiesForClosedCpc = getActiveAuthorities(cpcClosed);
 +      for (Map<String, ResourceSubscriber<? extends ResourceUpdate>> subscriberMap :
 +          resourceSubscribers.values()) {
 +        for (ResourceSubscriber<? extends ResourceUpdate> subscriber : subscriberMap.values()) {
 +          if (subscriber.hasResult() || !authoritiesForClosedCpc.contains(subscriber.authority)) {
 +            continue;
 +          }
 +
 +          // try to fallback to lower priority control plane client
 +          if (shouldTryFallback && manageControlPlaneClient(subscriber).didFallback) {

 In getOrCreateControlPlaneClient(String) we need to catch
 IllegalArgumentException from getOrCreateControlPlaneClient(ServerInfo),
 because we wouldn't want failure to create one transport to prevent us from
 trying other servers.

 But handling that error is tricky. If we log and keep going then
 addCpcToAuthority() will add null to the activateCpClients authority
 list. We'd need to put protections everywhere it is used. I think we'd be
 better off making FailingXdsTransport (I know, I know. Hide your surprise).
 The main issue I see is we probably shouldn't call eventHandler until
 StreamingCall.start() returns, which requires a separate thread. If we
 move the IllegalStateException handling into GrpcXdsTransport then we could
 use GrpcUtil.SHARED_CHANNEL_EXECUTOR.

 Ideas?

 —
 Reply to this email directly, view it on GitHub
 <#11254 (comment)>, or
 unsubscribe
 <https://github.com/notifications/unsubscribe-auth/AZQMCXTJJTCVPTBJHCVVC2L2CZDKJAVCNFSM6AAAAABIXKY5WKVHI2DSMVQWIX3LMV43YUDVNRWFEZLROVSXG5CSMV3GSZLXHMZDINRWGA2TAMJYGQ>
 .
 You are receiving this because you authored the thread.Message ID:
 ***@***.***> | The FailingXdsTransport seems like the cleanest approach, though I think
we need a FailingXdsStreamingCall to go with it.

Yeah, it implies a FailingXdsStreamingCall as well.",Rebased onto master | More fixes | Fix flaky test
grpc/grpc-java,11676,https://github.com/grpc/grpc-java/pull/11676,examples: Added README files for all missing Examples,"Added new README files for all the examples with details and changed the incorrectness/ typo in path of `examples/example-alts/example-alts/README.md` → `examples/example-alts/README.md`

Fixes https://github.com/grpc/grpc-java/issues/5467 
",2025-01-28T07:32:00Z,shivaspeaks,vinodhabib,easier to read,"I see this says Fixes #5467, and it mentions ""directory structure"" a few times. But I don't actually see it really changing the directory structure and I don't know what ""GRPC Examples documentation"" is communicating. The directory structure issue is that we have a mix of top-level examples and then example folders like example-alts. | I see this says Fixes #5467, and it mentions ""directory structure"" a few times. But I don't actually see it really changing the directory structure and I don't know what ""GRPC Examples documentation"" is communicating. The directory structure issue is that we have a mix of top-level examples and then example folders like example-alts.

@ejona86 Thanks for your reply, I was refering to one of the change as the ""directory structure"" for example-alts as it is not consistant with all the other examples projects directory so made this change/move as below in this PR apart from this no other changes related to directory structure.

""GRPC Example documentation"" was refering here only for adding new README files in all examples with explanations nothing else. | Sigh... my comment #11676 (comment) still applies. This doesn't fix #11676 and the directory structure it was talking about it not related to this change. | Oh, but I thought the need for those changes was striked out by your comment on #5467 (comment)

I don't disagree, but some of the reasons for the current directory structure: 1) separating examples with different dependencies and 2) letting users copy-paste the examples (because lots of people try to copy-paste).

If not, perhaps we can reopen #5467 and edit the issue description with what's left. | @shivaspeaks, I see that you improved the PR description, but that didn't get copied into the commit itself. That information should generally be in the commit; the only information I tend to omit in the commit message is messages to the reviewer (e.g., ""This is the first part; I will send out the second part next week."") Managing that is much easier for your own PRs as you can write a good commit message and have github auto-copy it to the PR description. But if the review changes the code substantially or when you merge someone else's change, it is good to double-check the commit message.
I do think the example structure could be improved, but #5467 didn't really say what was wrong or what would be better and it is low-quality, mixing of multiple things, and confusing enough that a replacement issue would maybe be better. FWIW, that needed discussion/design for what would be better. The issue was ""this needs improving"" and I agree with that, but that doesn't mean the solution is fully-formed.
(There are some examples, like the json one, that should be moved to its own directory because they have extra dependencies. But what changes to do where really requires an audit to look through it and see what could be improved.)",examples: following the consistency for example-alts directory structure | examples: added the missing examples to common readme file | examples: Updated common readme file | examples: Fixed review points | examples: Added README file with details for examples | examples: Added README file with details for examples | examples: Added README file with details for examples | examples: Added README file with details for examples | examples: Added README file with details for examples | examples: Added README file with details for examples | examples: Removed extra lines in the Readme files of Examples | examples: Added README file with details for examples | examples: Added README file with details for examples | examples: Added README file with details for examples | examples: Added README file with details for examples | examples: Added README file with details for examples | examples: Added README file with details for examples | examples: Added README file with details for examples | examples: Added README file with details for examples | Merge branch 'master' into defect-5467-example-doc-changes | examples: Fixed some Review points and others in progress | examples: Fixed all Review points | examples: Fixed the recent review points
grpc/grpc-java,11503,https://github.com/grpc/grpc-java/pull/11503,Detect transport executors with no remaining threads,"Created a way to detect insufficient threads to start the transport for read and write simultaneously. 

Fixes #11271 ",2024-09-16T11:02:52Z,shivaspeaks,shivaspeaks,easier to read,"Is another runnable actually needed here? The existing ClientFrameHandler can be started earlier, for example like this:
@@ -164,7 +167,7 @@
   private final ScheduledExecutorService scheduler;
   private final int maxMessageSize;
   private int connectionUnacknowledgedBytesRead;
-  private ClientFrameHandler clientFrameHandler;
+  private final ClientFrameHandler clientFrameHandler = new ClientFrameHandler();
   // Caution: Not synchronized, new value can only be safely read after the connection is complete.
   private Attributes attributes;
   /**
@@ -574,7 +577,7 @@
           onException(e);
           return;
         } finally {
-          clientFrameHandler = new ClientFrameHandler(variant.newReader(source, true));
+          clientFrameHandler.readerAndStartSignal.add(variant.newReader(source, true));
         }
         synchronized (lock) {
           socket = Preconditions.checkNotNull(sock, ""socket"");
@@ -591,15 +594,18 @@
       latch.countDown();
     }
 
+    // ClientFrameHandler need to be started after connectionPreface / settings, otherwise it
+    // may send goAway immediately.
+    executor.execute(clientFrameHandler);
+
     serializingExecutor.execute(new Runnable() {
       @Override
       public void run() {
         if (connectingCallback != null) {
           connectingCallback.run();
         }
-        // ClientFrameHandler need to be started after connectionPreface / settings, otherwise it
-        // may send goAway immediately.
-        executor.execute(clientFrameHandler);
+        clientFrameHandler.started.await(10, TimeUnit.SECONDS); // TODO error handling
+        clientFrameHandler.readerAndStartSignal.add(""START"");
         synchronized (lock) {
           maxConcurrentStreams = Integer.MAX_VALUE;
           startPendingStreams();
@@ -1090,19 +1096,19 @@
 
     private final OkHttpFrameLogger logger =
         new OkHttpFrameLogger(Level.FINE, OkHttpClientTransport.class);
-    FrameReader frameReader;
+    final CountDownLatch started = new CountDownLatch(1);
+    final BlockingQueue<Object> readerAndStartSignal = new ArrayBlockingQueue<>(2);
     boolean firstSettings = true;
 
-    ClientFrameHandler(FrameReader frameReader) {
-      this.frameReader = frameReader;
-    }
-
     @Override
     @SuppressWarnings(""Finally"")
     public void run() {
+      started.countDown();
       String threadName = Thread.currentThread().getName();
       Thread.currentThread().setName(""OkHttpClientTransport"");
       try {
+        FrameReader frameReader = (FrameReader) readerAndStartSignal.poll(1, TimeUnit.MINUTES);
+        readerAndStartSignal.poll(1, TimeUnit.MINUTES);
         // Read until the underlying socket closes.
         while (frameReader.nextFrame(this)) {
           if (keepAliveManager != null) { | @panchenko, it's unclear what you're optimizing for. The error handling looks harder to get right with reusing the reader. | @ejona86 starting an intermediate runnable does not guarantee there will be an available thread for clientFrameHandler, so I am thinking about starting exactly the runnable we need.
The error handling should be similar anyway. | starting an intermediate runnable does not guarantee there will be an available thread for clientFrameHandler, so I am thinking about starting exactly the runnable we need.

Oh, the concern isn't clientFrameHandler. If we can't get that thread, then at least the wedged transport is not impacting other transports. Also, we won't be able to receive the initial SETTINGS so the transport won't go READY... I guess we could have a timeout trigger in that case.
The concern is the serializingExecutor as used by AsyncSink. If we have N threads and start N transports, then clientFrameHandler will consume those N threads. But then no transport can actually send anything, as that uses an on-demand thread in serializingExecutor/AsyncSink. | Also, if you say ""Fixes #11271"" then merging the PR automatically closes the issue. See https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword",Detect transport executors with no remaining threads | OkHttp: Detect transport executors with no remaining threads | OkHttp: Detect transport executors with no remaining threads | Resolving blockers | Resolving blockers | Detect transport executors with no remaining threads | Detect transport executors with no remaining threads | Detect transport executors with no remaining threads
grpc/grpc-java,11490,https://github.com/grpc/grpc-java/pull/11490,core: SpiffeId parser,SpiffeId parser compliant with [official spec](https://github.com/spiffe/spiffe/blob/main/standards/SPIFFE-ID.md),2024-09-26T16:01:11Z,erm-g,erm-g,easier to read,"What's the plan for this?

@ejona86 This quarter @erm-g is adding the libraries for go/xds-spiffe to be able to do SPIFFE federation, given an in-memory SPIFFE trust map and the peer certificate chain. This is (my understanding of) what we aligned on at the start of the quarter.
The plan was to add the libraries with these core logic to io.grpc.util, so that they can be called in both XdsX509TrustManager (when the full execution of go/xds-spiffe is underway) and in AdvancedTlsX509TrustManager. Does that sound reasonable to you? Is there a different location you'd prefer us to put these libraries?
Thanks! | I'm not wild about having a public API for low-level Spiffe handling. If it is just code reuse we want, it can go in io.grpc.internal (grpc-core). | I'm not wild about having a public API for low-level Spiffe handling. If it is just code reuse we want, it can go in io.grpc.internal (grpc-core).

SGTM. | What's the plan for this?

Hi @ejona86 , we're done with internal review (there are few open comments where we'd like your input) - could you take a look?","Initial commit | Tests to cover overall format and trust domain | Happy path | Address PR comments | Improve test coverage | Address PR comments | Style fix | Style fix | Style fix | Address PR comments | Address PR comments | Address PR comments | Address PR comments | Style fix | Style fix | Put the private constructor back, aligned with JsonParser style."
grpc/grpc-java,11140,https://github.com/grpc/grpc-java/pull/11140,Adds MetricSink implementation for gRPC OpenTelemetry,"This PR adds following components that are required for gRPC Non per call metrics architecture.

- MetricSink implementation for gRPC OpenTelemetry
- Configurator for plumbing per call metrics ClientInterceptor and ServerStreamTracer.Factory via unified OpenTelemetryModule.

",2024-05-03T17:45:46Z,ejona86,DNVindhya,easier to read,,"Added MetricSink implementtaion for OpenTelemetry | recording null entry in sink measure for disabled metric and addressed review comments | removed instrument from MeasuresData | Remove default getMeasuresSize() returning -1

Implementations should implement it for real."
grpc/grpc-java,10722,https://github.com/grpc/grpc-java/pull/10722,RELEASING.md: Remove $ before commands,"Removing the $ prompt makes it easier to copy+paste. At no point are we running as root, so there's no # vs $ distinction, not that many people would even notice the difference.

There is a risk here that not all commands end up getting run. When pasting multiple commands at once, gradle or another tool might read in stdin and discard them. But it's probably not worth continuing the copy-each-command-separately-and-avoiding-the-$.",2023-12-12T16:57:20Z,ejona86,ejona86,easier to read,,"RELEASING.md: Remove $ before commands

Removing the $ prompt makes it easier to copy+paste. At no point are we
running as root, so there's no # vs $ distinction, not that many people
would even notice the difference.

There is a risk here that not all commands end up getting run. When
pasting multiple commands at once, gradle or another tool might read in
stdin and discard them. But it's probably not worth continuing the
copy-each-command-separately-and-avoiding-the-$."
grpc/grpc-java,10272,https://github.com/grpc/grpc-java/pull/10272,implemented and tested static stride scheduler for weighted round robin load balancing policy,"The core of the Weighted Round Robin load balancer policy on the client side is a stride scheduler originally implemented by an [EDFScheduler](http://google3/net/rpc2/edf_scheduler.h;rcl=438515966). However, the mutex lock required by the EDFScheduler has been a frequent source of thread contention at high request rates and a block on other cost saving efforts.

The Static Stride Scheduler is a generator of a practically equivalent sequence of picks as the current EDFScheduler. It removes the need for a priority queue (and thus a lock) and improves latency at high request rates. 

[Weighted Round Robin LB Policy](https://github.com/grpc/proposal/blob/master/A58-client-side-weighted-round-robin-lb-policy.md#earliest-deadline-first-edf-scheduler)",2023-07-06T17:03:08Z,tonyjongyoonan,tonyjongyoonan,easier to read,"The committers listed above are authorized under a signed CLA.✅ login: tonyjongyoonan / name: Tony An  (b5cf7b0, 44a5158, 32973d4, 4bde79d, d99d51a, 13a9fd8, 3d9e625, 2b054d3, 4addda3, 4a820a7, dc7960a, acd3425, ba1a0b7, 9f4a60d, e11e542, c76cbe5, 903d2ac, d5a0629, 5982115, 88a8e48, f9cae20, dba0778, 46a463e, 142b499, 5523337, 365ba8d, 442bea8, 2a0e489, 1731862, 5e5127f, f0421d2, ec46fe3, 8ddf284, 347b46f, 4a4762e, f526bf6, 21ceb85, 03de3f9, e2eb7f9, 4072907, f749e52, be21c23) | You can continue pushing to your branch and update the PR as you go. | TODO: I need to fix some warnings (which will probably show up as failures) for parts of the edf scheduler that are now unused (ex. random). This probably requires changing the constructor as well as everywhere where the WRR LB is initialized.","implemented static stride scheduler class/algorithm (currently unused)

go/static-stride-scheduler | added imports, fixed small errors | changed to array, final class vars, atomic integer | added static stride scheduler test cases | fixing test case datatypes | added check argument for edge case: no weights inputted | replaced edf scheduler with static stride scheduler | added test case | fixed style errors, renamed schedulers | quick fix | bug fix attempt | fixed verbose work in updateWeightSS(), float equality | fixed pickChannel() by removing kOffset, fixed atomic integer | added example test cases | types changed from long to int | added sss test cases | added more edge cases (negative/zero weights) | compile fix | more than 3 channels test case | fixed negative case | end to end test cases | fixed sequence | ident fix | replaced deterministic test cases with ones with many iterations, deleted edf cases | fixed sequence, added comments, deleted edf | adjusted constructor in tests | fixed constructor to remove random | optimized for scaling, fixed next sequence | added random to resolve static class failure | adjusted test cases to have random, deleted deterministic test cases from edf | fixed test case and number to hex | fixed sequence, changed long --> int, added comments | fixed sequence, changed types, added comments | fixing weird commit 1 | more syntax changes | Delete settings.json | modified test | fixed feedback | changed weights to short from int | fixed message and short logic | removed obselete comment | changed annotations"
grpc/grpc-java,9812,https://github.com/grpc/grpc-java/pull/9812,Move name resolution retry from managed channel to name resolver (take #2),"This change has these main aspects to it:

1. Removal of any name resolution responsibility from ManagedChannelImpl
2. Creation of a new RetryScheduler to own generic retry logic
    - Can also be used outside the name resolution context
3. Creation of a new RetryingNameScheduler that can be used to wrap any polling name resolver to add retry capability
4. Use of a temporary callback object in ResolutionResult attributes to allow ManagedChannelImpl to indicate if resolved addresses were accepted. This can be removed once the Listener2.onResult() API can be updated to return a boolean for this purpose. ",2023-02-04T02:23:32Z,temawi,temawi,easier to read,cc: @larry-safran,"Name resolution retry from channel to name resolver

This change has these main aspects to it:

1. Removal of any name resolution responsibility from ManagedChannelImpl
2. Creation of a new RetryScheduler to own generic retry logic
     - Can also be used outside the name resolution context
3. Creation of a new RetryingNameScheduler that can be used to wrap any
   polling name resolver to add retry capability
4. A new facility in NameResolver to allow implementations to notify
   listeners on the success of name resolution attempts
     - RetryingNameScheduler relies on this | Review feedback changes. | Fix problem with calling wrong refreshNameResolution() | More precise use of DnsNameResolver in tests | Review feedback

- Do not change the public API
- Use a hacky internal callback
- More unit test coverage | Call callback in NamesResolved. | ManagedChannelImpl to wrap the NameResolver | BackoffPolicyRetryScheduler uses sync context to serialize | Fail RetryingNameResolver if double-wrapped | BackoffPolicyRetryScheduler only usable within a sync context | Retry when initial service config is invalid | Don't call listener if NR already shut down"
grpc/grpc-java,9790,https://github.com/grpc/grpc-java/pull/9790,Upgrade to Checkstyle 8.28,"Trying to upgrade Gradle to 7.6 improved the checkstyle plugin such that it appears to have been running in new occasions. That in turn exposed us to https://github.com/checkstyle/checkstyle/issues/5088. That bug was fixed in 8.28, which also fixed lots of other bugs. So now we have better checking and some existing volations needed fixing. Since the code style fixes generated a lot of noise, this is a pre-fix to reduce the size of a Gradle upgrade.

I did not upgrade past 8.28 because at some point some other bugs were introduced, in particular with the Indentation module. I chose the oldest version that had the particular bug impacting me fixed. Upgrading to this old-but-newer version still makes it easier to upgrade to a newer version in the future.",2023-01-06T01:07:04Z,ejona86,ejona86,easier to read,,"Upgrade to Checkstyle 8.28

Trying to upgrade Gradle to 7.6 improved the checkstyle plugin such that
it appears to have been running in new occasions. That in turn exposed
us to https://github.com/checkstyle/checkstyle/issues/5088. That bug was
fixed in 8.28, which also fixed lots of other bugs. So now we have
better checking and some existing volations needed fixing. Since the
code style fixes generated a lot of noise, this is a pre-fix to reduce
the size of a Gradle upgrade.

I did not upgrade past 8.28 because at some point some other bugs were
introduced, in particular with the Indentation module. I chose the
oldest version that had the particular bug impacting me fixed. Upgrading
to this old-but-newer version still makes it easier to upgrade to a
newer version in the future."
grpc/grpc-java,9666,https://github.com/grpc/grpc-java/pull/9666,xds: Fake control plane test setup code to Rules,"This extracts the startup and shutdown code for the control and data plane server to separate JUnit rules, which allows this logic to be reused in other tests in a simple manner. Also makes the test easier to read with the boiler plate init code removed.",2022-11-03T17:48:53Z,temawi,temawi,easier to read,,"xds: Fake control plane test setup code to Rules

This extracts the startup and shutdown code for the control and data
plane server to reparate JUnit rules, which allows this logic to be
resued in other tests in a simple manner. Also makes the test easier to
read with the boiler plate init code removed."
grpc/grpc-java,9758,https://github.com/grpc/grpc-java/pull/9758,Move name resolution retry from managed channel to name resolver.,"This change has these main aspects to it:

1. Removal of any name resolution responsibility from ManagedChannelImpl
2. Creation of a new RetryScheduler to own generic retry logic
    - Can also be used outside the name resolution context
4. Creation of a new RetryingNameScheduler that can be used to wrap any polling name resolver to add retry capability
5. A new facility in NameResolver to allow implementations to notify listeners on the success of name resolution attempts
    - RetryingNameScheduler relies on this",2022-12-16T23:30:58Z,temawi,temawi,easier to read,"PR passed internal interop tests: https://fusion2.corp.google.com/ci/kokoro/prod:grpc%2Fjava%2Fmaster%2Fbranch%2Fxds_k8s_lb/activity/288eb952-6eb8-4073-a768-73e17deaee4d/invocations/288eb952-6eb8-4073-a768-73e17deaee4d/targets/grpc%2Fjava%2Fmaster%2Fbranch%2Fxds_k8s_lb/log | The boolean onResult() return will be wrong/delayed or thread-unsafe. There's two possibilities:

The NR is calling onResult from sync context. In this case lastAddressesAccepted won't be updated in-line in onResult() so the return value will be the previous call's value. Long-term, we probably need onResult to be called from sync context
The NR is not calling onResult from sync context. In this case lastAddressesAccepted may still be out-dated, as the runnable can run on other threads. But even if it has been run already lastAddressesAccepted is read outside of the sync context and isn't volatile.

I had seen that problem earlier and had suggested to Terry to avoid it by doing the callback. It seems the approach wasn't entirely clear. What I had imagined had no new public API, but would have some private hackery between RetryingNameResolver and ManagedChannelImpl until the point we can fix the threading issues.
Approach:

ManagedChannelImpl can't return a boolean, so use a callback instead. To avoid adding temporary API we'd delete once the return is fixed, pass the callback as an Attribute in ResolutionResult. ManagedChannelImpl and RetryingNameResolver would both have visibility to the Attributes.Key
RetryingNameResolver should implement start(Listener2) and wrap the Listener2 with its own Listener. Within onResult(), it would modify the ResolutionResult to add the Attributes.Key for the callback before calling the original onResult(). (Note that there's some annoyance here as start(Listener) will need an implementation copied from the base class to delegate to start(Listener2) since it is overridden in ForwardingNameResolver.)","Name resolution retry from channel to name resolver

This change has these main aspects to it:

1. Removal of any name resolution responsibility from ManagedChannelImpl
2. Creation of a new RetryScheduler to own generic retry logic
     - Can also be used outside the name resolution context
3. Creation of a new RetryingNameScheduler that can be used to wrap any
   polling name resolver to add retry capability
4. A new facility in NameResolver to allow implementations to notify
   listeners on the success of name resolution attempts
     - RetryingNameScheduler relies on this | Review feedback changes. | Fix problem with calling wrong refreshNameResolution() | More precise use of DnsNameResolver in tests"
grpc/grpc-java,8596,https://github.com/grpc/grpc-java/pull/8596,servlet: Implement gRPC server as a Servlet,"This is an updated version of the code found at https://github.com/grpc/grpc-java/pull/4738, merged up to latest, tests restored from [a different branch](https://github.com/dapengzhang0/grpc-java/commit/d1aaafcac), and with a few of the remaining review comments addressed as well. 

The tests run during the build against Jetty 9, 10, 11, Tomcat 9, 10, and Undertow 9. Note that Jetty 10 and 11 are largely the same except for the javax/jakarta package changes). There are still several tests marked as FIXME, and several more ignored due to how servlets function differently than something entirely self-contained like grpc-netty. Note also that while Jetty 9 works, it requires a workaround for missing support for writing trailers - I am amenable to dropping this workaround or documenting it in some way for downstream users, but without this Jetty 9 can't be tested in Java 8. Note also that a standard Jetty 9 installation will not support this use case, but it either needs to be lightly customized, or used as an embedded web server.

A second project is also added, grpc-servlet-jakarta, which consists of build logic to rewrite package names for `javax.servlet` imports and replace them with their corresponding `jakarta.servlet` imports. This also removes the Jetty workaround, as it is unnecessary (Jetty 11+ supports the `jakarta.servlet` APIs, which necessarily means that trailers are supported using the standard API calls).

--

As a new contributor to this project, I anticipate that I have made some mistakes in trying to bring my parts of this patch up to the standards expected for contribution, and it is quite possible that I missed some conversation where it was decided that this wouldn't make sense to merge to grpc-java itself. If that is the case, I will be releasing this separately - outside of tests, there should be no other changes to the modules in grpc-java itself. With this released one way or the other, my intention is to add grpc-web support here as well, to allow a standalone (i.e. without Envoy) Java web server to provide static html/js content and grpc services, which otherwise requires ~3 processes (and potentially one more to support non-SSL streaming to the browser).

Inspecting my own code, I think it makes sense to improve usability of the tests in IntelliJ, and add README.md files to each of grpc-servlet and grpc-servlet-jakarta. ",2023-01-20T21:17:58Z,larry-safran,niloc132,easier to read,"The committers are authorized under a signed CLA.✅  ZHANG Dapeng (a0c92df, 8719377, 46d00ec, 4fa6271, e793c7c, 393fcf6, 706a3c9, 5e11674, 1776917, bb3a720, 4c7cd4f, af7f6a3, 2fc04fc, 6fb4fc2, 8b78b6d, 680872b, b3473df, 3728866, 58fce63, 5b02064, d59f81e, 5b7496e, e98e4f8, 6aa26e4, c3dfaba, 44ea5f3, ef5f879, 47e3e95, c5fc2bd, 63e8cb2, 986a885, 3fd4ca8, 7d8410f, a39be4c)✅  Chengyuan Zhang (09967c1)✅  Colin Alworth (a53114b, ae0085f, 9a02c82, 7977de6, 811fcbe, ccbfd8d, 5d017a3, 0180d29, 7bb191c, 59d9754, 1f4b0ca, ccc5cd6, 7d90af8, aea3351, 70ca920, 83d405e, 18d20d3, f4c0c87, b12370e, dea10c6, 2154764, bf5c225, a0b5852, 512a2d3, 4bed353, 907f432, 75f9b82, cf3decb, 00b8f76, 35e0af2, 0c5664c, 8162d6e, 8e25c4c, 3bcd0af, 034771f, da85cf8, f84a156, f7677f6, dc2ddcf, cc9eb3f) | @niloc132 Really appreciate it for pushing forward my almost languishing PR due to limited bandwidth. Can you please sign the CLA so we can start to review? Let me know if you have issues signing. | Sorry for the delay, the person responsible for signing on my behalf is out of the office today, we should have it back soon.
Thanks for your excellent work in making this possible - I started using your branch about a year ago and had good success with it, but have only intermittently had the chance to merge it up to date further than what I needed it to be at the time. | Could it be done completely reflectively? I really don't want to have strong dependency on container-specific classes (e.g. org.eclipse.jetty.server.*) here.

(I can't seem to reply to your message here, so making a general reply instead.)
Perhaps, though you'll notice that this class only has a compile-time dependency on Jetty, and won't require a runtime dependency unless Jetty is already in use. This was written this way to avoid the need for explicit reflection, so as to make it easier to understand, but I'm of the opinion at this point that it isn't worth the trouble. My self-review pointed this out too, jetty 9 might just be more trouble than it is worth.
I had hoped to keep Jetty 9.4 support, as it seems to have better http2 support than the other two servlet containers you had written tests for, and it still supports Java 8, but the jetty project isn't interested in patching this bug as part of their release, and there are a few ways that this workaround can still go wrong, so I'm inclined to remove it entirely, except perhaps as a footnote to enable embedded jetty 9.4 support. | Sorry didn't get a chance to focus on this in the past weeks, will find time in December. | @niloc132, I pushed some small fixes to your branch. | We've copied these changes directly into the repo I'm working on, with one minor alteration so that a servlet-based grpc-web adapter can also use the same ServerTransportListener etc. This is certainly the wrong way to approach this problem, but I don't yet have the right one - I'll come back to the grpc-servlet with a followup as we finish implementing the other pieces we need on top, but my hunch is that the GrpcServlet will mostly be replaced with a GrpcFilter that handles application/grpc calls, and lets the others pass through to other appropriate servlets/filters. | Jetty9 workaround and tests have been removed, and the jakarta build correctly publishes an artifact now as well.
I'm still struggling with several tomcat tests that intermittently fail, I'll spend a little time on them over the next few days, but will probably err on the side of ignoring them (to be revisited as the tested tomcat version is updated) - does that seem reasonable?
Thanks for taking the time to review this, I appreciate it. | Jetty9 workaround and tests have been removed, and the jakarta build correctly publishes an artifact now as well.

Thank you so much!

I'm still struggling with several tomcat tests that intermittently fail, I'll spend a little time on them over the next few days, but will probably err on the side of ignoring them (to be revisited as the tested tomcat version is updated) - does that seem reasonable?

Yes, if it's not easy to fix/triage, for the moment we will ignore them and I'll file a tracking issue later, and investigate in the future. In the past, I filed a number of tomcat bugs/enhancement related to async servlet over HTTP/2, some of the intermittent failures took me a lot of time to find a minimal reproducible verifiable example, it was quite painful. | I've pushed another commit where i set another three flags to instruct tomcat to disregard other cases where the client is too chatty for its likes. Several dozen test iterations later, I feel confident in reporting that none of the three intermittent tests are failing any longer. | There were warnings from java 9-16 on tomcat's illegal reflective access on startup in the tests, I added the appropriate --add-opens and validated that tests pass cleanly on Java 11 and 17. | Is there anything I can do to help get this into the right shape to merge and release it? | Is there anything I can do to help get this into the right shape to merge and release it?

@niloc132 It's almost in the right shape now. I'm trying to add some unit tests. I'm actively on it. | Thanks - if there is something you'd like me to help out on, leave a note and I'll get to it as soon as possible.
We're doing some experimenting with grpc-web in another ServletAdapter-like class - this makes more sense if you can share the transportlistener, streamtracerfactories etc between them, but I don't have a pattern I like well enough yet to propose here. For the moment at least, we'll maintain our own fork as we iterate this, and propose something when we have some options to suggest, see what fits best into the rest of the project here.
We're also trying using a single Filter to handle all mapped services rather than bind a servlet to each path or bind a specific servlet for each service, so that unhandled paths (or non-grpc calls) can just fall through to other filters or servlets. This also makes it straightforward to create separate handlers for gRPC calls based on the same transportlisterner/streamtracerfactory/etc, as for example websockets* can't be configured consistently across servlet containers to start from a filter/servlet.
* Websockets of course are not a standard grpc/grpc-web transport mechanism, but https://github.com/improbable-eng/grpc-web/ offers a handy client transport that allows a browser to connect to streams with binary data through a websocket proxy. This has also made localhost web development easier, as http2 doesn't work in a browser without tls. I don't imagine that the grpc-java project will be interested in the in-process Java implementation of that transport that we've developed, but we are going to make it generally available for use cases such as ours. | Checking in on this - are there other changes I can make to help get this merged, or is there a reason we think it might not belong?
I've tried to update it locally to v1.46 or so, but despite cleaning I am consistently getting an error locally from the netty-shaded project, with both java 8 and 11.
io.grpc.netty.shaded.ShadingTest > tcnative FAILED
    java.lang.UnsatisfiedLinkError at ShadingTest.java:121
        Caused by: java.lang.IllegalArgumentException at ShadingTest.java:116

java.lang.UnsatisfiedLinkError: failed to load the required native library
	at io.grpc.netty.shaded.io.netty.handler.ssl.OpenSsl.ensureAvailability(OpenSsl.java:596)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslContext.newClientContextInternal(SslContext.java:830)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslContextBuilder.build(SslContextBuilder.java:611)
	at io.grpc.netty.shaded.ShadingTest.tcnative(ShadingTest.java:121)

I'm not sure if there is a specific environmental issue here, since it appears that CI builds aren't affected, and I also get this failure from building at current master.
In the latest CI run for this branch however, I see a failure I wasn't expecting for Java 11:
io.grpc.testing.integration.NettyFlowControlTest > largeBdp FAILED
    java.lang.AssertionError: Window was 688124 expecting 314570
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at io.grpc.testing.integration.NettyFlowControlTest.doTest(NettyFlowControlTest.java:152)
        at io.grpc.testing.integration.NettyFlowControlTest.largeBdp(NettyFlowControlTest.java:102)

However, this is netty specific (and this code shouldn't be affected by the PR), so I'm not clear why this would start failing now.

I have a working prototype to propose as a follow-up for this, which provides a grpc-web in-process proxy, handling only translating the relevant differences between grpc and grpc-web, which mostly so far has boiled down to rewriting the content-type header and correctly serializing trailers as a final http payload. This seems to be working for unary and server-streaming (as browsers can't yet support any client-streaming over either xhr or fetch) with http/1.1, I'll be testing http2 soon (with https, etc). Additionally, only the application/grpc-web content type is supported so far, though adding support for grpc-web-text shouldn't be too difficult.
I know there is an existing (though recently marked ""on hold"") java servlet implementation of grpc-web at https://github.com/grpc/grpc-web/blob/master/src/connector/, my prototype contrasts with that by using a servlet Filter to hand off the processing to the existing grpc servlet. Among other things, this has the advantage of not needing to re-implement message handling, and allows the existing grpc servlet to call into the normal grpc-core wiring to actually invoke the grpc service method (the linked connector creates service stub instances by reflection, which prevents ServiceDefinitions having the expected control over how things are actually wired up, which in turn means that server interceptors don't actually run, etc). This new filter, in conjunction with the current pull request, should be able to replace the existing connector project.
On the other hand, the filter in its current form is rather specific to the current GrpcServlet implementation, taking care to manage only headers that the expected implementation will expect, produce, so it might make sense to keep them together.
Testing is also a question, I'm not at this time familiar with any complete grpc-web client written in Java. I don't presently have plans to need this outside of testing this very filter, so my own work might remain limited to automated integration tests of a browser client connecting to the grpc-web filter performing an in-process proxy to the actual servlet.
...All of this being a long-winded way of saying that until this PR merges, it makes no sense to land this code to grpc/grpc-java or grpc/grpc-web directly, but that I should plan on expanding our grpc-java fork to include this new aspect. | @ejona86 Any bandwidth for the final review? It's becoming more and more difficult to make the PR up-to-date as there could be regressions in the main branch that's only impacting grpc-servlet such as #9308 .
@niloc132, I could not push new commits to this PR anymore. Did you disallow edit by the maintainers? | The checkbox for edits by maintainers is still checked, and I made no deliberate change on my end. Push to a branch on your repo and I'll pull the commits over directly? | Push to a branch on your repo and I'll pull the commits over directly?

Thanks. @niloc132 Please pull/redo the 4 commits from https://github.com/dapengzhang0/grpc-java/commits/servlet-niloc132
b13d505
639084a
7ae71dc
5829381 | Done - hopefully updating this will let this work again for you to push, otherwise just ping me and I'll update it again. | Thanks for all this effort. Is there anything blocking the merge (besides the failed kokro jobs) ? I'm really interested in this development as it can really simplify operations for us by sharing the same servlet container, same port, for both standard HTTP/REST calls and GRPC calls. | If you're interested, we have published the jakarta variation of this repo in our own groupid: https://search.maven.org/artifact/io.deephaven/deephaven-grpc-servlet-jakarta/0.14.0/jar. We also have another jar we publish, also licensed apache2, which re-implements the https://github.com/improbable-eng/grpc-web/ websocket proxy in Java, running in-process.
In the next month or so we will be shipping an in-process grpc-web proxy as well, so that http2 browser clients can connect directly to grpc services. We're also working on a variant of the above websocket proxy that treats the websocket more like a ""channel"" and less like a ""stream"", to allow many concurrent bidirectional binary streams (in contrast with the https://github.com/grpc/grpc-web project, which can only stream text). Feel free to contact me off-list for more discussion. | @niloc132 thanks a lot for your link. We'll definitively look at them. Unfortunately we use javax.* in our code and changing this will be difficult. @dapengzhang0 anything missing in this PR ? | @hypnoce given that we're coming up on a year of this PR having been updated, I'm going to be moving forward with publishing a complete version (i.e. javax.servlet as well as jakarta.servlet) in the next month or two, plus the additional enhancements (grpc-web support, websocket transports, etc) that we've developed. I'll update the linked ticket and this pull request with that information once I've finished this process. | @hypnoce given that we're coming up on a year of this PR having been updated, I'm going to be moving forward with publishing a complete version (i.e. javax.servlet as well as jakarta.servlet) in the next month or two, plus the additional enhancements (grpc-web support, websocket transports, etc) that we've developed. I'll update the linked ticket and this pull request with that information once I've finished this process.

@niloc132
Will you update this ticket here then as well? Any news? | @rufreakde

Will you update this ticket here then as well? Any news?

Yes, when I get the chance to complete this work, I'll update the ticket and this PR. I'm sorry that I havent had time to do so, but as mentioned above, we've shipped the jakarta.servlet jars to maven central already. | @larry-safran Thank you for the review, I'll try to follow up shortly to make those changes. Can you confirm that make those will lead us down the path of getting this merged? I'm a little hesitant to invest further at this point. | Yes.  We can merge if none of the tests are unexpectedly failing and will fix them if they are. | FYI, larry-safran was taking a look because I was saying I wanted to get this over the finish line. This has not been merged for far too long; it'd serve the code and everyone better to get it merged. For the small nits and fixes, I am completely fine with doing them in follow-ups or us maintainers pushing the fixes directly to this PR. I'm also fine with disabling a servlet-specific test or three (like #9308) in order to get it in.
(:-/, #9790 looks to have noticed new style failures which are failing the CI.) | Thank you - I don't mean to sound pessimistic, and will get these done ASAP. I'll have a few followups to propose that we've already made too, but didn't want to further burden this PR with. | @niloc132 Please go ahead and merge this. | @larry-safran, they don't have permission to merge this. For non-maintainer contributions, once we get two approvals we do the merge (typically the one who gives the second approval).
(Make sure to clean up the commit message when squashing.)","Update README etc to reference 1.33.0 | Merge commit '38952b0b3' into servletasync | Update servlet build to catch up to changes at around the 1.25 release | Merge commit '2d592642a' into servletasync | Merge commit '04cf90a9a' into servletasync | Update test to use the new conventions | Merge commit 'df1b67869' into servletasync | Merge commit '5c31dc6d7' into servletasync | Merge commit 'a86fc47c0' into servletasync | Add explicit guava dependency

Previously we transitively got it from grpc-core | Merge commit '7047209ba' into servletasync | Simple typos | Merge commit 'v1.33.0^' into servletasync

This was a mistake, should have merged to where v1.33 diverged | Merge commit '620d26667' into servletasync | Merge commit '73fe68eec' into servletasync | Merge commit '7d9ee8f05' into servletasync | Merge commit '828b03da2' into servletasync | Merge commit 'd2160ea70' into servletasync | Merge commit 'dc74a31be' into servletasync | Merge commit '1e858921e' into servletasync | tomcat | amend tomcat/jetty tests so they build after cherry-pick | Relax jetty rate control | Fix typo in log config, though apparently not used? | Update tests to keep classpaths apart, fix/ignore failures | Tomcat test cleanup, note about intermittent test | Typo in test class | Restore another test setup piece, with fixed dependencies | Update to java-library plugin, build javadoc | Stubbed in servlet-jakarta project, rewrite commit with vers | Simplify tests for javax.servlet | Checkpoint before rewriting without this jakarta plugin... | Working build+test on jakarta, with markers to delete jetty9 workaround | Finish removing jakarta plugin | Merge remote-tracking branch 'upstream/master' into servletasync | Handle Java8 as well as Java11 | Disable another intermittent tomcat test | Import cleanup | Upgrade various versions to latest, add some details about specific
setup | Example servlet readme | Remove note that no longer applies | Correct a different maven repo url | Rearrange test code, add jetty transport test | Update jakarta servlet tests too | Merge remote-tracking branch 'upstream/master' into servletasync | Fix readme link error | Address JdkObsolete build errors | Revert attempt to avoid StringBuffer, and add JdkObsolete suppress instead | Fix build bug preventing jetty10 tests from running | Merge branch 'master' into servlet-niloc132 | Make buildTransportServers() package private and VisibleForTesting | fix bad import | remove logging in test | use org.apache.tomcat:annotations-api instead of javax.annotation | update RELEASING.md | remove unnecessary line | Correctly publish jakarta artifact | Use empty() rather than making an empty byte[] | Remove jetty 9 workaround | Update to latest servlet impls for tests | Configure tomcat to allow tests, remove skipped tests this fixes | retain test SourceSet for (future) normal unit test | disable checkstyle in servlet-jakarta completely | Revert ""update RELEASING.md""

This reverts commit 33639da141c01a832301f437fff1eb6c7fc23198. | make grpc-core an implementation dependency | cleanup AsyncServletOutputStreamWriter | workaround tomcat outputStream.isReayd NPE after asyncCtx.complete | remove timeout rule for UndertowTransportTest | Merge branch 'master' into servlet-niloc132 | add jacoco coverage | fix jacoco | Change packages for jakarta servlet types | Suppress java9+ warnings/errors from tomcat illegal reflective access | Merge branch 'master' into servlet-niloc132 | bump grpc version in example | Un-ignore JettyInteropTest.gracefulShutdown | Add servlet to grpc-all:javadoc | fix jakarta build warning | Concurrency correctness test for AsyncServletOutputStreamWriter | enhance lincheck test | add test for builder scheduler and GrpcServlet constructor | add test for builder scheduler and GrpcServlet constructor 2 | ignore flaky undertow test | add Test annotation | improve builder test | response with error code for GET method | fix executorPool and TIMER_SERVICE leak after servlet.destroy() | Merge branch 'master' into servlet-niloc132 | bump grpc version in example | Merge commit '78ccc81fd' into servletasync

Up to date with master where 1.46 was cut | Merge branch 'master' of https://github.com/grpc/grpc-java into servlet-niloc132 | bump grpc version in example | Use Gradle's version catalog | regression while bumping to v1.47.0 | Merge commit '7bdca0c0e' into servletasync | Merge commit 'e998955d1' into servletasync | Merge commit 'a82ea0cb0' into servletasync | Merge commit 'e325dc911' into servletasync | Merge remote-tracking branch 'upstream/master' into servletasync | Review feedback, version bump"
grpc/grpc-java,9523,https://github.com/grpc/grpc-java/pull/9523,Use real objects instead of mocks.,"My motivation for making this change is that [`ByteBuffer` is becoming
`sealed`](https://download.java.net/java/early_access/loom/docs/api/java.base/java/nio/ByteBuffer.html)
in new versions of Java. This makes it impossible for Mockito's
_current_ default mockmaker to mock it.

That said, Mockito will likely [switch its default
mockmaker](https://github.com/mockito/mockito/issues/2589) to an
alternative that _is_ able to mock `sealed` classes. However, there are
downside to that, such as [slower
performance](https://github.com/mockito/mockito/issues/2589#issuecomment-1192725206),
so it's probably better to leave our options open by avoiding mocking at
all.

And in this case, it's equally easy to use real objects.

As a bonus, I think that real objects makes the code a little easier to
follow: Before, we created mocks that the code under test never
interacted with in any way. (The code just passed them through to a
delegate.) When I first read the tests, I was confused, since I assumed
that the mock we were creating was the same mock that we then passed to
`verify` at the end of the method. That turned out not to be the case.

Given that, I figured I'd switch not only to a real `ByteBuffer` but
also to a real `OutputStream`.",2022-09-07T22:50:31Z,ejona86,cpovirk,easier to read,"The committers listed above are authorized under a signed CLA.✅ login: cpovirk / name: Chris Povirk  (2459bac, ed75a1a) | The test failure looks unrelated to the change:
io.grpc.okhttp.OkHttpServerTransportTest > maxConnectionIdleTimer_respondWithError FAILED
    org.junit.runners.model.TestTimedOutException: test timed out after 10 seconds
        at java.lang.Object.wait(Native Method)
        at java.io.PipedInputStream.read(PipedInputStream.java:326)
        at java.io.PipedInputStream.read(PipedInputStream.java:377)
        at okio.Okio$2.read(Okio.java:140)
        at okio.RealBufferedSource.request(RealBufferedSource.java:72)
        at okio.RealBufferedSource.require(RealBufferedSource.java:65)
        at io.grpc.okhttp.internal.framed.Http2$Reader.nextFrame(Http2.java:120)
        at io.grpc.okhttp.OkHttpServerTransportTest.verifyGracefulShutdown(OkHttpServerTransportTest.java:1104)
        at io.grpc.okhttp.OkHttpServerTransportTest.maxConnectionIdleTimer_respondWithError(OkHttpServerTransportTest.java:206) | Should we then replace all occurrences of ByteBuffer mocks with actual instances instead of this one test? Or is this the only one?
Also you are right about the mock not being used in verify (or any other place). So one can very well use (ByteBuffer) null instead of dest without loss of functionality | As far as I know, this is the only mock of ByteBuffer: I found this one by testing in advance of the JDK upgrade inside Google. So, if there were others, I would have expected to see more failures during that testing. (I also don't see any other hits for git grep '[Mm]ock.*ByteBuffer', but there are other ways to create a mock instance besides that.) If we do find others, I agree that it would make sense to change them, too. | OutputStream is strictly not necessary (other than it being confusing). Do you still want to keep it? | I'm happy either way: Not only is the change not strictly necessary, it also doesn't address all mocking of OutputStream (not to mention InputStream!) in gRPC. Addressing possibly confusion could still be nice, but I'll go whichever way you prefer. | I'll approve if you remove the OutputStream change. We still need another approver CC @ejona86 | Removed, thanks. | I'm happy to see a mock die.","Use real objects instead of mocks.

My motivation for making this change is that [`ByteBuffer` is becoming
`sealed`](https://download.java.net/java/early_access/loom/docs/api/java.base/java/nio/ByteBuffer.html)
in new versions of Java. This makes it impossible for Mockito's
_current_ default mockmaker to mock it.

That said, Mockito will likely [switch its default
mockmaker](https://github.com/mockito/mockito/issues/2589) to an
alternative that _is_ able to mock `sealed` classes. However, there are
downside to that, such as [slower
performance](https://github.com/mockito/mockito/issues/2589#issuecomment-1192725206),
so it's probably better to leave our options open by avoiding mocking at
all.

And in this case, it's equally easy to use real objects.

As a bonus, I think that real objects makes the code a little easier to
follow: Before, we created mocks that the code under test never
interacted with in any way. (The code just passed them through to a
delegate.) When I first read the tests, I was confused, since I assumed
that the mock we were creating was the same mock that we then passed to
`verify` at the end of the method. That turned out not to be the case.

Given that, I figured I'd switch not only to a real `ByteBuffer` but
also to a real `OutputStream`. | Add back accidentally removed import. | Restore mocking for `OutputStream` only."
grpc/grpc-java,9485,https://github.com/grpc/grpc-java/pull/9485,"census,observability: suppress message-events in traces when used by observability","census: provide a flag `addMessageEvents` for the client/server tracers to conditionally emit message events
observability: call the tracers with addMessageEvents set to false",2022-08-26T23:39:54Z,sanjaypujare,sanjaypujare,easier to read,friendly ping @ejona86,"census,observability: suppress message-events in traces when used by observability | address review comments"
grpc/grpc-java,9390,https://github.com/grpc/grpc-java/pull/9390,rls: Change AdaptiveThrottler to use Ticker instead of TimeProvider,"Ticker is based on System.nanoTime, so isn't affected by updates to the system clock like TimeProvider is.

Fixes #9048 ",2022-07-21T18:41:03Z,larry-safran,larry-safran,easier to read,"This PR is for ""rls"" not ""core"". The commit/PR title is incorrect.","core: Change AdaptiveThrottler to use Ticker instead of TimeProvider

Fixes #9048 | Formatting | Add clarifying comment to new test | Change comparison to use subtraction to handle nanoTime overflowing | Fix previous commit by using NULL to mark an invalid slot rather than relying on a low initialization number. | Fix item missed in previous commit | Use a common fakeClock for both tests with setting the amount to rewind being based on the current ticker value plus the 300 ms. | Use a slot being null to mark invalid rather than the slot's endNanos.  Simplifies and increases readability."
grpc/grpc-java,9235,https://github.com/grpc/grpc-java/pull/9235,api: Add GlobalInterceptors API,"GlobalInterceptors API will be used to set global interceptors and global stream tracers.  

The API is intended for `Internal` use only for time being. It currently supports 

- `ClientInterceptor`
- `ServerInterceptor`
- `ClientStreamTracer Factory`

Sample use case:
These interceptors and tracer factories can be set by grpc observability and be queried later by grpc to set into each builder created as well as by direct users of Netty channel/server builders (as required).

CC @sanjaypujare 
",2022-06-09T23:27:25Z,sanjaypujare,DNVindhya,easier to read,@ejona86 you may want to take a look at this PR...,added GlobalInterceptors API | added single setter for interceptors and ServerStreamTracer factory | updated double locking with synchronized accessor | addressed comments | added InternalGlobalInterceptors API; removed non-static methods | replaced ImmutableList with Collections.unmodifiableList; addressed comments (2)
grpc/grpc-java,9271,https://github.com/grpc/grpc-java/pull/9271,Use Gradle's version catalog,"Note that there are two commits here. One makes a preping change.
I've confirmed the poms produced are equivalent.

This moves our depedencies into a plain file that can be read and
updated by tooling. While the current tooling is not particularly better
than just using gradle-versions-plugin, it should put us on better
footing. gradle-versions-plugin is actually pretty nice, but will be
incompatible with Gradle 8, so we need to wait a bit to see what the
future holds.
    
Left libraries as an alias for libs to reduce the commit size and make
it easier to revert if we don't end up liking this approach.
    
We're using Gradle 7.3.3 where it was an incubating fetaure. But in
Gradle 7.4 is became stable.

CC @YifeiZhuang ",2022-06-14T21:04:11Z,ejona86,ejona86,easier to read,"Sidenote: didn't notice any issues with building this with IntelliJ. | Yes, I meant to mention that plugins would be done in the future. I'm honestly pretty happy with how plugins are defined today; I actually wish the same ""define the dependency and just leave off the version"" approach was available for normal dependencies. With auto-upgrading it could be worthwhile, even if it requires a different syntax at all callsites. Right now I didn't 1) because this is large enough already and 2) we do some funky business with android in settings.gradle and I wanted to wait to see if that confuses any tooling.
Yeah, I saw the bundles but didn't see a need for them yet. If Netty used runtime scope instead of compile scope for its dependencies, they would probably be more useful to use. | I actually wish the same ""define the dependency and just leave off the version"" approach was available for normal dependencies

Another note: this is possible today, but it requires using a platform/version constraints. And those change all transitive dependencies which won't mix well with Maven and honestly not what I want because it can cause downgrades.","Remove classifiers from ext.libraries

Classifier can't be specified in version catalog. | Use Gradle's version catalog

This moves our depedencies into a plain file that can be read and
updated by tooling. While the current tooling is not particularly better
than just using gradle-versions-plugin, it should put us on better
footing. gradle-versions-plugin is actually pretty nice, but will be
incompatible with Gradle 8, so we need to wait a bit to see what the
future holds.

Left libraries as an alias for libs to reduce the commit size and make
it easier to revert if we don't end up liking this approach.

We're using Gradle 7.3.3 where it was an incubating fetaure. But in
Gradle 7.4 is became stable."
grpc/grpc-java,9214,https://github.com/grpc/grpc-java/pull/9214,Enable xDS custom LB config by default.,,2022-05-27T00:03:14Z,temawi,temawi,easier to read,,Enable xDS custom LB config by default.
grpc/grpc-java,9103,https://github.com/grpc/grpc-java/pull/9103,xds: A new wrr_locality load balancer.,This LB is the parent for weighted_target and will configure it based on the child policy it gets in its configuration and locality weights that come in a ResolvedAddresses attribute.,2022-04-28T20:51:47Z,temawi,temawi,easier to read,,"xds: A new wrr_locality load balancer.

This LB is the parent for weighted_target and will configure it based on the child policy it gets in its configuration and locality weights that come in a ResolvedAddresses attribute. | Review updates. | Remove unused variables. | Make GracefulSwitchLoadBalancer field final. | Use GracefulSwitchLoadBalancer unconditionally. | Remove unnecessary null checks."
grpc/grpc-java,8750,https://github.com/grpc/grpc-java/pull/8750,rls: fix child lb leak when client channel is shutdown,"When client channel is shutting down, the RlsLoadBalancer is shutting down. However, the child loadbalancers of RlsLoadBalancer are not shut down. This is causing the issue b/209831670

This PR contains two commits and it will be easier to review separately. The 1st commit change the lifecycle of child lb to make it easier to manage. The 2nd commit is to shut down all child lbs when the parent lb is shut down.",2022-01-12T22:58:44Z,dapengzhang0,dapengzhang0,easier to read,"@sergiitk A gentle ping. | Per IRL discussion, this needs more testing/fixing, so the review is on hold. | Per IRL discussion, this needs more testing/fixing, so the review is on hold.

Oh, I misunderstood you. I just meant it's higher priority than the other RLS PR. It's ready for review. The test was added (https://github.com/grpc/grpc-java/pull/8750/files#diff-14b2d9691a6f8fc60385a0bcf960a20b51b7415d9119e02e22d17cccc8103349R176). | @sergiitk Thank you so much for the review. I know this PR as well as RLS is so complex and hard to read.","init lb when ChildPolicyWrapper is created | rls: fix child lb leak when client channel is shutdown | fix synchronization | Merge branch 'master' into fix-rls-child-lb-leak | checkNotNull, final | fix concurrency issue for RefCountedChildPolicyWrapper | assertWithMessage | invalidAll by iterating entrySet()"
grpc/grpc-java,8956,https://github.com/grpc/grpc-java/pull/8956,observability: implement client interceptor for logging,"This PR implements Client Interceptor for logging i.e `InternalLoggingChannelInterceptor`. Events are intercepted and a log record is written to logging as part of intercepting methods. 

For now, following events are being logged,
- Request Header
- Response Header
- Request Message
- Response Message
- Trailer
- Half Close 
- Cancel

Implemented Sink(`GcpLogSink`) for GCP Cloud Logging, which uses Google Cloud Logging java client library for writing entries to cloud logging.

",2022-03-17T05:09:25Z,sanjaypujare,DNVindhya,easier to read,CC @sanjaypujare,"implement GcpLogSink | added helper methods for logging events | implement logging client interceptor; initialize sink | added rpc_id, InetAddressUtil (temp) | added unit tests for logger helper methods | added unit tests for client interceptor and sink | refactored GcpLogSink to not invoke external API during test; addressed comments | updated testcase to not invoke cloud logging API | refactored log helper class; added test cases | moved ByteArrayMarshaller to LogHelperTest | added TODO to use destinationProjectId from config | addressed comments"
grpc/grpc-java,8095,https://github.com/grpc/grpc-java/pull/8095,core: clarify exception message,`SizeEnforcingInputStream` is applied on the decompressed stream. It looks like it has been the case since it was introduced in https://github.com/grpc/grpc-java/commit/15f02ba19c4a2c21afee39cb73cd0340ad2f9d1d.,2021-09-02T22:18:01Z,ejona86,bjaglin,easier to read,"The committers are authorized under a signed CLA.✅  Brice Jaglin (a964c87) | I don't think the message is wrong, but it can be misleading. It was a compressed message. The part that is unclear is whether the compressed or uncompressed size exceeded the max size. It is useful for us to have ""compression"" in the error messages for these compression-specific error paths, even if at this precise point it is dealing with uncompressed data. The fact it was ever compressed is important.
Looking at the ""%d bytes read"", that's actually not all that useful since we haven't fully-decompressed the message. Removing that would also be fair, since it is also misleading.
Maybe something like this would clear up the situation:
""Compressed gRPC message exceeded maximum size %d during decompression""
""Decompressed gRPC message exceeds maximum size %d"" | Thanks for the review. I went for your second suggestion in the amended commit. Either of them would have made my investigation easier, and fit well with the ""stream check"" (for which we don't know whether compression is present or not) at 
  
    
      grpc-java/core/src/main/java/io/grpc/internal/MessageDeframer.java
    
    
         Line 389
      in
      800f828
    
  
  
    

        
          
           String.format(""gRPC message exceeds maximum size %d: %d"",","core: clarify exception message

Reformulate message to highlight that SizeEnforcingInputStream is
applied on the message size of the message after decompression."
grpc/grpc-java,8175,https://github.com/grpc/grpc-java/pull/8175,advancedtls: adding AdvancedTlsX509TrustManager and AdvancedTlsX509KeyManager,"This pull request adds the following classes to `io.grpc.util`:

1. an `AdvancedTlsX509TrustManager` that supports 
   -  reloading root certificates from the file system or memory
   -  disabling host name verification check, and applying any custom-implemented  in-handshake verification checks 
2. an `AdvancedTlsX509KeyManager` that supports
   -  reloading identity credentials(private key and certificate chain)  from the file system or memory

This also adds the integration tests to `io.grpc.netty`.",2021-08-17T23:13:31Z,ZhenLian,ZhenLian,easier to read,"Hi @ejona86 , this is the rest of the parts for the advancedtls code. Could you please take a look at it please?
The Travis CI passes, while I still got a few other pre-submit failures that I will need to solve. But I think the majority of the code is there. Feel free to let me know if you have any questions/concerns about this. Thank you in advance for your review! | Some questions/comments:


This should fix #5335?


I would like support for PKCS1 keys (generated by e.g. Istio), would you accept a small merge to CertificateUtils?


Can OpenSslCachingX509KeyManagerFactory cause issues by caching the key material? (have not looked into it deeper)


Any reason for using a scheduled executor and not a file watcher, similar to e.g.?
https://github.com/cloudfoundry/java-buildpack-security-provider/blob/main/src/main/java/org/cloudfoundry/security/FileWatchingX509ExtendedKeyManager.java | @ejona86 Thanks for the review so far! There are still two remaining comments and I will work on fixing them the week after next week(Just FYI, I am OOO next week). | @cfredri4 Please see my comments below.

This should fix #5335?

Yeah, strictly speaking, this is a utility class, so it doesn't ""fix"" anything, but it did make reloading certificates easier in Java.

I would like support for PKCS1 keys (generated by e.g. Istio), would you accept a small merge to CertificateUtils?

Sure! I think in general, contributions to support other key formats are welcome. Probably @ejona86 can help review it, or I can also review once I am back(I am OOO next week).

Can OpenSslCachingX509KeyManagerFactory cause issues by caching the key material? (have not looked into it deeper)

I am not entirely sure what kind of issues are in your mind. If using the classes here to build your credentials in Netty, we will use SimpleKeyManagerFactory as the key manager factory. This is a simple implementation without any caching mechanism, so I don't see any correlations with OpenSslCachingX509KeyManagerFactory, unless I miss something.

Any reason for using a scheduled executor and not a file watcher, similar to e.g.?

Sorry, I didn't get a chance to look at the full details of the code you pointed me, but it seems the file watcher you pointed me is also using a executor(https://github.com/cloudfoundry/java-buildpack-security-provider/blob/a70aa0c6e87e311fcb645ebd747864e33426d101/src/main/java/org/cloudfoundry/security/FileWatcher.java#L43). So I guess inherently both ways are similar? | @ZhenLian ⬇️


I would like support for PKCS1 keys (generated by e.g. Istio), would you accept a small merge to CertificateUtils?

Sure! I think in general, contributions to support other key formats are welcome. Probably @ejona86 can help review it, or I can also review once I am back(I am OOO next week).

Given the small change required, I added a gist here: https://gist.github.com/cfredri4/5655c1e54dc94f236054ec97b43e0a08
Please look over feel free to change as required.


Can OpenSslCachingX509KeyManagerFactory cause issues by caching the key material? (have not looked into it deeper)

I am not entirely sure what kind of issues are in your mind. If using the classes here to build your credentials in Netty, we will use SimpleKeyManagerFactory as the key manager factory. This is a simple implementation without any caching mechanism, so I don't see any correlations with OpenSslCachingX509KeyManagerFactory, unless I miss something.

I had some recollection of seeing that OpenSslCachingX509KeyManagerFactory will wrap the given KeyManagerFactory and do caching, but after checking further it seems that this is only done when KeyManagerFactory has just been constructed from key material (not when given an existing KeyManagerFactory).


Any reason for using a scheduled executor and not a file watcher, similar to e.g.?

Sorry, I didn't get a chance to look at the full details of the code you pointed me, but it seems the file watcher you pointed me is also using a executor(https://github.com/cloudfoundry/java-buildpack-security-provider/blob/a70aa0c6e87e311fcb645ebd747864e33426d101/src/main/java/org/cloudfoundry/security/FileWatcher.java#L43). So I guess inherently both ways are similar?

The difference is that with a file watcher, the thread is sleeping until the file is changed and then woken up, i.e. there is no polling being done and changes are immedately notified. | @cfredri4  Sorry for the delay. I just got some time to work on this.

Given the small change required, I added a gist here

The changes look good to me overall. Feel free to open a PR for that, and we can iterate on that. We will also need some tests.

The difference is that with a file watcher, the thread is sleeping until the file is changed and then woken up, i.e. there is no polling being done and changes are immedately notified.

OK I see. We've considered both approaches(polling model or the notification model), but finally decided to go for the polling model, for the following reasons:

The Java notification API we considered at that moment was WatchService, which seems to have some limitations, e.g. ""If a watched file is not located on a local storage device then it is implementation specific if changes to the file can be detected""
We want this behavior to be consistent across multiple gRPC languages, but it seems at least for some languages, the actual notification behavior highly depends on the system it uses, which is hard to unify
If we adopt the notification model, if the credentials are updated and we are notified, but for some reasons our credential update failed(it could happen when the update is not done atomically), we won't get notified again with this newer version of credentials. But if we choose the polling model, hopefully the newer version can be picked up by the next poll | @ejona86 Hi Eric, I think I've fixed all the comments and the build failures. Can you please take another look again, please? Thank you so much! | Discussed outside of github. The KeyManager was looking better before the swap to KeyStore, although a KeyStore would be fine if we used KeyStore.Entry/PrivateKeyEntry; as-is though the KeyStore is pretty annoying and doesn't add much value. The TrustManager would be very well suited to mostly just save the delegate TrustManager as a field and delegate to it simply, instead of storing the KeyStore and the like.
We also discussed the fact that the KeyManager is racy, because the API itself is racy when an alias mutates. There's an option to mostly avoid the race by changing the alias name each time we do an update, and keeping a previous update. But that might not be essential. | @ejona86 Hi Eric, I've updated the code based on the discussion we had last Friday. Would you mind taking a look again please? Thank you so much for the review so far! | Thanks again for the review! Merging now...",a basic test using new credential API | add advanced TLS classes and more tests | fix style errors | add sleep period | resolve some of the comments | resolve comments; add more tests | fix the build issue | change to use key store only; fix the latest comments | updates based on the discussion offline; add a new test | fix the latest comments | fix latest comment | small fix
grpc/grpc-java,8031,https://github.com/grpc/grpc-java/pull/8031,binder: BinderTransport implementation.,"This is the first major code drop for binderchannel, containing the actual transport class and its internals.

Channel & server builders will follow in a subsequent PR, but this is enough to have BinderTransportTest running.
",2021-05-26T12:54:32Z,markb74,markb74,easier to read,,"Merge pull request #1 from grpc/master

Sync | Merge pull request #2 from grpc/master

Sync | Merge pull request #3 from grpc/master

Sync | Merge branch 'master' of https://github.com/grpc/grpc-java | binderchannel: BinderTransport implementation.

This is the first major code drop for binderchannel, containng
the actual transport class and its internals. | fix bad param | Small update.

rename a method, fix a bad javadoc comment, and fix a bug. | address review comments | Break dependency on protolite, since it was only used for javadoc. | Address review comments. | Fix test shutdown in the HostServices helper.

This is the same set of changes made internally in cl/370111269. | Address review comments. | Make outbound more understandable."
grpc/grpc-java,7960,https://github.com/grpc/grpc-java/pull/7960,grpclb: turn into TRANSIENT_FAILURE if given (by balancer or fallback) an empty list of addresses,"Fixes #7935

There are two cases gRPCLB needs to handle an empty list of backend addresses:
- The remote balancer gives an empty list.
  - #6734 partially addressed the issue, but it only solved pick_first's immediate problem. Also, that change makes round_robin and pick_first diverge in terms of the behavior for receiving an empty list of backend addresses: [round_robin buffers RPCs](https://github.com/grpc/grpc-java/blob/9c562c8a6fb216e6de80310a3f00b10edbc16e93/grpclb/src/main/java/io/grpc/grpclb/GrpclbState.java#L796-L799) (which could potentially hang forever as fallback timer had already been cancelled when receiving the response) while [pick_first fails RPCs immediately](https://github.com/grpc/grpc-java/blob/9c562c8a6fb216e6de80310a3f00b10edbc16e93/grpclb/src/main/java/io/grpc/grpclb/GrpclbState.java#L806-L810).  
 
- The fallback list given by the resolver, when it is used, turns out to be empty. This is what #7935 is describing.
  - In this case, both round_robin and pick_first buffer RPCs.


This PR fixes the entire problem. It turns into TRANSIENT_FAILURE as long as the list of backends to be used is found to be empty.

There could be other choices of behaviors such as do not cancel the fallback timer if balancer gives an empty list and only turn into TF if fallback also fail. The choice in this PR is the simplest handling, we don't unnecessarily complicate ourselves.",2021-03-13T01:25:33Z,voidzcy,voidzcy,easier to read,,Turn LB state into TRANSIENT_FAILURE if an empty list of backend addresses is given. It applies to both the address list given by the balancer and fallback list given by the resolver whenever it is being used. | Merge branch 'master' of github.com:grpc/grpc-java into bugfix/grpclb_turn_to_tf_if_no_addrs | Impove style.
grpc/grpc-java,7932,https://github.com/grpc/grpc-java/pull/7932,xds: HttpFilter support,"See for 

Design doc:
https://github.com/grpc/proposal/blob/master/A39-xds-http-filters.md

C core impl:
https://github.com/grpc/grpc/pull/25558

Go impl:
https://github.com/grpc/grpc-go/pull/4206

The java approach is similar to Go because they both use FilterConfigs to generate interceptor rather than method config in ConfigSelector.

The new API `io.xds.Filter` is equivalent to `httpfilter.go`.",2021-03-17T23:37:14Z,dapengzhang0,dapengzhang0,easier to read,"Btw, don't you think the tests are weak/test too litttle?

What (strong) tests do you think should be added? | Don't forget to make types of supported HttpFilter messages printable.",xds: HttpFilter support | add tests | move constructor after fields declaration | add javadoc and rename field name in Registry | Merge branch 'master' of https://github.com/grpc/grpc-java into httpfilter | minor cleanup | change parseFilterConfig argument type | Merge branch 'master' of https://github.com/grpc/grpc-java into httpfilter | remove empty line | fix typo | use util method to avoid duplicate code | futhure reduce duplicate code | enhance error message | remove unused fields in FaultConfig data type | change generateServiceConfig() signature | Merge branch 'master' of https://github.com/grpc/grpc-java into httpfilter | move test | append LameFilter | override toString() for filter config | Merge branch 'master' of https://github.com/grpc/grpc-java into httpfilter | simplify field assignment | remove hasLameFilter() | equals() and hashCode() | add HttpFilter to printables | refactor Filter.Registry | refactor NamedFilter | rename Filter.StructOrError to Filter.ConfigOrError
JabRef/jabref,12018,https://github.com/JabRef/jabref/pull/12018,"Add a title guess method to get ""better"" title","- Closes:#11999
- As the issue said I should ""traditional"" (non-AI) code, so I write a AI-liked code in a simple traditional way. Since I don't have enough data, so the rules is limited now.
- I didn't add it `PdfMergeMetadataImporterTest` now, since I'm not sure if we like this kind of method or not, so I want to set up the PR with a demo code to discuss it first. I'll add it to `PdfMergeMetadataImporter` with a better clear code if we like it.
<!-- 
Describe the changes you have made here: what, why, ... 
Link the issue that will be closed, e.g., ""Closes #333"".
If your PR closes a koppor issue, link it using its URL, e.g., ""Closes https://github.com/koppor/jabref/issues/47"".
""Closes"" is a keyword GitHub uses to link PRs with issues; do not change it.
Don't reference an issue in the PR title because GitHub does not support auto-linking there.
-->

### Mandatory checks

<!-- 
- Go through the list below. Please don't remove any items.
- [x] done; [ ] not done / not applicable
-->

- [x] I own the copyright of the code submitted and I licence it under the [MIT license](https://github.com/JabRef/jabref/blob/main/LICENSE)
- [ ] Change in `CHANGELOG.md` described in a way that is understandable for the average user (if applicable)
- [x] Tests created for changes (if applicable)
- [x] Manually tested changed features in running JabRef (always required)
- [ ] Screenshots added in PR description (for UI changes)
- [ ] [Checked developer's documentation](https://devdocs.jabref.org/): Is the information available and up to date? If not, I outlined it in this pull request.
- [ ] [Checked documentation](https://docs.jabref.org/): Is the information available and up to date? If not, I created an issue at <https://github.com/JabRef/user-documentation/issues> or, even better, I submitted a pull request to the documentation repository.
",2024-10-30T09:32:06Z,koppor,leaf-soba,understandability,"Good to have a test case.
But I think, there should be some earlier switch. See the hint stripper.setSortByPosition(true);
Please try again longer how to handle two-column PDFs. -- The title should be parsed corectly by org.jabref.logic.importer.fileformat.PdfContentImporter#getEntryFromPDFContent
What I meant is: Do two passes: One with sorted by coordinates and one keeping text blocks together. The abstract is probably better prased with keeping text blocks together; the title better with position.
One can tweak org.jabref.logic.importer.fileformat.PdfContentImporter#getEntryFromPDFContent to see if the paper is a single-column or two column paper. One could even go further and detect if it is Springer, IEEE or another format.

Wow, I thought I wrote something cool and AI-like method, but it looks like it’s not quite what we need! 😅 I’ve now added a method to grab the title by position. This is just a demo code to see if the approach works for us—there’s definitely room for improvement, especially in formatting and commented code. If we’re happy with it, I’ll clean things up and refactor the org.jabref.logic.importer.fileformat.PdfContentImporter#getEntryFromPDFContent input! | Since I don't have enough data, so the rules is limited now.


Please list the files available for others to be able to review.
I think, JabRef should have an example file for the first four templates listed at https://latextemplates.github.io/.
If not, pleae add.
I see following dirctories containing pre-formatted files

https://github.com/JabRef/jabref/tree/main/src/test/resources/org/jabref/logic/importer/util
https://github.com/JabRef/jabref/tree/main/src/test/resources/pdfs/IEEE

And some more at https://github.com/JabRef/jabref/tree/main/src/test/resources/pdfs - where I think, most of them do not follow a publisher format.

I thought that first dowing a parsing regarding position sorted for title and author, then reparsing with onsorted, but with blocks for abstract is the most working approach.
First, the format of the PDF should be guessed. If IEEE, then set parameters for IEEE. If LNCS, set format for LNCS.
Guessing the title by a rectangle works if you know the publisher format. Thus, that guessing should go first.
For 563 TB of test data - outside Europe and US and maybe many other countries, go to https://annas-archive.org/. -- You can also start with 32 TB of test data. | Since I don't have enough data, so the rules is limited now.


Please list the files available for others to be able to review.
I think, JabRef should have an example file for the first four templates listed at https://latextemplates.github.io/.
If not, pleae add.
I see following dirctories containing pre-formatted files

https://github.com/JabRef/jabref/tree/main/src/test/resources/org/jabref/logic/importer/util
https://github.com/JabRef/jabref/tree/main/src/test/resources/pdfs/IEEE

And some more at https://github.com/JabRef/jabref/tree/main/src/test/resources/pdfs - where I think, most of them do not follow a publisher format.
I thought that first dowing a parsing regarding position sorted for title and author, then reparsing with onsorted, but with blocks for abstract is the most working approach.
First, the format of the PDF should be guessed. If IEEE, then set parameters for IEEE. If LNCS, set format for LNCS.
Guessing the title by a rectangle works if you know the publisher format. Thus, that guessing should go first.
For 563 TB of test data - outside Europe and US and maybe many other countries, go to https://annas-archive.org/. -- You can also start with 32 TB of test data.


Sorry, so far I have only tested it with the single file mentioned in the issue, se2paper.pdf. I haven’t written academic papers in years, so I’m not very familiar with the IEEE and LNCS formats. I’ll do my best to learn and research them from the test data tomorrow | I added 4 more unit test case with ""IEEE"", ""LNCS"", ""scientificThesis"", and a ""thesis-example"" I found in the pdf folder, new code can pass them all so far, if we need more test I can add it.
The new idea is the title should be the largest font size in most of papers which I checked in the test data. | Remove commented code. You have it in your git history if you ever need it. | Remove commented code. You have it in your git history if you ever need it.

sorry it is a dirty code now, I want to set up a demo code to check if we like this solution or not, if the solution confirmed I'll refactor it. | Please propose a PR to the user documentation at https://docs.jabref.org/collect/findunlinkedfiles#pdfs-for-which-it-works - explaining the larger titles. | I am currently very busy, thus quick replies only. Sorry for that.

I added 4 more unit test case with ""IEEE"", ""LNCS"", ""scientificThesis"", and a ""thesis-example"" I found in the pdf folder,

Sounds good.
The idea using the largest object is smart! For future work, maybe, the second page needs to be checked, too. Sometimes, there is a cover page in front. I currently do not have an example at hand, but I don't have an example at hand - and I think, the heuristics will get harder then.

if we need more test I can add it.

Can you add follwing papers as test cases:

https://link.springer.com/article/10.1007/s10664-023-10367-y
https://link.springer.com/article/10.1007/s10664-020-09875-y --> note that it would be great if the sub title was parsed
https://onlinelibrary.wiley.com/doi/10.1002/spe.3169
https://peerj.com/articles/cs-213/#
https://dl.acm.org/doi/10.1145/3597503.3639130 (ACM format)

When adding, add a README.md to the folder where there are stored with the source of the PDF. I mean, a table with filename as first column and link as second column.

In all these papers, you can also test for the abstract.
In all thesse papers, you can test for the authors (which is probably a brain-teaser and left for future work). | Follow-up (copied from the docs): The next development step is to extract the title of the PDF, use the ""Lookup DOI"" functionality and then merge that information. -- I think, there is already much code existing using the merge, but the lookup DOI is not done yet. Could be a three-liner. | I add 5 more unit test and the old code can pass 9 of them, 1 of them(s10664-023-10367-y,""Do RESTful API design rules have an impact on the understandability of Web APIs?"") have a special icon in the bottom right with an extra large font size character in it. I check some test data most of titles should not be in the place 10% bottom, so I change my code a little to fix it.

ToDo in this PR:


add a README.md to the folder where there are stored with the source of the PDF.
refactor my code to make the logic clear.
extract the title of the PDF, use the ""Lookup DOI"" functionality and then merge that information.(Or it should be in a new PR?)


ToDo in a new PR:


get abstract in a new PR
get authors in a new PR
get subTitle in a new PR
propose a PR to the user documentation https://docs.jabref.org/collect/findunlinkedfiles#pdfs-for-which-it-works explaining the larger titles. | I add 5 more unit test and the old code can pass 9 of them, 1 of them(s10664-023-10367-y,""Do RESTful API design rules have an impact on the understandability of Web APIs?"") have a special icon in the bottom right with an extra large font size character in it. I check some test data most of titles should not be in the place 10% bottom, so I change my code a little to fix it.

Nice!

1. add a `README.md` to the folder where there are stored with the source of the PDF.


Yes.
Even better would be to

Have a test-pdfs.bib file, where ich .pdf in the folder is linked
Each entry in the test-pdfs.bib has the url set
README.md says: ""Open test.pdfs.bib to see information on the PDFs""
Rename all PDFs to [bibtexkey] - [title] pattern

I know, this is 30 min of concentrated work - but maybe easier than the README.md generation.

2. refactor my code to make the logic clear.


If you see something. Otherwise, just remove commented-out code (and adress my comments)
We need to get this in soon, because we have > 50 pull requests open - and all reviewers loose context. If this is around for weeks, we start from scratch at revewieing.

3. extract the title of the PDF, use the ""Lookup DOI"" functionality and then merge that information.(Or it should be in a new PR?)


New pull request - to keep PRs small and reviewable.

* ToDo in a new PR:


You can do each of them in a seperate PR.
If the code is ""far enough"" from each other in the file, you can use https://gitbutler.com/ to create separate PRs, but have one ""merged"" view on yoru machine.

4. propose a PR to the user documentation [docs.jabref.org/collect/findunlinkedfiles#pdfs-for-which-it-works](https://docs.jabref.org/collect/findunlinkedfiles#pdfs-for-which-it-works) explaining the larger titles.


This is already done: JabRef/user-documentation#537 - Update I checked. I think, I did not. Update still required. | If you see something. Otherwise, just remove commented-out code (and adress my comments)
We need to get this in soon, because we have > 50 pull requests open - and all reviewers loose context. If this is around for weeks, we start from scratch at revewieing.

Definitely, I’ll start by removing the commented-out code and addressing your feedback tomorrow.
Apologies for the delay—since I’ve been balancing this with my full-time job, I worked on it in smaller parts. I understand this has taken more time, and I appreciate your patience. While I’m still getting familiar with this project, I’m comfortable with code reviews, so please feel free to assign any tasks to help move things forward. | revert the temp change of unit test to original one.
all of title's character should stay together, add a isFarAway method to it to pass the unit test for hello world case.
change guess title variable name form old version titleByPosition to titleByFontSize
remove all commented code.
Add a @VisibleForTesting to getEntryFromPDFContent.
rewrite the javaDoc for getEntryFromPDFContent
fix the logic issue for setting title. | I added a CHANGELOG.md entry | @leaf-soba I think, you need to do some cleanup of the filenames before a merge can go through. See test output on Windows test:
 Error: error: invalid path 'src/test/resources/pdfs/PdfContentImporter/peerj-cs-213 - On the impact of service-oriented patterns on software evolvability: a controlled experiment and metric-based analysis.pdf'

Sorry für that.
Maybe, you can adress all comments by me then. | Sorry I know it should be in another follow-up PR as you said before we should keep PR small but this one is too big, but I need to change the file name to pass the CI workflow, so it is a little hard to separate them.

Set . as library-specific directory (in the library properties)
This one is only comment I didn't finish, since I don't understand it very clearly.

Do you mean:

Open JabRef, load my BibTeX file.
Go to Library → Library Properties.
In File Directory, enter a .
save my settings

But I don't know how to change it in code or project file as a PR, could you explain it in the follow-up PR. | Do you mean:

[...]]

4. save my settings


Yes.
Then save the file. The .bib file is modified then. Then you can normally usie ""git"" to commit ato a new branch.
Since this is not a show-stopper, I let this PR an and we can do that in a follow-up PR.","Add title guess method

Add title guess method | fix unit test

fix unit test | update unit test to JDK 21 style

update unit test to JDK 21 style | update unit test

update unit test | update get title by area

update get title by area | remove StringUtils.isBlank and add @AllowedToUseAwt

remove StringUtils.isBlank and add @AllowedToUseAwt | add unit test

ToDo:find a minimal pdf for test | change to get title by font size

change to get title by font size
add more unittest | Merge branch 'main' into close-issue-11999 | RemoveTestPrefix

RemoveTestPrefix | Merge branch 'close-issue-11999' of https://github.com/leaf-soba/jabref into close-issue-11999 | temp fix the unit test

I should change the pdf used in importTwiceWorksAsExpected, or my code need to deal with the paper with same font size in AUTHOR and TITLE? | fix the unit test and open rewrite issue

fix the unit test and open rewrite issue | remove commented code

remove commented code | Add 5 more unittest case

Add 5 more unittest case | resolve all comments so far

1. revert the temp change of unit test to original one.
2. all of title's character should stay together, add a `isFarAway` method to it to pass the unit test for hello world case.
3. change guess title variable name form old version `titleByPosition` to `titleByFontSize`
4. remove all commented code.
5. Add a `@VisibleForTesting` to `getEntryFromPDFContent`.
6. rewrite the javaDoc for `getEntryFromPDFContent`
7. fix the logic issue for setting title. | remove Blank line at start of block

remove Blank line at start of block | rename and replace unit test file

rename and replace unit test file | add bib and readme.md

add bib and readme.md | Merge branch 'main' into close-issue-11999 | Update CHANGELOG.md | rename the file to pass CI

rename the file to pass CI | address all comments

address all comments | fix the file name in unit test

fix the file name in unit test"
apache/accumulo,5980,https://github.com/apache/accumulo/pull/5980,Reduce verbose log messages,Moves some of these log messages from debug to trace to help reduce output in log files.,2025-11-22T02:18:08Z,ddanielr,ddanielr,"readability, readable",,"Reduce unnessessary log messages

Moves some of these log messages from debug to trace to help reduce
output in log files. | Adds store name and separates deadlog lines

Adds the store name as a log message prefix.
Separates the map print log message from a single line message into a
line per dead tserver"
apache/accumulo,5737,https://github.com/apache/accumulo/pull/5737,(trivial) Improve readability of logIfFixed,Based on discussion in https://github.com/apache/accumulo/pull/5632#discussion_r2169727023,2025-07-14T13:23:02Z,kevinrr888,kevinrr888,"readability, readable",,Improve readability of logIfFixed
apache/accumulo,5874,https://github.com/apache/accumulo/pull/5874,Migrate instanceof usage from java 11 to improved java 17 pattern,"use Java 17 pattern matching for `instanceof` checks, eliminating redundant casting and improving readability.

  ### Example Transformation
Before (Java 11):
  ```java
  if (obj instanceof String) {
      String str = (String) obj;
      return str.length();
  }
```

  After (Java 17):
  ```java
  if (obj instanceof String str) {
      return str.length();
  }
```

These changes were made by my IDE",2025-09-11T14:32:42Z,DomGarguilo,DomGarguilo,"readability, readable",,IDE auto-migrate to instanceof pattern
apache/accumulo,5894,https://github.com/apache/accumulo/pull/5894,Manager info on Monitor `rest/manager` -> `rest-v2/manager`,"Manager info on the Monitor (displayed on homepage (`/`) and Manager page (`/manager`)) now obtains and displays info from the existing (but unused) `rest-v2/manager` endpoint.

* Updated the homepage to display table data corresponding to the `rest-v2/manager` endpoint.
* Updated the manager page to display table data corresponding to the `rest-v2/manager` endpoint.
* Created new endpoint `rest-v2/manager/metrics` which returns the Manager metrics as json. This is linked in the Manager table in both the homepage and the manager page.
* Server navigation bar now only considers the Manager status (as obtained from `rest/status`) (ERROR, WARN, OK). Previously took into account the manager state (e.g., if the state or goal state was SAFE_MODE or CLEAN_STOP), but this info was only included in the `rest/manager` endpoint.
* Deleted the `rest/manager` endpoint and associated code used to gather data for this endpoint.
* Deleted `systemAlert.js` and `systemAlert.ftl` as this alert was entirely based on Manager info included in `rest/manager` (manager state being SAFE_MODE or CLEAN_STOP). No longer applicable with `rest-v2/manager`.
* Removed no longer applicable tables: badTServersTable and deadTServersTable from `/tservers` page.

closes #5882",2026-02-02T16:27:36Z,kevinrr888,kevinrr888,"readability, readable","The following are images of some of the Monitor pages with these changes
Image of homepage with Manager up:

Image of Manager page with Manager up:

Image of new rest-v2/manager/monitor endpoint

Image of existing rest-v2/manager endpoint (no metrics)

Image of homepage with Manager down:

Image of Manager page with Manager down: | The following are images of some of the Monitor pages before these changes
Image of homepage with Manager up:

Image of Manager page with Manager up:

Image of rest/manager endpoint

Image of homepage with Manager down:

Image of Manager page with Manager down: | Ran locally. The manager pages worked but I could never get metric information to display at rest-v2/manager/metrics
It would just return a 200 code and [] | Enabled auto-refresh and did get an error message
Uncaught ReferenceError: updateSystemAlerts is not defined
    at refreshNavBar (navbar.js:131:3)
    at functions.js:71:5 | Ran locally. The manager pages worked but I could never get metric information to display at rest-v2/manager/metrics
It would just return a 200 code and []

Metrics aren't configured to emit by default. Can view the pictures I posted above for an idea, or configure accumulo to gather metrics. | @dlmarion do you have any more comments/concerns? Otherwise, I'll merge this in | @dlmarion do you have any more comments/concerns? Otherwise, I'll merge this in

No issue with you merging this. @DomGarguilo captured the follow-ons.","Manager info on Monitor `rest/manager` -> `rest-v2/manager`

Manager info on the Monitor (displayed on homepage (`/`) and Manager page (`/manager`)) now obtains and displays info from the existing (but unused) `rest-v2/manager` endpoint.

* Updated the homepage to display table data corresponding to the `rest-v2/manager` endpoint.
* Updated the manager page to display table data corresponding to the `rest-v2/manager` endpoint.
* Created new endpoint `rest-v2/manager/metrics` which returns the Manager metrics as json. This is linked in the Manager table in both the homepage and the manager page.
* Server navigation bar now only considers the Manager status (as obtained from `rest/status`) (ERROR, WARN, OK). Previously took into account the manager state (e.g., if the state or goal state was SAFE_MODE or CLEAN_STOP), but this info was only included in the `rest/manager` endpoint.
* Deleted the `rest/manager` endpoint and associated code used to gather data for this endpoint.
* Deleted `systemAlert.js` and `systemAlert.ftl` as this alert was entirely based on Manager info included in `rest/manager` (manager state being SAFE_MODE or CLEAN_STOP). No longer applicable with `rest-v2/manager`. | removed unused declared dependency | Minor fixes:

- Fix error on using auto-refresh
- Move REST_V2_PREFIX to top of script | fixes issue with auto-refresh:

Did not properly auto-refresh ""/manager"" and ""/"" pages when the manager was
killed after the page was already loaded | trivial: avoid repeated call | Handle a couple edge cases:

Handles a couple edge cases for /manager endpoint:
- If manager is dead on first loading the page (in this case, no manager
  table is created), but later comes online
  and we are using the auto-refresh feature, previously would never
create the manager table. auto-refresh will now create the table if it
does not yet exist.
- There was a case where /manager could result in a DataTables
  alert/error. If the manager was up but the endpoint used to get the
table data was not yet available (small window where this could occur),
a DataTables alert would occur. Handle this instead as a console log and
populate the table with no data."
apache/accumulo,4737,https://github.com/apache/accumulo/pull/4737,Improve readability of timekeeping code in idle server check,"Tried to improve things by refactoring the code to make things more readable. Also converted the longs to use the `NanoTime` object.

The logic should be identical before and after this change.

I did notice that the compactor idle check seems to include the same check that is happening in the code that reads it (`AbstractServer.idleProcessCheck()`) where it checks the `idleReportingPeriod` has elapsed since the last compaction has completed before returning true. Unless there is a reason this was added, it seems like that part could just be removed and the same check would take place upstream.",2024-09-11T17:19:12Z,DomGarguilo,DomGarguilo,"readability, readable",Converting to draft for now. Going to make another PR in 2.1 to clean up the duplicate logic outline above and merge that up first. | Marking as ready for review now that the Timer object has been merged up. Related metrics tests are passing.,Improve readability of timekeeping in idle server check code | Merge remote-tracking branch 'upstream/main' into idleCheckImprovement | Merge remote-tracking branch 'upstream/main' into idleCheckImprovement | Merge remote-tracking branch 'upstream/3.1' into idleCheckImprovement | Update to use new Timer object | Merge remote-tracking branch 'upstream/3.1' into idleCheckImprovement
apache/accumulo,5332,https://github.com/apache/accumulo/pull/5332,Added --json flag to ec-admin running command,"This PR enhances the accumulo ec-admin running command by adding a --format flag to support structured output in CSV and JSON formats. This allows for easier programmatic parsing of running compaction details.

Changes Introduced:
Added --format flag (json, csv, human [default]) to ec-admin running.
Implemented structured output formatting for better readability and parsing.
Updated runningCompactions() method to handle different output formats.
Ensured compatibility with existing behavior by keeping table format as the default.

Fixes #5205",2025-04-21T17:37:04Z,dlmarion,Suvrat1629,"readability, readable","@kevinrr888 I hope this fixes the fix the issue with the commit history :) | This looks pretty good. Have you tested this locally? | @dlmarion No, I did not understand how to setup the development environment but I wanted to contribute so I decided to open a pr regardless. Sorry about that😅 | @Suvrat1629 - I would suggest looking at https://github.com/apache/fluo-uno to get a development environment set up so that you can test this locally. | @dlmarion
git clone https://github.com/apache/fluo-uno.git
cd fluo-uno
./bin/uno fetch accumulo            # Fetches binary tarballs of Accumulo and its dependencies
./bin/uno setup accumulo            # Sets up Accumulo and its dependencies (Hadoop & ZooKeeper)
source <(./bin/uno env)             # Bash-specific command that sets up current shell

I ran this command and got done with the set up.
But i don't how I can test the changes that I have made locally. Where do i run the ./accumulo ec-admin command? | @Suvrat1629 - looking at the fluo-uno readme, it looks like you should be able to set the ACCUMULO_REPO environment variable in the uno.conf file and then fluo-uno will build Accumulo from your feature branch. See https://github.com/apache/fluo-uno/blob/main/conf/uno.conf#L46. Once everything is  running, Accumulo should be running from the directory specified by the ACCUMULO_HOME variable (see https://github.com/apache/fluo-uno/blob/main/conf/uno.conf#L126). You should be able to run the accumulo ec-admin from the bin directory in $ACCUMULO_HOME. | I like the idea of having a standard computer-readable output, but I'd prefer not to have CSV as an option. There are multiple different CSV standards, and they all do things slightly differently with quoting, spacing, redundant delimiters, etc. Plus, Java's String.split function is very confusing with splitting on a delimiter, making it harder to parse correctly. JSON or some other standard is fine, though.
(Note: this is a general comment, as I haven't looked at this specific PR's implementation yet) | I think the csv output is due to how I wrote the issue ticket. I believe a json only output would be satisfactory to close the issue ticket.","added formatting flags | Update server/base/src/main/java/org/apache/accumulo/server/util/ECAdmin.java

Co-authored-by: Kevin Rathbun <kevinrr888@gmail.com> | switched to gson for json formatting | reverted TExternalCompactionMap to TExternalCompactionList | added format validation and removed csv build up | fixes format validation

Co-authored-by: Dave Marion <dlmarion@apache.org> | json fix | removed a comment :') | formatting | Merge branch '2.1' into formatting-flags | Remove csv as a formatting option | Remove copy of running compaction map, seems unnecessary | Added ECAdminIT, confirmed JSON output can be parsed

Refactored the ECAdmin command and added a test. Downside to
this implementation is that it will consume more memory in the
case where json output is not requested."
apache/accumulo,5605,https://github.com/apache/accumulo/pull/5605,Disable metrics for thread pools when metrics disabled,Metrics were being registered and emittied for thread pools regardless of the metrics property setting. Added a mechanism to disable metrics collection for thread pools when the property is false.,2025-06-02T17:26:02Z,dlmarion,dlmarion,"readability, readable",,"Disable metrics for thread pools when metrics disabled

Metrics were being registered and emittied for thread
pools regardless of the metrics property setting. Added
a mechanism to disable metrics collection for thread
pools when the property is false. | Merge branch '2.1' into disable-thread-pools | Update core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java

Co-authored-by: Keith Turner <kturner@apache.org> | Update core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java

Co-authored-by: Keith Turner <kturner@apache.org> | Revert ""Update core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java""

This reverts commit 17e7e33b0cd067e8b7ade8acfca815321ad4c9d2. | Revert ""Update core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java""

This reverts commit 95c82726904c705a99a522553b100e7fc3e4ec34. | Use a supplier for client context thread pools to call getConfiguration()"
apache/accumulo,4747,https://github.com/apache/accumulo/pull/4747,Cleanup of main branch,"* Remove log4j-1.2-api dependency except as a bridge in the binary assembly (left a note in the assemble/pom.xml)
* Move log4j dependencies in core back to test scope. These were moved to the provided scope in #4743 to work around an issue because log4j-1.2-api was still used in the core module in 2.1; now that the legacy log4j stuff was removed from core in 3.1 and later, we can move these back to the test scope where they belong as #4558 intended
* Specify the exact version for deprecation ""since"" parameters (3.1.0, instead of 3.1)
* Drop the unused and deprecated `tserver.workq.threads` property
* Remove a few unused imports and variables, and drop the legacy MeterRegistryFactory that was deprecated in 2.1.3
* Suppress some deprecation warnings and use var in a few places for readability
* Delete unused test code in CompactionIT",2024-07-24T03:57:04Z,ctubbsii,ctubbsii,"readability, readable","It looks like we still need log4j-1.2-api on the classpath for hadoop minicluster's runtime. So, I'll have to re-add that back in for any tests or mini accumulo that use it before this can be merged. | It looks like I fixed all the issues with the dependencies, so I'll merge this now.","Cleanup of main branch

* Remove log4j-1.2-api dependency except as a bridge in the binary
  assembly (left a note in the assemble/pom.xml)
* Move log4j dependencies in core back to test scope. These were moved
  to the provided scope in #4743 to work around an issue because
  log4j-1.2-api was still used in the core module in 2.1; now that the
  legacy log4j stuff was removed from core in 3.1 and later, we can move
  these back to the test scope where they belong as #4558 intended
* Specify the exact version for deprecation ""since"" parameters (3.1.0,
  instead of 3.1)
* Drop the unused and deprecated `tserver.workq.threads` property
* Remove a few unused imports and variables, and drop the legacy
  MeterRegistryFactory that was deprecated in 2.1.3
* Suppress some deprecation warnings and use var in a few places for
  readability
* Delete unused test code in CompactionIT | Fix HadoopCredentialProviderTest

Add legacy log4j API back as test dependency for hadoop minicluster,
which fixes HadoopCredentialProviderTest | Restore log4j-1.2-api for hadoop minicluster

Re-add log4j-1.2-api dependency in the proper scopes, as a required
runtime dependency of hadoop-client-minicluster"
apache/accumulo,3984,https://github.com/apache/accumulo/pull/3984,Fix TServerClient.getThriftServerConnection for compactor queue names,This fixes an issue introducted in #3951 where the Compactor and ScanServer now expose the ClientService. The issue fixed here is that #3951 did not account for the extra part in the compactor path in ZooKeeper that is used for the queue name.,2023-11-27T21:35:37Z,dlmarion,dlmarion,"readability, readable",,"Fix TServerClient.getThriftServerConnection for compactor queue names

This fixes an issue introducted in #3951 where the Compactor and
ScanServer now expose the ClientService. The issue fixed here is
that #3951 did not account for the extra part in the compactor path
in ZooKeeper that is used for the queue name. | reverse condition for readability"
apache/accumulo,4364,https://github.com/apache/accumulo/pull/4364,Adds NanoTime wrapper for System.nanoTime,"Adds a strong type for System.nanoTime() and uses it in a few places.  Could be used in many more places if this is merged.

fixes #4360 ",2024-03-13T18:08:17Z,keith-turner,keith-turner,"readability, readable","Could consider implementing Comparable - the method could just delegate to Long.comparable  and possibly eqauls, hashCode ?
Thinking about this delegating to Long.comparable may not work when the nanos are negative.  But maybe something based on this.nanoSinceAO - other,nanoSinceAO ? | Could consider implementing Comparable - the method could just delegate to Long.comparable and possibly eqauls, hashCode ?

added equals and hashcode, can not add Comparable as its not safe to compare nano times | its not safe to compare nano times
Two nano measurements, in the same vm can be compared to determine before, after and equal by using the delta between the two values.  The use case would be if you were trying to find the oldest / newest NanoTime between two instances (or in a list / set)
This is highlights that maybe it should be documented that instances of NanoTime should not be persisted and cannot be shared across processes.  The nano measurement is only valid within the runtime and is determined at the start of the JVM. | can not add Comparable as its not safe to compare nano times

Actually we could add a compareTo method that may be safe by doing the following.  However I don't think I will add that now unless there is a need.
  @Override
  public int compareTo(NanoTime other) {
    long now = System.nanoTime();
    // all operations w/ nanoTimes must compute elapsed times first
    long elapsed1 = now - nanosSinceAO;
    long elapsed2 = now - other.nanosSinceAO;
    return Long.compare(elapsed1, elapsed2);
  } | I added the compareTo method, was thinking about it and it would be useful to have.  I think I need to add some unit test, will add those to this PR. | I approved the changes - but see you intend to add tests,  I'll wait until those are available.","Adds NanoTime wrapper for System.nanoTime

Adds a strong type for System.nanoTime() and uses it in a few places.  Could be used in many more places if this is merged. | code review update | code review update | update javadoc | Update core/src/main/java/org/apache/accumulo/core/util/NanoTime.java

Co-authored-by: EdColeman <dev1@etcoleman.com> | add compare to | Add unit test | fix and add tests"
apache/accumulo,3792,https://github.com/apache/accumulo/pull/3792,Bump default Accumulo RPC client to TLSv1.3,"This also adds TLSv1.3 to the list of accepted protocols for the RPC server and Monitor server, while still supporting TLSv1.2.

I didn't add any extra tests as all of the existing SSL/TLS testing should provide sufficient coverage.

This closes #3786",2023-10-06T22:15:04Z,cshannon,cshannon,"readability, readable","We don't want to be creating a situation where a partially upgraded cluster (where we support less coordination for upgrades, as patch releases are typically drop-in substitutes) can't properly talk between services because servers are using different default configs.

Fair point. | I made the change to TLSv1.3 as the only protocol by default, I can go ahead and merge if no other comments","Bump default Accumulo RPC client to TLSv1.3

This commit also adds TLSv1.3 to the list of accepted protocols for the
RPC server and Monitor server, while still supporting TLSv1.2.

This closes #3786 | Change default enabled protocols to TLSv1.3

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Merge branch 'main' into accumulo-3786 | Fix formatting | Minor style update"
apache/accumulo,5167,https://github.com/apache/accumulo/pull/5167,Fix accumulo-cluster,"* Update help descriptions for deprecated items
* Streamline debug output and add ""DEBUG:"" prefix
* Bash grammar fixes (double square brackets, remove unneeded quotes/curlies, etc.)
* Remove eval (use `bash -c` for local case instead)
* Use printf instead of bash 4.4 features for printing command to execute
* Use local variables where possible
* Remove unneded quoting in calls to accumulo-service using guilty knowledge of the fact that most of the options passed will be scalars without any spaces or special characters (improves readability)
* Fix broken computation of localhost addresses by filtering getent entries",2024-12-11T21:45:18Z,ctubbsii,ctubbsii,"readability, readable","I'm working on backporting these fixes to 2.1 and 3.1, along with the other recent changes to the scripts, so that they are more similar across branches, and easier to maintain. | Backport to 2.1 available in #5174","Fix accumulo-cluster

* Update help descriptions for deprecated items
* Streamline debug output and add ""DEBUG:"" prefix
* Bash grammar fixes (double square brackets, remove unneeded
  quotes/curlies, etc.)
* Remove eval (use `bash -c` for local case instead)
* Use printf instead of bash 4.4 features for printing command to
  execute
* Use local variables where possible
* Remove unneded quoting in calls to accumulo-service using guilty
  knowledge of the fact that most of the options passed will be scalars
  without any spaces or special characters (improves readability)
* Fix broken computation of localhost addresses by filtering getent
  entries | Fix bug in accumulo-service

Fixes an bug in accumulo-service where if a single running service of
the desired type was encountered, the script would exit prematurely"
apache/accumulo,4871,https://github.com/apache/accumulo/pull/4871,Bug fix no scan id for batch scans,"closes #4868
- Batch scans scan ids are now set
- At the same time, got rid of code duplication and improved readability of `SessionManager.getActiveScans()`
- Added functionality to test this case in `ScanIdIT`",2024-09-13T15:39:25Z,kevinrr888,kevinrr888,"readability, readable",,"Bug fix no scan id for batch scans

closes #4868
- Batch scans scan ids are now set
- At the same time, got rid of code duplication and improved readability
  of SessionManager.getActiveScans()
- Added functionality to test this case in ScanIdIT | simplified SessionManager.getActiveScans() further"
apache/accumulo,5301,https://github.com/apache/accumulo/pull/5301,FATE Threads Changes,"This PR:
- Changes FATE from having one pool of workers per FATE type (META and USER) to having a set of pools per FATE type which each work on their own subset of FATE operations:
	- This is accomplished through a new class `FateExecutor`. Each FATE has a set of `FateExecutor`s. Each `FateExecutor` is assigned to one set of FATE operations, has a `WorkFinder` thread, and a pool of `TransactionRunner` threads. The `WorkFinder` finds transactions this `FateExecutor` can work on, and assigns them to the `TransactionRunner`s which work on them.
	- `Fate` class now only handles the `DeadReservationCleaner` thread and the `FatePoolsWatcher` thread (previously `FatePoolWatcher`). The `DeadReservationCleaner` is unchanged. `FatePoolsWatcher` now oversees all the `FateExecutor`s/pools. The thread periodically checks for changes to the config, resizing any pools as necessary, safely stopping (if a thread is working on a transaction, it is not terminated until it finishes that work) any `FateExecutor`s that are invalidated by config changes (i.e., its assigned set of FATE operations is no longer present in the config), and starting any new/replacement `FateExecutor`s needed. The `FatePoolsWatcher` also keeps track of the number of idle threads to warn the user if a pool is experiencing a high load and should have its pool size increased (this functionality is unchanged from before, only difference is now a slightly different message is logged when warning the user).
	- The set of pools is determined by the new `MANAGER_USER_FATE_CONFIG`/`MANAGER_META_FATE_CONFIG` properties (replaces `MANAGER_FATE_THREADPOOL_SIZE`).
	- Another difference from this change is now USER and META FATEs can be configured differently whereas previously they shared the same property (`MANAGER_FATE_THREADPOOL_SIZE`)
	- These properties are json where the keys are a set of FATE operations and values are a pool size.
	- The properties are ensured to be valid json, pool size > 0, contain all FATE operations, contain no invalid FATE operations, and have no duplicate FATE operations
	- The FATE operations can be split across any number of pools of any size.
- Tests which used the `MANAGER_FATE_THREADPOOL_SIZE` have now been changed to `MANAGER_USER_FATE_CONFIG`/`MANAGER_META_FATE_CONFIG` properties. To keep testing functionality the same, the properties set are json of 1 key/value where the key is all FATE ops and the value is the value originally used in the test.
- `FatePoolResizeIT`/`MetaFatePoolResizeIT`/`UserFatePoolResizeIT` have been renamed to `FatePoolsWatcherIT`/`MetaFatePoolsWatcherIT`/`UserFatePoolsWatcherIT` since they now test other functionality of the `FatePoolsWatcher` not just pool resizing. The tests have also been completely changed since resizing has completely changed.

I also ensured all FATE tests still pass and sunny day still passes. I have not yet run the full set of ITs for this.

closes #5130",2025-03-24T17:39:40Z,kevinrr888,kevinrr888,"readability, readable","Changes:

Moved the resizing task from Fate.FatePoolsWatcher into FateExecutor. This cleaned up the code a bit getting rid of some getters in FateExecutor
When looking at the new MANAGER_FATE_<META/USER>_CONFIG props again realized the description I wrote wasn't correct. Updated
When tinkering with MANAGER_FATE_META_CONFIG and running the sunny day build realized that the config could be blank and still pass the sunny day build (meaning no FATE operations would be able to run for the Accumulo system tables). I assumed the sunny day build would catch something like this, but it doesn't appear that is the case. Perhaps we should create a sunny day test which tests fate operations on Accumulo system tables or maybe this would be caught in the full build and that is sufficient (I have not run the full build to see if that's true)... Regardless, this made me less confident in the correctness of my original attempt to write the META config (and looking at it again, I don't think it's correct). I changed the config to allow and expect all FateOperations, instead of trying to determine what subset would be allowed and expected for Accumulo system tables.
Minor changes to PropertyTypeTest testTypeFATE_<META/USER>_CONFIG to make the test easier to understand
Added new test to FatePoolsWatcherIT as described

@keith-turner - Could you take a look at these changes? Most of the changes to Fate and FateExecutor were copy-paste so those can be ignored. Mostly curious what you think about the changes to Property and the new test. | Perhaps we should create a sunny day test which tests fate operations on Accumulo system tables or maybe this would be caught in the full build and that is sufficient (I have not run the full build to see if that's true).

It does not have to be sunny day test, but we should probably have ITs that execute every table opertion against the system tables to ensure they behave as expected.  Would need to survey and see what coverage we currently have for this, I know there is some but not sure its complete.  Not something for this PR.

@keith-turner - Could you take a look at these changes? Most of the changes to Fate and FateExecutor were copy-paste so those can be ignored. Mostly curious what you think about the changes to Property and the new test.

@kevinrr888 I looked over the changes and they look good. I made one comment about a possible improvement to the test that would be nice if its workable. | @keith-turner

I made one comment about a possible improvement to the test that would be nice if its workable.

That is a nice additional check. I added it throughout the tests where applicable.
I also merged main into this branch. The only thing noteworthy of that is the new SYSTEM_MERGE operation that was recently added. In the default value of the new properties, I added this as shown for the config for META and USER:
         ""{\""TABLE_CREATE,TABLE_DELETE,TABLE_RENAME,TABLE_ONLINE,TABLE_OFFLINE,NAMESPACE_CREATE,""
          + ""NAMESPACE_DELETE,NAMESPACE_RENAME,TABLE_TABLET_AVAILABILITY,SHUTDOWN_TSERVER,""
          + ""TABLE_BULK_IMPORT2,TABLE_COMPACT,TABLE_CANCEL_COMPACT,TABLE_MERGE,TABLE_DELETE_RANGE,""
          + ""TABLE_SPLIT,TABLE_CLONE,TABLE_IMPORT,TABLE_EXPORT,SYSTEM_MERGE\"": 4,""
          + ""\""COMMIT_COMPACTION\"": 4,\""SYSTEM_SPLIT\"": 4}""

If we find that SYSTEM_MERGE (or any of the ops) need more resources, we can change this default.

It does not have to be sunny day test, but we should probably have ITs that execute every table opertion against the system tables to ensure they behave as expected. Would need to survey and see what coverage we currently have for this, I know there is some but not sure its complete. Not something for this PR.

I'll open an issue for this
Merging this into main now","FATE Threads Changes:

This commit:
- Changes FATE from having one pool of workers per FATE type (META and USER) to having a set of pools per FATE type which each work on their own subset of FATE operations:
	- This is accomplished through a new class `FateExecutor`. Each FATE has a set of `FateExecutor`s. Each `FateExecutor` is assigned to one set of FATE operations, has a `WorkFinder` thread, and a pool of `TransactionRunner` threads. The `WorkFinder` finds transactions this `FateExecutor` can work on, and assigns them to the `TransactionRunner`s which work on them.
	- `Fate` class now only handles the `DeadReservationCleaner` thread and the `FatePoolsWatcher` thread (previously `FatePoolWatcher`). The `DeadReservationCleaner` is unchanged. `FatePoolsWatcher` now oversees all the `FateExecutor`s/pools. The thread periodically checks for changes to the config, resizing any pools as necessary, safely stopping (if a thread is working on a transaction, it is not terminated until it finishes that work) any `FateExecutor`s that are invalidated by config changes (i.e., its assigned set of FATE operations is no longer present in the config), and starting any new/replacement `FateExecutor`s needed. The `FatePoolsWatcher` also keeps track of the number of idle threads to warn the user if a pool is experiencing a high load and should have its pool size increased (this functionality is unchanged from before, only difference is now a slightly different message is logged when warning the user).
	- The set of pools is determined by the new `MANAGER_USER_FATE_CONFIG`/`MANAGER_META_FATE_CONFIG` properties (replaces `MANAGER_FATE_THREADPOOL_SIZE`).
	- Another difference from this change is now USER and META FATEs can be configured differently whereas previously they shared the same property (`MANAGER_FATE_THREADPOOL_SIZE`)
	- These properties are json where the keys are a set of FATE operations and values are a pool size.
	- The properties are ensured to be valid json, pool size > 0, contain all FATE operations, contain no invalid FATE operations, and have no duplicate FATE operations
	- The FATE operations can be split across any number of pools of any size.
- Tests which used the `MANAGER_FATE_THREADPOOL_SIZE` have now been changed to `MANAGER_USER_FATE_CONFIG`/`MANAGER_META_FATE_CONFIG` properties. To keep testing functionality the same, the properties set are json of 1 key/value where the key is all FATE ops and the value is the value originally used in the test.
- `FatePoolResizeIT`/`MetaFatePoolResizeIT`/`UserFatePoolResizeIT` have been renamed to `FatePoolsWatcherIT`/`MetaFatePoolsWatcherIT`/`UserFatePoolsWatcherIT` since they now test other functionality of the `FatePoolsWatcher` not just pool resizing. The tests have also been completely changed since resizing has completely changed. | Merge branch 'main' into 4.0-feature-5130 | minor changes: addresses review | build fix | review changes | Merge branch 'main' into 4.0-feature-5130 | additional check in FatePoolsWatcherIT tests | Merge branch 'main' into 4.0-feature-5130"
apache/accumulo,4097,https://github.com/apache/accumulo/pull/4097,improves readability of code in DefaultCompactionPlanner,Rewrites sliding window code to be more readable.  It was difficult to verify the correctness of this code via inspection.,2023-12-20T21:33:09Z,keith-turner,keith-turner,"readability, readable","Nice improvement | Did some napkin math using example values of sortedFiles.size = 10 and maxFilesToCompact = 8.

Yeah the old code required a spread sheet or napkin to verify.  I could not really just look at it and say it was correct.","improves readability of code in DefaultCompactionPlanner

Rewrites sliding window code to be more readable.  It was difficult to
verify the correctness of this code via inspection. | format code"
apache/accumulo,2251,https://github.com/apache/accumulo/pull/2251,Create inner class in CompactionService,"* Improve readability of CompactionService by creating inner class
for the implementation of PlanningParameters",2021-08-31T18:35:30Z,milleruntime,milleruntime,"readability, readable",,"Create inner class in CompactionService

* Improve readability of CompactionService by creating inner class
for the implementation of PlanningParameters"
apache/accumulo,4237,https://github.com/apache/accumulo/pull/4237,Remove excessive logging message,"When testing scan servers this log message was spamming the contents of the log.

Dropping it down to `trace` level from `debug` to help improve readability.",2024-02-06T23:59:56Z,ddanielr,ddanielr,"readability, readable",,Remove excessive logging message
apache/accumulo,3320,https://github.com/apache/accumulo/pull/3320,update to use nanotime in TimedProcessor calculations,"Use nano time instead of System.currentTime in thrift metric calculation.

This also renames an variable to improve readability.  The time captured as stored locally as `now` but was also used as start time for second calculation.  Renamed and added comment,",2023-04-20T12:21:13Z,EdColeman,EdColeman,"readability, readable",,update to use nanotime in TimedProcessor calculations
apache/accumulo,2438,https://github.com/apache/accumulo/pull/2438,Refactor Initialize into multiple classes,"* Create ZooKeeperInitializer
* Create FileSystemInitializer
* Create InitialConfiguration
* Clean up code in Initialize by dropping redundant try/catch blocks,
improving error handling, passing around new InitialConfiguration object
to get config properties and improve readability
* Create new methods in Initialize: checkSASL(), checkUploadProps(), resetSecurity()",2022-01-31T10:06:49Z,ctubbsii,milleruntime,"readability, readable",@ctubbsii thanks for the tweaks,"Refactor Initialize into multiple classes

* Create ZooKeeperInitializer
* Create FileSystemInitializer
* Create InitialConfiguration
* Clean up code in Initialize by dropping redundant try/catch blocks,
improving error handling, passing around new InitialConfiguration object
to get config properties and improve readability
* Create new methods in Initialize: checkSASL(), checkUploadProps(), resetSecurity() | Merge branch 'main' into refactor-Initialize | Code review for Initialize changes in #2438

* Limit visibility to private or package-private where possible
* Pass Opts.clearInstanceName as a boolean parameter instead of passing
  all of Opts, so Opts can be private
* Rename 'go' to 'initialize' so that the method name that performed the
  action matched the object's name"
apache/accumulo,3176,https://github.com/apache/accumulo/pull/3176,fix parameter check and minor variable rename,"- Fixes parameter check that was using instance variable (timeOut) instead of passed value (timeout)
- Renames variable timeOut to retryTimeout to improve readability and reflect usage
- Renames variable batchTimeOut to batchTimeout for consistent case convention.

Replaces https://github.com/apache/accumulo/pull/3172 that was based on 2.1",2023-02-08T17:42:50Z,ctubbsii,EdColeman,"readability, readable",,fix parameter check and minor variable rename
apache/accumulo,2381,https://github.com/apache/accumulo/pull/2381,Refactor listSplits operation when using maxSplits,"Refactored listSplits method in TableOperationsImpl. This change affects the listSplits command which takes maxSplits as an option.

* Renamed variable names to enhance readability.
* Added documentation for the method.
* Replaced while-loop with if-loop after determining while-loop was only run at most once each time.
* Created IT test for method in ShellIT class.

Closes #2371 
",2021-12-13T14:17:36Z,jmark99,jmark99,"readability, readable",,"Refactor listSplits operation when using maxSplits

Refactored listSplits method in TableOperationsImpl. This change affects
the listSplits command which takes maxSplits as an option.

* Renamed variable names to enhance readability.
* Added documentation for the method.
* Replaced while-loop with if-loop after determining while-loop was only
run at most once each time.
* Created IT test for method in ShellIT class. | Update core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java

Coorect misspelled variable name.

Co-authored-by: Keith Turner <kturner@apache.org> | Update core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java

Correct misspelled variable name.

Co-authored-by: Keith Turner <kturner@apache.org> | Update core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java

Use size of splitsSubset rather than extra 'selectedSoFar' variable.

Co-authored-by: Keith Turner <kturner@apache.org> | Refactor listSplits operation when using maxSplits

Removed use of selectedSoFar variable from the method."
apache/accumulo,2304,https://github.com/apache/accumulo/pull/2304,ConcurrentDeleteTableIT bug fix and improvements,"Fixes: #1841 

This test was failing with a ""Table is being deleted"" exception which is to be expected when attempting to delete and run other ops concurrently. It appears #2240 prevents compactions from starting while a table is being deleted which I think is what caused this test to start to fail again.

* Main change was to catch the acceptable exception within `DelayedTableOp.run`
* While I was changing the file, I made quite a bit of other readability/refactoring changes that seem like improvements to me",2021-10-07T14:52:02Z,DomGarguilo,DomGarguilo,"readability, readable","I still on occasion run into this exception:
Details
java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.accumulo.core.client.AccumuloException: Compaction canceled

	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
	at org.apache.accumulo.test.functional.ConcurrentDeleteTableIT.testConcurrentFateOpsWithDelete(ConcurrentDeleteTableIT.java:214)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.base/java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:264)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: org.apache.accumulo.core.client.AccumuloException: Compaction canceled
	at org.apache.accumulo.test.functional.ConcurrentDeleteTableIT$DelayedTableOp.run(ConcurrentDeleteTableIT.java:254)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:264)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.accumulo.core.client.AccumuloException: Compaction canceled
	at org.apache.accumulo.core.clientImpl.TableOperationsImpl.doFateOperation(TableOperationsImpl.java:401)
	at org.apache.accumulo.core.clientImpl.TableOperationsImpl.compact(TableOperationsImpl.java:866)
	at org.apache.accumulo.test.functional.ConcurrentDeleteTableIT$1.doTableOp(ConcurrentDeleteTableIT.java:163)
	at org.apache.accumulo.test.functional.ConcurrentDeleteTableIT$DelayedTableOp.run(ConcurrentDeleteTableIT.java:244)
	... 6 more
Caused by: ThriftTableOperationException(tableId:3, tableName:null, op:COMPACT, type:OTHER, description:Compaction canceled)
	at org.apache.accumulo.core.manager.thrift.FateService$waitForFateOperation_result$waitForFateOperation_resultStandardScheme.read(FateService.java:4719)
	at org.apache.accumulo.core.manager.thrift.FateService$waitForFateOperation_result$waitForFateOperation_resultStandardScheme.read(FateService.java:4688)
	at org.apache.accumulo.core.manager.thrift.FateService$waitForFateOperation_result.read(FateService.java:4614)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)
	at org.apache.accumulo.core.manager.thrift.FateService$Client.recv_waitForFateOperation(FateService.java:157)
	at org.apache.accumulo.core.manager.thrift.FateService$Client.waitForFateOperation(FateService.java:142)
	at org.apache.accumulo.core.clientImpl.TableOperationsImpl.waitForFateOperation(TableOperationsImpl.java:303)
	at org.apache.accumulo.core.clientImpl.TableOperationsImpl.doFateOperation(TableOperationsImpl.java:372)
	... 9 more | I still on occasion run into this exception:

Oops forgot to include this. I've seen this too and believe its in the same boat as the other exception we are seeing. I'll account for that here as well. | I am still seeing this IT timeout. I noticed looking at the logs that processes were still running between the 2 tests. This makes me think things aren't shutting down properly between the tests or there . Here are the file timestamps of the logs:
02:59:39 (main) ~/workspace/accumulo/test/target/mini-tests$ ls -ltr
drwxrwxr-x 7 mike mike 4096 Nov  9 14:45 org.apache.accumulo.test.functional.ConcurrentDeleteTableIT_testConcurrentFateOpsWithDelete
drwxrwxr-x 7 mike mike 4096 Nov  9 14:52 org.apache.accumulo.test.functional.ConcurrentDeleteTableIT_testConcurrentDeleteTablesOps

Those directory creation times tell me that testConcurrentFateOpsWithDelete started first. But when I look at the timestamps of the logs:
testConcurrentFateOpsWithDelete/logs$ ls -ltr
-rw-rw-r-- 1 mike mike   1131284 Nov  9 14:59 SimpleGarbageCollector_1239693537.out
-rw-rw-r-- 1 mike mike 135419459 Nov  9 14:59 Manager_717820111.out
-rw-rw-r-- 1 mike mike   2588193 Nov  9 14:59 TabletServer_517210456.out
-rw-rw-r-- 1 mike mike   1777859 Nov  9 14:59 TabletServer_73347924.out
-rw-rw-r-- 1 mike mike     23255 Nov  9 14:59 ZooKeeperServerMain_1076296094.out
testConcurrentDeleteTablesOps/logs$ ls -ltr
-rw-rw-r-- 1 mike mike  5555 Nov  9 14:52 Initialize_121937298.out
-rw-rw-r-- 1 mike mike  3910 Nov  9 14:52 Main_2053248710.out
-rw-rw-r-- 1 mike mike     0 Nov  9 14:52 Manager_749914505.out
-rw-rw-r-- 1 mike mike     0 Nov  9 14:52 SimpleGarbageCollector_450467038.out
-rw-rw-r-- 1 mike mike 31288 Nov  9 14:52 TabletServer_2141990664.out
-rw-rw-r-- 1 mike mike 34509 Nov  9 14:52 TabletServer_687609284.out
-rw-rw-r-- 1 mike mike 21957 Nov  9 14:52 ZooKeeperServerMain_231826773.out

They appear to be running at the same time. It looks like the Manager and GC won't start up for the second test because they are still running in the first. | I also saw this fail again. I collected the logs and saw that Manager got stuck for a few minutes, while printing hundreds of thousands of mergeInfo overlaps DEBUG messages at a rate of around 25 lines/millisecond. I did not see leftover processes or indications of leftover processes like @milleruntime  saw, though. | I cannot get this test to fail locally anymore. I did some rebasing to older commits and it passed there, now it is also passing when I add those commits back. The test will still take over 5 minutes to complete but no timeouts and no super large manager logs. | I am now seeing some similar logs @ctubbsii and I had a run where Manager logs are quite large. Though the IT still passed for me. Looking into it further.
All the overlap logs seem to be when taking each tablet offline for deletion.",Initial improvements | Catch 'table is being deleted exception' | Add exception message constants | Rename variables | Add usage of table deleted msg constant
apache/accumulo,3276,https://github.com/apache/accumulo/pull/3276,Optimize internal data structures in Authorizations,"Instead of creating collections with default initial capacities, create them with the known capacities. This will reduce the time taken and amount of memory consumed when populating these objects during Authorizations object creation when the number of elements in the Authorizations is larger than the default initial capacity of the collection data structure. This may also reduce the time it takes the VisibilityEvaluator to determine if the Authorizations object contains a token found in a ColumnVisibility.",2023-04-05T11:08:07Z,dlmarion,dlmarion,"readability, readable, clarity",,"Optimize internal data structures in Authorizations

Instead of creating collections with default initial capacities,
create them with the known capacities. This will reduce the time
taken and amount of memory consumed when populating these objects
during Authorizations object creation when the number of elements
in the Authorizations is larger than the default initial capacity
of the collection data structure. This may also reduce the time
it takes the VisibilityEvaluator to determine if the Authoriations
object contains a token found in a ColumnVisibility. | Centralized collection creation into one method | broke internal collection method into two, removing the need for Pair"
apache/accumulo,2233,https://github.com/apache/accumulo/pull/2233,Drop ServerUtil and static state,"* Move static state from ServerUtil in to ServerConstants object methods
* Move creation of ServerConstants object into ServerInfo
* Move upgrade code from ServerUtil into UpgradeCoordinator
* Move methods dealing with server stuff to ServerContext
* Create getter for ServerConstants in ServerContext
* Move AccumuloTest to manager to avoid adding a dependency
* Drop ServerUtilTest and move testing of method to ServerContextTest",2021-08-17T13:51:23Z,milleruntime,milleruntime,"readability, readable",,"Drop ServerUtil and static state

* Move static state from ServerUtil in to ServerConstants object methods
* Move creation of ServerConstants object into ServerInfo
* Move upgrade code from ServerUtil into UpgradeCoordinator
* Move methods dealing with server stuff to ServerContext
* Create getter for ServerConstants in ServerContext
* Move AccumuloTest to manager to avoid adding a dependency
* Drop ServerUtilTest and move testing of method to ServerContextTest | Update server/base/src/main/java/org/apache/accumulo/server/ServerContext.java

Co-authored-by: EdColeman <dev1@etcoleman.com> | Updates"
apache/accumulo,2152,https://github.com/apache/accumulo/pull/2152,Adds Ample support for tablets and uses this in compaction finalizer,fixes #1974,2021-06-09T21:53:02Z,keith-turner,keith-turner,"readability, readable",,"Adds Ample support for tablets and uses this compaction finalizer

fixes #1974 | remove todos | fix build error and add comment | Update core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update server/compaction-coordinator/src/main/java/org/apache/accumulo/coordinator/CompactionFinalizer.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | fix imports | code review update | code review update | inline methods to avoid odd intermethod deps | statically import Preconditions | Update core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | format code"
apache/accumulo,1972,https://github.com/apache/accumulo/pull/1972,Add specific types for Fate locks and Zoo locks,"A possible solution for #1966.  

Giving lock paths a more specific type over just `String` will reduce confusion on which type of lock is being used and which appropriate function calls to make.

This is just one possible implementation of this. I wavered on including a specific path function or not. Any ideas on improvements are welcomed.",2021-03-18T16:07:27Z,ctubbsii,Manno15,"readability, readable","The extra commits are from merging these changes from an alternate branch I had. | Thanks for taking the time to review it! | I have no remaining issues once Christopher's suggestions are resolved. | I do have some follow-up suggestions I was working on that I thought would make sense as part of this PR. @Manno15 any problem if I push them directly to your branch for inclusion? We can revert if there's any objection. | @ctubbsii Go ahead. I will review them in the morning. | I like your additions. Some comments below, in order of the commit points.

The renaming was going to happen in #1967 which is why I didn't do it here.
I did have something akin to this in my original commit but wasn't quite sure if it was better or not. So I removed it in the next commit. I did it this way so I can drop the second one if I changed my mind about it. I  do personally like having the path function better though.
I am indifferent to the var and method name change. I agree the shorter name is probably better, especially with a comment on the method now to explain its intent.
Good change, I thought the fields were already final so that was my mistake.
Make sense if it wasn't used. Good change.
Everything else looks good. | I like your additions. Some comments below, in order of the commit points.

The renaming was going to happen in #1967 which is why I didn't do it here.


This PR can fix both #1966 and #1967. One PR to address two issues. I think ServiceLock is a good name for ZooLock, so if there aren't any objections to that name, I'll add that rename to this PR also, and then we can merge it and close both issues.


Good change, I thought the fields were already final so that was my mistake.


To be clear, my additions weren't corrective in any way. I didn't see any mistakes. My additions were supplemental only, building on what you did. 😺 | This PR can fix both #1966 and #1967. One PR to address two issues. I think ServiceLock is a good name for ZooLock, so if there aren't any objections to that name, I'll add that rename to this PR also, and then we can merge it and close both issues.

I am also okay with that name if no one else objects or has any other alternatives for the rename in #1967. | @ctubbsii Everything looks good to me. Is there anything else that you think needs to be added? If not, I can merge this in. | I also ran the IT suite and everything passes. | @Manno15 I merged it. Thanks!","make explicit lockpath types for Zoolock and Fate lock | remove path function | Merge commit 'da759df19098824925fd8792aea881116f059e1d' into acc_1966 | Merge commit 'e5dd196640647dbb292ffca5e5a1e50588f496a0' into acc_1966 | remove comment | use path type in zooqueuelock | Updates to PR 1972

* Rename `ZooQueueLock` to `FateLock` for consistency with
  `FateLockPath`
* Use `ZooLock.path()` instead of `new ZooLockPath()` and similar for
  `FateLockPath` (it's slightly shorter, and makes code look a little
  more fluent than use of `new` everywhere; it also avoids imports when
  not needed)
* Shorten lots of lines by renaming `validateAndSortByLockPrefix` to
  just `validateAndSort`, using `var` keyword
* Add javadoc to `FateLock` class and make its fields final
* Remove unused `ephemeral` field (`FateLock` only uses persistent
  nodes, and the extra parameter when not used makes it harder to reason
  about the actual implementation)
* Guard against null parameters by using `Objects.requireNonNull` in key
  places
* Add brief javadoc to `FateLock.validateAndSort` | Rename ZooLock to ServiceLock

Fix #1967 by renaming ZooLock to ServiceLock (and also rename related
test classes)"
apache/accumulo,3349,https://github.com/apache/accumulo/pull/3349,improves fetching mulitple extents in ample,Its possible to pass a collection of extents to ample and get the metadata for those tablets.  However only metadata for the tablets that exists in the metadata are returned.  Using this method correctly is tricky because it may in rare cases (like concurrent splits and merges) only return a subset of what was requested. This commit changes the ample interface to add handling for these missing extents by adding a Consumer<KeyExtent> to the interface to which missing extents are passed.  Now its no longer possible to use this ample functionality without considering this rare case.  While making this change one place was found where the missing extents were not being considered.,2023-05-05T20:00:26Z,keith-turner,keith-turner,"readability, readable",,"improves fetching mulitple extents in ample

Its possible to pass a collection of extents to ample and get the
metadata for those tablets.  However only metadata for the tablets that
exists in the metadata are returned.  Using this method correctly is
tricky because it may in rare cases (like concurrent splits and merges)
only return a subset of what was requested. This commit changes the
ample interface to add handling for these missing extents by adding a
Consumer<KeyExtent> to the interface to which missing extents are
passed.  Now its no longer possible to use this ample functionality
without considering this rare case.  While making this change one
place was found where the missing extents were not being considered. | format code | Update server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java

Co-authored-by: Dave Marion <dlmarion@apache.org> | added test and made non existent extent handler optional | fix build error | fix build error | fix build issue | adds more detail to docs | Merge remote-tracking branch 'upstream/main' into fetchextents"
apache/accumulo,2517,https://github.com/apache/accumulo/pull/2517,Move Tables class into Context,"* To avoid making ClientContext too big, move Tables methods into a new
  class, TableZooUtil, which ClientContext has a single instance of, and
  is closed when ClientContext is closed
* All TableZooUtil methods are changed from static to non-static, and
  the ClientContext is passed in during construction; this class is only
  ever intended to be used with ClientContext, so nothing else should
  instantiate it
* Move other helper classes into the same package as TableZooUtil
* Move table name manipulation into its own class (in the same package)
* Eliminate now unneeded PowerMock from the monitor and shell modules

This fixes #2300",2022-02-23T18:46:05Z,ctubbsii,ctubbsii,"readability, readable",,"Move Tables class into Context

* To avoid making ClientContext too big, move Tables methods into a new
  class, TableZooUtil, which ClientContext has a single instance of, and
  is closed when ClientContext is closed
* All TableZooUtil methods are changed from static to non-static, and
  the ClientContext is passed in during construction; this class is only
  ever intended to be used with ClientContext, so nothing else should
  instantiate it
* Move other helper classes into the same package as TableZooUtil
* Move table name manipulation into its own class (in the same package)
* Eliminate now unneeded PowerMock from the monitor and shell modules

This fixes #2300 | Rename TableZooUtil to TableZooHelper"
apache/accumulo,2590,https://github.com/apache/accumulo/pull/2590,Change location where Thread.start() is called to resolve ErrorProne warning,Closes #2581 ,2022-03-25T17:05:13Z,dlmarion,dlmarion,"readability, readable",LGTM! Thanks for taking care of this Dave. EP no longer flags it with this change.,Change location where Thread.start() is called to resolve ErrorProne warning
apache/accumulo,1945,https://github.com/apache/accumulo/pull/1945,Create readTablets method in Ample. Closes #1473,"* Modify TabletsMetadata to impl readTablets in Ample
* Modify classes calling TabletsMetadata to pass the client object as
part of the builder init, instead of the final build method
* Add testAmpleReadTablets to MetadataIT

Co-authored-by: cradal <20303105+cradal@users.noreply.github.com>

Updated version of #1651 ",2021-02-24T13:46:46Z,milleruntime,milleruntime,"readability, readable",,"Create readTablets method in Ample. Closes #1473

* Modify TabletsMetadata to impl readTablets in Ample
* Modify classes calling TabletsMetadata to pass the client object as
part of the builder init, instead of the final build method
* Add testAmpleReadTablets to MetadataIT

Co-authored-by: cradal <20303105+cradal@users.noreply.github.com> | Update core/src/main/java/org/apache/accumulo/core/metadata/schema/AmpleImpl.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update core/src/main/java/org/apache/accumulo/core/metadata/schema/Ample.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Clean up | CR update"
apache/accumulo,2098,https://github.com/apache/accumulo/pull/2098,Fix #1628 add test where selector throws an error,,2021-05-12T19:05:16Z,ctubbsii,dlmarion,"readability, readable",QA build failed due to connection reset pulling down a jar - not related to the code changes.,"Fixes #1628 - add test where selector throws an error | Fixes #1628 - update test to set property at table creation, verify data in table at end | Fixes #1628 - remove Logger from test selector | Fixes #1628 - fix QA bug | Update test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java"
apache/accumulo,3297,https://github.com/apache/accumulo/pull/3297,"fix metrics tags, tserver hostname, remove tags from thrift metrics","- Fixes tserver initialization so host name and port are reported in metrics
- remove extra tags from thrift metrics
- Update opentelemetry and micrometer versions
- update test to check tag length is sane.
- Includes fix for tag length submitted as PR #3296 

Currently, tserver metrics tags do not include the host and there are extra tags on the thrift metrics.  Currently the tags are reported as:
```
METRICS, name: 'accumulo.thrift.idle' num tags: 4, tags: {process.name=tserver, server=TabletServer, Address=0.0.0.0, thread=Thrift Client Server}
METRICS, name: 'accumulo.thrift.idle' num tags: 4, tags: {process.name=tserver, server=TabletServer, Address=0.0.0.0, thread=Thrift Client Server}
METRICS, name: 'accumulo.thrift.idle' num tags: 4, tags: {process.name=manager, server=Manager, Address=server1 thread=Manager Client Service Handler}
METRICS, name: 'accumulo.tserver.scans.files.open' num tags: 3, tags: {process.name=tserver, statistic=value, Address=0.0.0.0}
METRICS, name: 'accumulo.tserver.compactions.majc.queued' num tags: 4, tags: {process.name=tserver, statistic=value, Address=0.0.0.0, id=i.meta.huge}


```

This update changes the global tags to always include the host and the port number:

```
METRICS, name: 'accumulo.thrift.idle' num tags: 3, tags: {process.name=tserver, port=32779, host=server1}
METRICS, name: 'accumulo.thrift.idle' num tags: 3, tags: {process.name=tserver, port=39729, host=server1}
METRICS, name: 'accumulo.thrift.idle' num tags: 3, tags: {process.name=manager, port=43165, host=server1}
METRICS, name: 'accumulo.tserver.scans.files.open' num tags: 4, tags: {process.name=tserver, statistic=value, port=32779, host=server1}
METRICS, name: 'accumulo.tserver.compactions.majc.running' num tags: 5, tags: {process.name=tserver, statistic=value, port=32779, host=server1, id=i.root.huge}
```

Co-authored-by: NAME @ddanielr ",2023-04-17T14:49:56Z,EdColeman,EdColeman,"readability, readable","What was the reason for removing the thread tag? | Over-all it seemed that it was not providing information that was not already there - its a thrift metric, the process is identified so repeating that it was in the client server handler could be inferred.  From the samples seen, the tag was appearing as either thread=Thrift Client Server or Manager Client Service Handler
If it could take on different values, it also seemed unlikely that it would be something that end-users would need to trend / watch by specific thread.  Monitoring thift times could provide a view into the health of the processes - knowing it was a specific thread seemed like something that would be needed by developers that were profiling specific code sections and metrics may not be the best way to determine that specific thrift thread timing.
Each different tag value increase the number of time-series that need to be tracked. It did not seem necessary to potentially increase the number of time-series than need to be tracked (assuming it could take different values) - but I can restore it if there is a use-case that I am unaware of. | Overall the changes look good! Thanks for including my changes from #3296.

If it could take on different values, it also seemed unlikely that it would be something that end-users would need to trend / watch by specific thread. Monitoring thift times could provide a view into the health of the processes - knowing it was a specific thread seemed like something that would be needed by developers that were profiling specific code sections and metrics may not be the best way to determine that specific thrift thread timing.

Agreed! If profiling specific code sections is required then we should focus on adding traceability support as a separate feature vs attempting to combine that data into the existing metrics store.","fix metrics tags, tserver hostname, remove extra tags from thrift metrics | address PR comments"
apache/accumulo,2724,https://github.com/apache/accumulo/pull/2724,Use href links in Javadoc,,2022-05-24T18:48:31Z,ctubbsii,milleruntime,"readability, readable",,"Use href links in Javadoc | Update core/src/main/java/org/apache/accumulo/core/file/BloomFilterLayer.java

Co-authored-by: Luke Foster <84727868+foster33@users.noreply.github.com> | Update core/src/main/java/org/apache/accumulo/core/client/Accumulo.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update core/src/main/java/org/apache/accumulo/core/client/Accumulo.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | update | Update core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java

Co-authored-by: Dom G. <domgarguilo@apache.org> | Update core/src/main/java/org/apache/accumulo/core/clientImpl/TableOperationsImpl.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update core/src/main/java/org/apache/accumulo/core/iterators/user/IntersectingIterator.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControl.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update server/monitor/src/main/java/org/apache/accumulo/monitor/rest/problems/ProblemsResource.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update shell/src/main/java/org/apache/accumulo/shell/Shell.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Format | Update core/src/main/java/org/apache/accumulo/core/iterators/user/IntersectingIterator.java | Update core/src/main/java/org/apache/accumulo/core/file/BloomFilterLayer.java | Apply suggestions from code review | Merge branch 'main' into pr/2724 | formatting"
apache/accumulo,2490,https://github.com/apache/accumulo/pull/2490,Changes to ExternalCompaction ITs,"Added CompactionCoordinator and Compactor to MiniAccumuloCluster. Modified ITs
to extend SharedMiniClusterBase so that MAC was not being restarted for each
test method.",2022-02-14T20:38:46Z,dlmarion,dlmarion,"readability, readable, comprehensible",,"Changes to ExternalCompaction ITs

Added CompactionCoordinator and Compactor to MiniAccumuloCluster. Modified ITs
to extend SharedMiniClusterBase so that MAC was not being restarted for each
test method. | Add missing javadoc information | Updates based on review comments | Minor edits from comments"
apache/accumulo,4668,https://github.com/apache/accumulo/pull/4668,removes code that automatically removes multiple tablet locations,"Code was added for [ACCUMULO-2261][1] that would automatically remove duplication locations from tablet metadata.  This was an expected event in the past because a tablet server could write a location after it lost its lock and before the process died.

Now that conditional mutations are used to write locations and with the changes in #4620 to make this really strict, the specific problem described in ACCUMULO-2261 should no longer happen. The code that was added to cleanup the metadata table for ACCUMULO-2261 could hide a new or different bug and prevent it from being found and fixed. If a new cause of duplicate locations is found in the elasticity branch then a fix that is appropriate for it should be created.

Code was added to log detailed information when a duplicate location is seen.  As part of this the root tablet metadata needed to be read and that code was all over the place, so that was consolidated.

fixes #3888

[1]:https://issues.apache.org/jira/browse/ACCUMULO-2261",2024-06-14T13:29:18Z,keith-turner,keith-turner,"readability, readable",While researching this found #1121 and closed it.,"removes code that automatically removes multiple tablet locations

Code was added for [ACCUMULO-2261][1] that would automatically remove
duplication locations from tablet metadata.  This was an expected
event in the past because a tablet server could write a location
after it lost its lock and before the process died.

Now that conditional mutations are used to write locations and
with the changes in #4620 to make this really strict, the specific
problem described in ACCUMULO-2261 should no longer happen.
The code that was added to cleanup the metadata table for ACCUMULO-2261
could hide a new or different bug and prevent it from being found
and fixed. If a new cause of duplicate locations is found in the
elasticity branch then a fix that is appropriate for it should be
created.

Code was added to log detailed information when a duplicate location
is seen.  As part of this the root tablet metadata needed to be read
and that code was all over the place, so that was consolidated.

fixes #3888

[1]:https://issues.apache.org/jira/browse/ACCUMULO-2261 | code review updates | add comment | code review updates | Update test/src/main/java/org/apache/accumulo/test/TestDualAssignment.java

Co-authored-by: Dom G. <domgarguilo@apache.org> | test ample read"
apache/accumulo,2432,https://github.com/apache/accumulo/pull/2432,Modified ThreadPools to conditionally register metrics,"When ThreadPools calls ExecutorServiceMetrics, it creates Meters
that are registered with the MeterRegistry. However, there is no
easy way to remove the Meters from the MeterRegistry when the
ThreadPool is shut down. Modified the ThreadPools methods to accept
a boolean value for registering metrics for the newly created
ThreadPool. Modified classes that use ThreadPools class to only
pass true for this boolean value when the ThreadPool is long lived.

Closes #2423",2022-01-28T15:33:48Z,dlmarion,dlmarion,"readability, readable","Boolean parameters can make code harder to read. Instead of boolean parameters, you could have a separate method name, like .action() and .actionWithMetrics(), or you could use a builder, like builder.withMetrics().build(), or you could have enums for flags, like .action(EnumSet.of(WITH_METRICS, OTHER_FLAG))

Another option would be to replace the last boolean argument in the method with a new enum type.  The single boolean here is not too bad, an enum would be more readable.  We have had methods in the past w/ multiple boolean arguments and that was awful to read. | Boolean parameters can make code harder to read. Instead of boolean parameters, you could have a separate method name, like .action() and .actionWithMetrics(), or you could use a builder, like builder.withMetrics().build(), or you could have enums for flags, like .action(EnumSet.of(WITH_METRICS, OTHER_FLAG))

I have no issue with modifying ThreadPools to use a builder pattern, but I think that should be done in a different issue. IDEs make it pretty easy to see the formal parameter name for the method and the formal parameter name is descriptive in this case.

Another option would be to replace the last boolean argument in the method with a new enum type. The single boolean here is not too bad, an enum would be more readable. We have had methods in the past w/ multiple boolean arguments and that was awful to read.

I have no issue with modifying this PR to use an enum for the boolean parameter, but I'm not sure that it's necessary if there is going to be a follow-on issue created to modify the class to use a builder pattern. | IDEs make it pretty easy to see the formal parameter name for the method and the formal parameter name is descriptive in this case.

That is a good point.  That does dramatically increase readability. However when reviewing a PR on GH, you dont have that nice feature.  But thinking back to my review that was not a problem at all. The review was very easy for me to review because I knew what the single boolean did.  The more difficult part of the review was that I had to look at code outside the PR sometimes to see how code was called externally, which had nothing to do with the boolean.  More than one boolean would certainly have given me a headache during review though.

I'm not sure that it's necessary if there is going to be a follow-on issue created to modify the class to use a builder pattern.

That makes sense to me.","Modified ThreadPools to conditionally register metrics

When ThreadPools calls ExecutorServiceMetrics, it creates Meters
that are registered with the MeterRegistry. However, there is no
easy way to remove the Meters from the MeterRegistry when the
ThreadPool is shut down. Modified the ThreadPools methods to accept
a boolean value for registering metrics for the newly created
ThreadPool. Modified classes that use ThreadPools class to only
pass true for this boolean value when the ThreadPool is long lived.

Closes #2423 | Add/Update ThreadPools javadoc, enable metrics for Fate transaction runner thread pool"
apache/accumulo,3257,https://github.com/apache/accumulo/pull/3257,Refactor TabletMetadata.Location to encapsulate TServerInstance,"The previous version where Location extends TServerInstance was error prone if euqals/hashcode was overriden. This fixes things by encapsulating TServerInstance and creating proper equals/hashcode methods so comparsions won't fail.

This fixes #3254",2023-03-25T12:24:24Z,cshannon,cshannon,"readability, readable","@keith-turner  - I went ahead and targeted this for 2.1.1 since it's not public APIs but that could be changed. I updated the Ample interface to pass in a Location instead of the tserver and location type. If we want we could keep backwards compatibility by adding some default methods that delegate to the new methods, for example:
default T putLocation(TServerInstance tserver, TabletMetadata.LocationType type) {
  return putLocation(new TabletMetadata.Location(tserver, type));
}

default T deleteLocation(TServerInstance tserver, TabletMetadata.LocationType type) {
  return putLocation(new TabletMetadata.Location(tserver, type));
}
But I did not do that for now as I figured it would be cleaner to just change the existing. Also, I didn't add any new tests because all of the existing tests seem to have enough coverage. I ran through the Sunny ITs and everything passed and I can kick off a full IT. I also grabbed your branch from #3251 locally and merged this in and after fixing the merge conflicts I verified the TabletMetadataTest still passed with the new update. | But I did not do that for now as I figured it would be cleaner to just change the existing

I agree with that. | The full IT build passed except for 3 tests (AssignLocationModeIT, CompactLocationModeIT, ManagerAssignmentIT) but this was a good thing as the test themselves needed fixing and the tests breaking showed the new equals/hashcode methods are in fact implemented correctly in Location.
The tests were comparing that the server instances were the same between different states (current, last, etc) and now that we use Location and compare location type the equals failed because the server instances were the same but the LocationTypes were different of course.
Here is how the current comparsion is in main where it's comparing the server instance types: 
  
    
      accumulo/test/src/main/java/org/apache/accumulo/test/functional/AssignLocationModeIT.java
    
    
         Line 70
      in
      ecbff2e
    
  
  
    

        
          
           assertEquals(newTablet.current, newTablet.last); 
        
    
  


And here is how it's now fixed: 
  
    
      accumulo/test/src/main/java/org/apache/accumulo/test/functional/AssignLocationModeIT.java
    
    
         Line 70
      in
      7a3caf8
    
  
  
    

        
          
           assertEquals(newTablet.getCurrentServer(), newTablet.getLastServer());","Refactor TabletMetadata.Location to encapsulate TServerInstance

The previous version where Location extends TServerInstance was
error prone if euqals/hashcode was overriden. This fixes things by
encapsulating TServerInstance and creating proper equals/hashcode
methods so comparsions won't fail.

This fixes #3254 | Import Location directly and use Location constructors private to be consistent | call getHostPortSession() delegate method on location | Fix incorrect delegation of static method for last Location | Add helper methods to return TServerInstance in TabletLocationState

This allows for less code changes in TabletGroupWatcher is that class
only cares about the TServerInstance inside of the Location in many
places | Merge branch '2.1' into accumulo-3254 | Fix tests after implementing equals for Location"
apache/accumulo,1892,https://github.com/apache/accumulo/pull/1892,Converting table creation to pre-split,"In many cases, tables are created and then splits are added. This is not as efficient as creating the tables with the splits.  Significant decreases in time taken in tests with pre-split tables is observed, especially in cases with many splits. This PR aims to convert to pre-split tables in cases where improvement is obvious, and discuss to what extent and in what scenarios this conversion should take place.",2021-02-04T16:07:49Z,ctubbsii,DomGarguilo,"readability, readable, clarity","The most recent commit contains the remaining cases where the conversion to pre-split tables seems appropriate. The remaining cases that add splits after the table is created seemed to do so on purpose so they were left alone. | @DomGarguilo It looks like this change broke some integration tests. Would you mind looking into those? Specifically, I saw the following tests fail after this change was merged: ReadWriteIT, RecoveryWithEmptyRFileIT, RenameIT, ShutdownIT, SslIT, SslWithClientAuthIT (some of these may extend each other, so there may be fewer than 6 problems) | Would you mind looking into those?

Sure, will do @ctubbsii | Was able to reproduce the failures in these tests. Tracked the error down to a single change, which after fixing, these tests pass successfully. I can make a PR for this fix @ctubbsii | Did a bit of rough testing and found that with this change, the tests ran a bit over 10 minutes faster on average.","Converting created tables to pre-split

convert ConcurrentDeleteTableIT

converted BulkImportMonitoringIT

revert back from useless cast to ClientContext

removed unnecessary cast to ClientContext for BulkImportMonitoringIT

converted to pre-splitting table for ConcurrentDeleteTableIT

converted CreateAndUseIT to pre-split

converted VolumeIT to pre-split tables

converted BinaryIT and a helper

removed unecessary cast to ClientContext for ConcurrentDeleteTableIT

removed unecessary cast to ClientContext for COncurrentDeleteTableIT | Converted more tests to pre-split tables.

removed duplicate license header

switched to var

converted and cleaned CompactionExecutorIT

CompactionExecutorIT consistency change

converted ConditionalWriterIT

converted CountNameNodeOpsBulkIT to pre-split

converted MasterRepairsDualAssignmentIT to pre-split

converted TestIngest to pre-split

converted UserCompactionStrategyIT to pre-split tables

converted to pre-splits compationstrategyIT, bulkoldit, clonetestIT

converted fatStarvationIT

converted logical time IT to pre-split

convert manyWriteAheadLogsIT

convert MaxOpenIT

converted MergeIT

converted MetadataMaxFileIT

converted ScanRangeIT

converted SummaryIT | typo"
apache/accumulo,3805,https://github.com/apache/accumulo/pull/3805,Adds ReferenceScan type and updates GC,"This change adds a new subclass of Reference called ReferenceScan. This is so the Gc can determine if a candidate has been defeated by a file, dir, or scan reference.

If a candidate is considered to be inUse due to a scan reference, the Gc will not add it to the list of inUse candidates for possible deletion. This change closes #3804.

There are also minor improvements to various comments and javadocs. I also added a warning statement to the experimental property description for `GC_REMOVE_IN_USE_CANDIDATES` to reference #3802. 

I don't know if this is the best way to solve the issue mentioned in #3804, but it did accomplish it without changing the number of scan operations the GC has to make in a cycle. ",2023-10-24T16:16:31Z,ddanielr,ddanielr,"readability, readable","Added the discussed fix for #3802 and removed the warning statement from the GC_REMOVE_IN_USE_CANDIDATES property.
fixes #3802 | @ctubbsii Would it be possible to get a review of these changes? This PR has been sitting for a while and I'd like to close it out for 2.1.3","Adds ReferenceScan type and updates GC

* Adds the ReferenceScan type and ensures inuse scan candidates are not removed.
* Adds various comments, updates javadocs, and adds warning to
  experimental property description. | Fix possible race condition with InUse Candidates

This change writes the gcCandidates twice when performing a major
compaction to ensure that valid candidates were not removed before the
tablet mutation had completed.

Fixes: #3802 | Add static named constructors

Adds static named constructors instead of creating a ReferenceScan
subclass.

Cleans up comments and refactored variable names for better readability | Refactored test method name

Renamed `assertRemoved` to `assertFileRemoved` to convey that the
candidate is now an hdfs file reference that has been deleted by the GC | Fix missed references"
apache/accumulo,3231,https://github.com/apache/accumulo/pull/3231,Fix wait timeout logic for available tservers,"Replaces the retry object with a wait loop.
Log messages are generated independent of max wait time.

Closes #3159 ",2023-03-10T18:27:26Z,EdColeman,ddanielr,"readability, readable","I don't think the sleepInterval is calculated elegantly. Dividing the wait interval into 10  sleep periods seemed to be ok for now, but I welcome better suggestions.
I wanted to get this change in to close out the open items for 1.10.3 | Overall this looks okay, but removing the retry in favor of a wait loop looses some functionality - mainly that there was a small pause at start. It also seems cleaner because the wait times do not need to be calculated. Also, with the retry, there would be a most 3 log messages generated for the wait.
Why did you choose not to use retry?

I attempted using a Retry with an earlier version of this fix. However, while wait times do not need to be calculated with a Retry, the amount of retries in the given time duration window still need to be calculated. Otherwise it never completes and just keeps retrying.
In my first iteration, the retry never matched up to my time value as the increment value would always cause the retry to overshoot the defined property value.
ddanielr@f93c327#diff-56945d7261689b2323a668699ab5865a1e48ec8424b0d612091d29ae0f2fc67cR1510
Because of the complications with the Retry object, and the fact thatblockForTservers() is not attempting an operation as it's just blocking, I chose to simplify it for a wait loop.
I can add in a small wait at the start if you'd prefer, or I'm happy to dig further into the Retry object with someone and see if I'm just completely missing something.
I'm not super concerned about the logs being spammed as this is a block on the main thread so nothing else should really be showing up in the logs until this method has completed. | The following seems close to what is wanted (I built this against 3.0, so there may be changes needed for 1.10)  Ignore the time and count values - they were picked for convenience and not what we'd what to use here.
     Retry.builder().maxRetries(10).retryAfter(50, MILLISECONDS).incrementBy(100, MILLISECONDS)
                        .maxWait(maxWaitSec, SECONDS).backOffFactor(2.0).logInterval(1, SECONDS).createRetry();

...
  while(retry.canRetry()){
            attempts++;
            retry.logRetry(log, ""current counts "" + attempts);
            retry.waitForNextAttempt(log, ""pause for next attempt"");
            retry.useRetry();
        }

I think things to note:

maxWait set the max delay and factors in with the backoff, the delay time will increase up to that value with each attempt.
maxRetries is a count
removing infiniteRetries() and then adding useRetry is required to advance the retry attempt logic | Dug into the retry object a bit with @EdColeman and found that removing the incrementing backoff and just using maxRetries makes the Retry object function as expected.
I've pushed an updated commit that switches back to the Retry object, but calculates an exact max wait vs an approximate value.
It also switches the logging statement to use the Retry logging method which takes advantage of the Retry object's logging interval. | GitHub's UI seems to create a new branch to revert the change if you click the revert button, even if you abort the process and don't actually follow through with reverting anything. I'll delete the unintentionally created revert branch. #3235 is a fix subsequent to this PR that should be finished before merging the 1.10 branch forward into 2.1 and on.","Fix wait timeout logic for available tservers

Replaces the retry object with a wait loop.
Log messages are generated independent of max wait time. | Cleanup time conversions

Implemented PR feedback to switch time checks over to using `System.nanoTime()`.
Removed old import for Retry object.
Added static import for timeUnit.NANOSECONDS. | Switch back to Retry Object

This commit switches back to using a retry object without an
incremental backoff.
This allows the specified max Wait to always equal the exact amount
of time that the main thread waits for tservers.
It also switches the logging statement to use the retry log instead.
This takes advantage of the Retry object's logging interval. | Use static imports for TimeUnit durations

Switches to using static imports for MILLISECONDS and removes the
TimeUnit import for improved readability."
apache/accumulo,3389,https://github.com/apache/accumulo/pull/3389,Show manager goal state in monitor,"Fixes #3373 

This PR adds a banner to the Manager page in 2.1 that will be displayed when the Managers state is not normal or when the state and the goal state differ. The new manager state banner will not be displayed if the manager is down.

Here are some screenshots from the updated Manager page:

When state and goal state differ:
![Screenshot from 2023-05-09 15-19-43](https://github.com/apache/accumulo/assets/47725857/d470c9c1-a739-46fa-905d-8550c4cf9e69)

After state and goal state converge (and the state is not 'NORMAL'):
![Screenshot from 2023-05-09 15-19-49](https://github.com/apache/accumulo/assets/47725857/ea55b0a5-fafe-4c13-a50f-13db758f8600)

When the manager is not running:
![Screenshot from 2023-05-09 15-20-25](https://github.com/apache/accumulo/assets/47725857/fed8a46a-aca1-42e4-920d-7c8579cc41fc)

",2023-05-12T03:32:35Z,ctubbsii,DomGarguilo,"readability, readable","Will this show the status when it is running and normal? | Will this show the status when it is running and normal?

No it should only show if the manager state is not NORMAL or when the manager state and manager goal state differ. | From a user experience perspective - would it be better to always show the status?  Seeing that things are ""normal"" seems that it provides feedback to the user rather than needing to know that lack of status means ""good""? | From a user experience perspective - would it be better to always show the status? Seeing that things are ""normal"" seems that it provides feedback to the user rather than needing to know that lack of status means ""good""?

Yea that makes sense to me. The only reason I made it this way is because that is how things are done in the 1.10 monitor.
If it seems like we want to change things so that the status is always displayed, I am good with that. | From a user experience perspective - would it be better to always show the status? Seeing that things are ""normal"" seems that it provides feedback to the user rather than needing to know that lack of status means ""good""?

Yea that makes sense to me. The only reason I made it this way is because that is how things are done in the 1.10 monitor.
If it seems like we want to change things so that the status is always displayed, I am good with that.

I don't think we need to do that. In the normal state, the manager information table is displayed with all the information about it. There's nothing to alert the user about. The dialog boxes are visually presented as exceptions to the normal state. If we provide a box under the normal situation, then we break that visual experience and possibly make it harder for the users to notice when the exceptional circumstances do occur.
Besides, we already have a visual indicator of manager normality, with the green icon in the upper-right next to the manager menu item.","Show manager goal state in monitor | Create new class. Invert boolean | Formatting | Change manager-banner-message from class to ID

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org>"
apache/accumulo,1824,https://github.com/apache/accumulo/pull/1824,Upgrade to jline3,"There is still some work to be done to this pull request but I wanted to get this out there to get some feedback in some areas.

I still need to do more robust testing on the shell and go back to a few areas that I commented out. I will also refactor/optimize areas if possible. ",2021-02-01T18:34:38Z,Manno15,Manno15,"readability, readable, understandable","@Manno15 This is a great start. It looks like a beast of a problem, but it looks like you're headed in the right direction! The fact that so much of the API of JLine is subject to change makes me wonder if we want to just create our own internal abstraction... like a single class that wraps all the JLine stuff, and puts it all in one place, and provides more friendly methods to get to what we need. I'm thinking something along the lines of:
  class SomeClassName {
    SomeClassName() {
      // set up JLine terminal and related objects here
      // no JLine types used outside this class
      // if we need anything from JLine, we add a convenience method below, so JLine is entirely accessed through this class
    }
    println(...) { ... }
    readline(...) { ... }
    saveHistory(...) { ... }
  }

  var console = new SomeClassName();
  console.println(""something"") | Yeah, that sounds like a great idea @ctubbsii. I will look into to implementing something like that while I continue to fix some of the areas I was unsure on. | This is getting close to being finished. Some issues I am facing are history file incompatibilities from previous versions (I believe jline3 requires timestamps in their history file) and hitting 'q' while in the help command doesn't exit out of the command until you hit enter after the 'q'. | I believe this pull request is ready for review. There are a couple of areas where I unsure of the proper route to go on. One is the use of DumbTerminal in ShellIT and ShellServerIT. The other concern is here | Looks good from what I can tell. Manually ran a lot of the commands and everything appears as expected. | I am running into an issue getting these changes to run with zookeeper 3.6.2 on fluo-uno. I will investigate. Specific error below:
Exception in thread ""shell"" java.lang.UnsatisfiedLinkError: 'int org.fusesource.jansi.internal.CLibrary.ioctl(int, long, org.fusesource.jansi.internal.CLibrary$WinSize)'
	at org.fusesource.jansi.internal.CLibrary.ioctl(Native Method)
	at org.jline.terminal.impl.jansi.JansiNativePty.getSize(JansiNativePty.java:146)
	at org.jline.terminal.impl.AbstractPosixTerminal.getSize(AbstractPosixTerminal.java:60)
	at org.jline.terminal.Terminal.getBufferSize(Terminal.java:216)
	at org.jline.reader.impl.LineReaderImpl.doDisplay(LineReaderImpl.java:746)
	at org.jline.reader.impl.LineReaderImpl.(LineReaderImpl.java:303)
	at org.jline.reader.LineReaderBuilder.build(LineReaderBuilder.java:115)
	at org.apache.accumulo.shell.Shell.config(Shell.java:275)
	at org.apache.accumulo.shell.Shell.execute(Shell.java:520)
	at org.apache.accumulo.start.Main.lambda$execKeyword$0(Main.java:126)
	at java.base/java.lang.Thread.run(Thread.java:834)


EDIT: My guess is for some reason LineReader is trying to default to using Jansi for the terminal and we don't have the correct libraries for it? I will push a commit that tells the terminal to not allow Jansi so it should default to a different type. I believe the old default type was xterm so not sure why the new zookeeper version updates the default to Jansi. | @ctubbsii  I believe I hit on everyones questions and suggestions. Do you have anything further to add or any concerns? | @ctubbsii I believe I hit on everyones questions and suggestions. Do you have anything further to add or any concerns?

I can take another look today and re-run the ITs. | @ctubbsii I believe I hit on everyones questions and suggestions. Do you have anything further to add or any concerns?

I can take another look today and re-run the ITs.

I re-ran all the ITs, and they all pass, except for a few known flaky ones. | I think this is probably mostly okay to merge now. However, I haven't done any manual testing... only code inspection and running tests. So, we should be careful to observe any behavioral changes to the shell as we continue development/testing on 2.1.

Understandable. Fortunately, most of the commands themselves have stayed the same. The things that have changed the most is tab completion and how the history file is set. Those I have tested thoroughly. The tests have also changed quite a bit and possible improvements could be made to those in future versions of Jline.
Another possible concern is how the terminal and reader will be set up on different machines. We let Jline determine the best terminal type and use the default implementations for other various features that maybe we would want to add later on.  This isn't different from what we did in Jline2 so it isn't a big concern but I have already noticed several changes in functionality between ConsoleReader and the terminal/lineReader combo that is used now.

There is one more thing that does need to be done before this can be merged, though:
the LICENSE file in the assembly tarball needs to be updated (assemble/src/main/resources/LICENSE) based on the upstream license information (there are only slight differences with https://github.com/jline/jline3/blob/jline-parent-3.17.1/LICENSE.txt
By the way, you may wish to consider updating to 3.19.0, in case some things have improved since you started working on this

I will add these Monday. I am curious to see what they changed in 3.19. | I updated the license and upgraded to 3.19.0. There wasn't anything noticeable to the upgrade that would benefit us so no further changes from it. I will merge this today unless there are any objections.","fix conflict | minor changes | changes from mailing list suggestions | changes from testing | Optimizations, more changes based on jline3 javadocs | fix build error | fix clear command. Add some reader opts | fix build error, work on more missing pieces | small changes for testing | fix history by disabling timestamp requirment | Merge branch 'main' of https://github.com/apache/accumulo into upgrade_to_Jline3 | Merge branch 'main' of https://github.com/apache/accumulo into upgrade_to_Jline3 | formatting and removal of commented code | fix help command so quits on 'q' properly | formatting and removal of more comments | fix clear command/test and history command test | fix history command test | fix SetIterCommand test | add assumption so github QA will pass | get everything read for review | Merge branch 'main' of https://github.com/apache/accumulo into upgrade_to_Jline3 | clarify signal handler and use print exception function | fix line wrapping | fix shellIT and shellServerIT | fix shellConfigTest | remove suprressed warnings | catch EndOfFileException and add printLn back | possible fix for deleterFormatterTest | diable terminal from using Jansi as default | add code suggestion | edit on CTRL-D, remove redundant if statement | update license | upgrade to 3.19.0"
apache/accumulo,1865,https://github.com/apache/accumulo/pull/1865,Re-throw exception from LoggingRunnable (re #1808),"There is a robust fix in #1818 that is probably too much to backport to 1.10. This fix simply re-throws the original exception (unless there is an exception when logging and that is more serious (i.e., it's an Error and the original Throwable was not an Error)).",2021-01-13T16:31:50Z,brianloss,brianloss,"readability, readable","Added 2.1.0 since this was merged to the main branch in bdcab44 , even though it will be superseded by #1818 when that work is merged. | @brianloss When you merge these in the UI, GitHub usually appends the PR number as (#1865) (in this case) to the end of the subject line of the commit message. It's helpful to have these in the history, even if the commit additionally references other issues in the body of the git commit message (or elsewhere in the subject line). I noticed in this PR (and your previous one), these references to the PR in which the work was done were not included in the commit message when it was merged, so I wanted to bring this to your attention, because it's helpful to keep these, so it's easier to find the PR from the commit.","Re-throw exception from LoggingRunnable (re #1808)

* Re-throw the caught Throwable unless we received a more serious
  Error when attempting to log the original Throwable. In that case
  propagate the more serious error instead."
apache/accumulo,3219,https://github.com/apache/accumulo/pull/3219,Use AtomicInteger for index pointers in SeekableByteArrayInputStream,"Fixes #3218 

This PR converts `cur` and `max` to `AtomicInteger`. This should address the concerns in the ticket.

One thing I considered is converting `buffer` from a `volatile` variable to an `AtomicRefference` but I am not too familiar with the trade offs associated. If anyone has any insight into this that would be helpful.",2023-03-17T19:35:10Z,ctubbsii,DomGarguilo,"readability, readable","It seems to me that we should probably synchronizing around the code that uses or modifies the cur pointer, to fix that, which is a whole different thing than merely making it volatile.

It seems like synchronization was intentionally avoided here for some reason. The comment at the top mentions avoiding synchronization.

  
    
      accumulo/core/src/main/java/org/apache/accumulo/core/file/blockfile/impl/SeekableByteArrayInputStream.java
    
    
        Lines 28 to 31
      in
      fe31930
    
  
  
    

        
          
           /** 
        

        
          
            * This class is like byte array input stream with two differences. It supports seeking and avoids 
        

        
          
            * synchronization. 
        

        
          
            */ | I'm still not sure what the volatile on the byte array buffer is doing, though... that doesn't seem to have any impact, as far as I can tell.

I made it final in b821992
Also, I tried to refactor some of the repeated logic in aa0f2c9. Not sure if it is an improvement in terms of readability (or anything else for that matter). I am not opposed to reverting the commit, take a look and let me know what you think.","Use AtomicInteger for index pointers in SeekableByteArrayInputStream | Convert max to final int | Merge branch 'apache:main' into makePointersVolatile | Merge branch 'apache:main' into makePointersVolatile | Apply suggestions from code review

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Merge suggestions | Make buffer final | make read() atomic | make skip() atomic | Refactor common code | Remove SuppressFBWarnings and old comment | Add comments and shorten lambdas

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org>"
apache/accumulo,2647,https://github.com/apache/accumulo/pull/2647,Consolidated duplicate Thrift client code,"Removed the following classes: ManagerClient, FateManagerClient,
ServerClient, ReplicationClient, and ClientExec. Moved ClientExecReturn
class into the replication code as that is the only places its used now. Logic
from the different Thrift client classes was reworked into the existing ThriftUtil
and ThriftClientTypes classes.

Closes #2641 ",2022-05-13T21:47:39Z,ctubbsii,dlmarion,"readability, readable","I'd like to take a closer look this week

@ctubbsii - I know you have been busy finishing the ZK changes. Any chance you can look at this again? | The main concern as-is, is that it seems to be incomplete. The client objects are nice, but there's a lot of places where we're still accessing the generated client type directly. Those could be done in a future change.

That wasn't the goal of this PR. The goal here was to remove the duplicate code in FateManagerClient, ManagerClient,  ReplicationClient, and ServerClient

However, the most significant piece of it being incomplete are the parts that are using the new client objects, but calling the methods that aren't implemented.

I did find one place where ThriftClientTypes.execute was being called and I fixed that in 3761865. Not all of the new client objects need to implement these methods, as they are not called. | I saw that NamespacesIT failed when I tested this, but haven't looked into it. | I'll kick off a full build for this branch","wip | Merge branch 'main' into 2641-consolidate-thrift-code

Conflicts:
	core/src/main/java/org/apache/accumulo/core/rpc/ThriftClientTypes.java | Consolidated Thrift client methods, removed ManagerClient, ServerClient and ReplicationClient | Add missing dependency | Removed unused ClientExec, moved ClientExecReturn into replication code | Addressed comments in PR

Moved ThriftClientTypes to a new subpackage and created new classes for
each implementation that required special logic. Added an executeVoid
method and used that to remove the extraneous `return null` lines. | Addressed PR comments | Add missing override annotations for errorprone | Merge branch 'main' into 2641-consolidate-thrift-code | Consolidated code added in #2622 into utility method | Merge branch 'main' into 2641-consolidate-thrift-code | Merge branch 'main' into 2641-consolidate-thrift-code | Code review feedback

* Narrow the exceptions being thrown from Exec methods to just Thrift
  TException (plus RuntimeExceptions)
* Remove catch clauses that aren't needed due to never being thrown or
  redundant with another catch clause
* Remove unnecessary class nesting layers
* Remove unnecessary second generic parameter for ThriftClientTypes
* Use AtomicBoolean to force pass by reference, instead of trying to
  pass boxed Boolean objects, which is a bit dubious, because it's not
  clear to the reader that it's not being auto-boxed/auto-unboxed and
  using pass by value instead of the intended pass by reference
* Make ThriftClientTypes abstract, to make implementing subclasses more
  clear and to more easily identify classes that don't implement certain
  critical methods
* IDE also optimized a few things, removing extra parens, converting
  some anonymous inner classes to lambdas, using method references for
  simple lambdas, and removing unnecessary return statements | Code review feedback

* Get rid of unnecessary passing AccumuloConfiguration
* Avoid unnecessary rewrapping of RuntimeExceptions as another
  RuntimeException when creating the processors | Fix extremely trivial warnings suppression | Address exceptions

* Use narrower exceptions
* Rethrow exception in missing case, where cause of
  TableNotFoundException is not NamespaceNotFoundException
* Add compile-time dependency to shell (hopefully we can fix that in
  future, but it's hard to track down) | Implement the execute method in ReplicationCoordinatorThriftClient | Fix failure in NamespacesIT"
apache/accumulo,2238,https://github.com/apache/accumulo/pull/2238,Add compaction coordinator and compactor to cluster start / stop scripts,"Modified the cluster start/stop scripts to use a new file (cluster.yml)
for defining the hosts that will run the different server components. Added
a class (and test) that parses the yaml file into a form that is usable by
the scripts. Added ability to specify and start/stop the external compaction
server processes.

Closes #2138",2021-09-30T18:08:19Z,dlmarion,dlmarion,"readability, readable","Based on the conversation on #2138 it seems there was consensus on using a YAML file to configure the cluster. Any reason why a simple properties file (similar to accumulo.properties) wouldn't work? | It was brought up here | It was brought up here

Thanks. Sorry I missed that. | Sure, no problem. | @ctubbsii - Thanks for the suggestions. I have made several modifications based on them. | FYI this change broke the Uno install scripts. I opened an issue in Fluo: apache/accumulo-fluo-uno#271","Add compaction coordinator and compactor to cluster start / stop scripts

Modified the cluster start/stop scripts to use a new file (cluster.yml)
for defining the hosts that will run the different server components. Added
a class (and test) that parses the yaml file into a form that is usable by
the scripts. Added ability to specify and start/stop the external compaction
server processes.

Closes #2138 | Fixed FindBugs error, added Coordinator and Compactor to ZooZap | Multiple modifications based on feedback | Changes from testing locally | Changes from testing locally | Update assemble/bin/accumulo-cluster

Co-authored-by: Keith Turner <kturner@apache.org> | Merge branch 'main' into 2138-yaml-cluster-config | Check for existence of old files and exit with warning | Merge branch '2138-yaml-cluster-config' of github.com:dlmarion/accumulo into 2138-yaml-cluster-config | Merge branch 'main' into 2138-yaml-cluster-config | Merge branch 'main' into 2138-yaml-cluster-config | Changed coordinators to compaction-coordinators in ZooZap | Merge branch 'main' into 2138-yaml-cluster-config | Addressed comments from PR.

Renamed yml files to yaml
Ran shellcheck on accumulo-cluster
Fixup test | Fixed all yml references, changed to yaml | Reduce scope of suppressed warning"
apache/accumulo,2914,https://github.com/apache/accumulo/pull/2914,Deprecate FateCommand,"* Deprecate FateCommand by adding warning to usage
* Drop recently added tests from FateCommandTest
* Supports #2215
* See discussion on #2780 about problems trying to complete #2215",2022-09-21T12:52:36Z,milleruntime,milleruntime,"readability, readable","Operators are familiar with the current shell fate operations and may be using them more that expected. I'm not opposed to moving it to the admin command, but the same functionality should be preserved. It would also be nice if the shell could print a message that points to the moved command and at least a hint on how to execute it. | @EdColeman while moving the test testSummary() from FateCommandTest, I noticed that it is calling the overridden method in TestFateCommand class. Was that the intention? That class was really only created for testCommandLineOptions(). | I guess I used it because it was available and used it when testing the command line  - it looks like without it there is a missing config issue that would need to be resolved it it is not to be used. | I want to do more testing with the new commands but this PR is ready for review. I had to make some changes with the commands being moved out of the shell, mainly dropping the FateCommand unit tests and the pagination option, but it was things that didn't translate over to the Admin command. | The newlines in the FateSummaryReport were specifically included to visually break up the sections to make it easier for an operator to visually scan and pull out relevant information. | The newlines in the FateSummaryReport were specifically included to visually break up the sections to make it easier for an operator to visually scan and pull out relevant information.

Right. There were too many line breaks when it gets called and printed to standard out. I think the extra spaces for indents helps break up the sections fine. Here is what gets printed from Admin:
Report Time: 2022-09-14T16:12:00Z
Status counts:
  IN_PROGRESS: 2
Command counts:
  TABLE_COMPACT: 2
Step counts:
  CompactionDriver: 2
Fate transactions (oldest first):
Status Filters: [NONE]
Running	txn_id				Status		Command		Step (top)		locks held:(table id, name)	locks waiting:(table id, name)
0:00:16	70bdabd00c24a451	IN_PROGRESS	TABLE_COMPACT	CompactionDriver	held:[R:(+default,ns:), R:(1,t:ci)]	waiting:[]
0:00:07	5de86f2aefb1b8b6	IN_PROGRESS	TABLE_COMPACT	CompactionDriver	held:[R:(+default,ns:), R:(1,t:ci)]	waiting:[]

I could add blank lines between sections. What do you think? | There is a section in the docs (fate.md) that will need to be updated when these changes are merged. | I am not sure that we should have two versions of the same command that use separate options in this release.  The current command should be deprecated, but could still support the additions made in 2.1.
Having the new options available in the deprecated version seems like it could easy discovery of the new options and serve as a transition.  If someone is using the ""deprecated"" version - they would see the warning, but they may not realize that there are new options available - I guess that adding to the deprecation warning message that new options are available on in the admin version could help - even better would be if the deprecated version help listed the new options as ""unavailable"" - but it may be easier just to keep the two versions in sync until the deprecated version is removed.
As far as the removing line breaks in the summary - I feel they increase readability, but that is a personal preference and can live with it either way. | I am not sure that we should have two versions of the same command that use separate options in this release.  The current command should be deprecated, but could still support the additions made in 2.1.

I could just close this for 2.1. Where do you suggest to move the functionality then? | I am not sure that we should have two versions of the same command that use separate options in this release. The current command should be deprecated, but could still support the additions made in 2.1.
Having the new options available in the deprecated version seems like it could easy discovery of the new options and serve as a transition. If someone is using the ""deprecated"" version - they would see the warning, but they may not realize that there are new options available - I guess that adding to the deprecation warning message that new options are available on in the admin version could help - even better would be if the deprecated version help listed the new options as ""unavailable"" - but it may be easier just to keep the two versions in sync until the deprecated version is removed.

I think you are right that it will be confusing to split the command up between the shell and the admin command. I think it will be better to just deprecate the command here and then move it completely to Admin in the next version. | I think you are right that it will be confusing to split the command up between the shell and the admin command. I think it will be better to just deprecate the command here and then move it completely to Admin in the next version.

I'm not sure I understand this statement. Are you proposing for it to be deprecated without a replacement in 2.1, and then in 3.0 to move it to the Admin command?
I would prefer it exist in the Admin command at the time we deprecate it (which I prefer in 2.1, so we can remove it in 3.0). I do not care, though, if the new options are available or not in the deprecated shell version. | I think you are right that it will be confusing to split the command up between the shell and the admin command. I think it will be better to just deprecate the command here and then move it completely to Admin in the next version.

I'm not sure I understand this statement. Are you proposing for it to be deprecated without a replacement in 2.1, and then in 3.0 to move it to the Admin command?

Yes.

I would prefer it exist in the Admin command at the time we deprecate it (which I prefer in 2.1, so we can remove it in 3.0). I do not care, though, if the new options are available or not in the deprecated shell version.

What do you prefer exists in the Admin command? The entire fate command? | I think you are right that it will be confusing to split the command up between the shell and the admin command. I think it will be better to just deprecate the command here and then move it completely to Admin in the next version.

I'm not sure I understand this statement. Are you proposing for it to be deprecated without a replacement in 2.1, and then in 3.0 to move it to the Admin command?

Yes.

I don't think we should deprecate without a replacement. The replacement should be in place, or it would be confusing to have it deprecated.


I would prefer it exist in the Admin command at the time we deprecate it (which I prefer in 2.1, so we can remove it in 3.0). I do not care, though, if the new options are available or not in the deprecated shell version.

What do you prefer exists in the Admin command? The entire fate command?

Anything being deprecated/replaced.
If it is deprecated now, then moved later, then users will never be pointed in the direction of the new command... because in the deprecated code, the new command won't exist, and when the new command is created, the deprecation won't exist. They need to exist at the same time for a deprecation warning/message to be useful to users. | They need to exist at the same time for a deprecation warning/message to be useful to users.

That's a good point. Do you think we should just move the logic from the shell to the Admin now? That will probably be more work then creating an admin command to call the FateCommand. Either way, having it in two places for this version is significant. | They need to exist at the same time for a deprecation warning/message to be useful to users.

That's a good point. Do you think we should just move the logic from the shell to the Admin now? That will probably be more work then creating an admin command to call the FateCommand. Either way, having it in two places for this version is significant.

We still have a FateAdmin command that lives alongside FateCommand for the shell. I figure just move FateAdmin into the Admin command, add any missing actions that were added to the shell's FateCommand, then change FateCommand so its description says it's deprecated. If it's easier to make them share code, fine. Either way, we should target deletion of the shell FateCommand in 3.0, so as long as FateCommand is deprecated, and Admin's ""fate"" command is fully functional, it should be enough. | Looking for opinions on how the Admin fate -list and fate -print commands should function... The original FateAdmin (which will be dropped with this change) and the shell in 1.10, only allowed printing all transactions. In the current version, we added the ability to print one or many transactions (making list and print identical). Moving to Admin, I think we need 2 different commands. We could keep print functioning the same way and make list take one or many txIds. But grammatically, I think it makes more sense to have the new list command list all transactions and have print print information about about one or more txIds. Thoughts? | More on differentiating list and print... This is due to the way that JCommander works. We can't use an ArrayList as the type because there is no way to know if the user entered fate -list with 0 txIds or didn't enter the command. So that's why I was thinking we need a second different type boolean for a listAll option. The user won't know any TxIds from the start, so we still need a listAll option. | Would something like
fate -list --all      or
fate -list tx1 tx2 ....

work?  And pick either list or print? | fate -list all
fate -list tx1 tx2 tx3
Make all a reserved word effectively for the list command? | Thanks for the ideas. I think I figured it out. If I make the ""main"" command for JCommander the list of Txs, then all the rest can be booleans. I will just have to add some validation since it won't make sense to just have the only options be a list of TXs. | I duplicated the FateCommand in Admin. To be more obvious, I overrode the usage to print the deprecation warning first and printed the warning in shell. So it won't matter how the command is called, the user should get the message.
The only question now is if we should drop FateAdmin. This version of Accumulo also includes 2 classes for FateAdmin, in the master and manager packages. I think it should be dropped in this version. A previous version deprecated FateAdmin in favor of the shell. This version we deprecated the shell FateCommand in favor of the Admin command. | @EdColeman correct me if I am wrong but I think you are saying: ""It would be nice if the new options (cancel and summary) were available in FateCommand exposing them to users"".
I will look at making the new options in FateCommand call code in Admin. | I manual tested these changes using Uno and it works as expected. | Yes - I would like cancel and summary to be available - or have a strong pointer to the admin command. | Yes - I would like cancel and summary to be available - or have a strong pointer to the admin command.

So the Shell can't call Admin directly because they are in different packages. If we keep the new options in the Shell, I could print the new Admin command for the user to run if they try running the new options in the Shell. Although this feels kinda sneaky, it would look something like:
root@uno> fate -cancel 3cb69aefefe6b6d0
2022-09-20T14:18:48,053 [shell.Shell] WARN : WARNING: This command is deprecated for removal. Use 'accumulo admin'
Run 'accumulo admin fate -c 3cb69aefefe6b6d0'

Another option would be to move the code for the new options to the AdminUtil since that is in core and then have them both call AdminUtil, similar to the other options. | I think it would be enough it if was.
fate -cancel 3cb69aefefe6b6d0
2022-09-20T14:18:48,053 [shell.Shell] WARN : WARNING: This command is deprecated for removal. Use 'accumulo admin'
This option in not available - use 'accumulo admin' | It is a bit strange to add new options to the Shell FateCommand just to tell the user to use the Admin command but I think it accomplishes all of our goals.

Deprecate FateCommand for removal to stop reading SiteConfig
Provide full functionality of Fate operations in the Admin command.
Expose the new options (cancel & summary) in Shell FateCommand to user, directing them to Admin

I will merge this PR and open a follow on in order to determine when to drop FateAdmin.","Move new FateCommand options to Admin

* Move new cancel and summary options to Admin.
* Deprecate FateCommand by adding warning to usage
* Drop pagination option for Admin
* Drop recently added tests from FateCommandTest
* Supports #2215 | Cleanup | Format | Update shell/src/main/java/org/apache/accumulo/shell/commands/FateCommand.java

Co-authored-by: Dave Marion <dlmarion@apache.org> | CR update | Duplicate FateCommand in Admin & other improvements | Replace system exit with exceptions | Make warning static | Print message for new options in FateCommand"
apache/accumulo,2224,https://github.com/apache/accumulo/pull/2224,Versioned Properties - refactored to address PR comments,"This replaces PR #2194 Refactored to address PR comments

This the first step in moving towards a refactored ZooKeeper property storage. Intended to replace using individual nodes in ZooKeeper to save properties to a versioned group that is stored on a ZooKeeper single node. The PR contains the changes that provide serialization / deserialization of the properties along with maintaining versioning information.

Designed to allow evolution of the storage scheme.
Provides a header that maintains schema version, data version, and timestamp.
Optional compression of the byte storage array.",2021-09-28T22:47:02Z,EdColeman,EdColeman,"readability, readable",The partially addresses #1454 and #1225,"Versioned Properties - refactored to address PR comments | Reworked to incorporate PR comments.

  - Removed codec factory
  - Removed PropSerDes interface
  - Added second example codec that uses encryption
  - Simplified property access | Clean-up and added convenience methods

  - Moved encrypting codec to test package
  - added convenience method to update single property k,v pair | Address PR comments

  - renamed update to addOrUpdate
  - java doc corrections. | VersionPropEncoding - addresses latest PR comments"
apache/accumulo,4524,https://github.com/apache/accumulo/pull/4524,Fate reservations moved out of memory,"closes #4131
Fate reservations were moved out of memory. This is a WIP PR* for one part that is needed for having Fate be distributed in the future.

- Reservations for MetaFateStore were moved out of memory into ZooKeeper
- Reservations for UserFateStore were moved out of memory into the Accumulo Fate table
	- Is an additional column which just indicates whether the FateId of that row is reserved or not
- Added test MultipleStoresIT
	- Tests the new functionality of how reservations are stored
	- I also included a couple tests in MultipleStoresIT that are testing the reserve/unreserve functionality and that it still works as intended. This can be moved from this class if desired.
- Verified existing tests in the fate test package (FateIT, FateOpsCommandsIT, FateStoreIT, ZooMutatorIT, FateMutatorImplIT, UserFateStoreIT) still pass

*There are still a few things I have marked as TODO that I would like input/suggestions on. These are marked in the code as ""TODO 4131"".",2024-09-19T16:49:25Z,kevinrr888,kevinrr888,"readability, readable","Marking this as ready for review, since it may not be reviewed otherwise | I have made the following changes in e6671fb:

Combined the LockID and reservation attempt UUID into one new object: FateReservation
Now cover edge case with tryReserve() where a write may make it to the server, but the server dies before a response is received
New FATE Thread: DeadReservationCleaner which deletes reservations held by Managers that have since died
Created new classes: ColumnValueMappingIterator and ReservationMappingIterator. ColumnValueMappingIterator abstracts out the common functionality of ReservationMappingIterator and StatusMappingIterator. ReservationMappingIterator is an iterator used for determining if the reservation column for a FateId has a FateReservation set or not
Expanded/improved/simplified MultipleStoresIT tests

There are also some comments/questions labeled ""TODO 4131"" that I would like input on.
I also have to resolve the merge conflicts which I will do now. (edit: resolved) | @keith-turner Thanks for the review. I addressed the suggested changes. This PR is ready for re-review | @keith-turner, @DomGarguilo
I have addressed all of the review changes, and this PR is ready for re-review. However, some tests are no longer passing (e.g., ComprehensiveIT). I believe this is an issue with my new implementation of createAndReserve. I believe the issue is with the UserFateStore implementation, but could be both. Posting this new commit now to get the other changes out, and to potentially receive feedback on what might be wrong with createAndReserve(). I will continue to look into what the issue might be in the meantime. Please take a look at my new review comments and my new commit message | I have found and addressed the bug in my most recent commit. This PR is now ready for full review.
The tests verified to still pass: the sunny day tests, all fate tests (ZooMutatorIT, FateMutatorImplIT, UserFateStoreIT, FateInterleavingIT, FateIT, FateOpsCommandsIT, ComprehensiveFlakyFateIT, DeleteRowsFlakyFateIT, ManagerRepoIT, and MultipleStoresIT (new test added)) except FateStoreIT. FateStoreIT reflects a method that has been removed in this PR, and want to receive feedback about the removal of this method before changes are made to this test. Have a ""TODO 4131"" marking this so this is not forgotten. | @keith-turner - Addressed all review comments, this PR is ready for re-review.
See new commit bd1f174
I also fixed FateStoreIT to work with the new impl of createAndReserve(). A method was deleted in 6760035 that was no longer used which FateStoreIT relied on. Upon fixing this test, noticed that the original impl of createAndReserve expected an error to be thrown in a certain situation, but in the new implementation, an empty optional was returned instead. Fixed the new impl to match the previous impl for this case. This meant changing UFS.createAndReserve and MFS.createAndReserve.
Also verified existing Fate tests still pass: ZooMutatorIT,FateMutatorImplIT,UserFateStoreIT,FateInterleavingIT,FateIT,FateOpsCommandsIT,ComprehensiveFlakyFateIT,DeleteRowsFlakyFateIT,ManagerRepoIT,MultipleStoresIT,FateStoreIT
and verified sunny day tests still pass. | I am finally looking through the test changes right now.   Everything else looks good. | @keith-turner I have compiled and wrote all the follow on issues for this to be merged. This covers all the issues I marked in this PR as TODO 4131 and all the unresolved review comments.
Here is a synopsis of the follow on issues I will create (so far - still need some questions answered):

Refactor MultipleStoresIT to function more similarly to other Fate tests like FateIT
Add FateKey toString() method
Move MetaFateStore to org.apache.accumulo.core.fate.zookeeper
Deprecate AbstractFateStore.createDummyLockID(): this includes creating a path in ZK for utilities to get a ZK lock, changing admin fate fail and admin fate delete commands to get a LockID at this path and no longer require the Manager to be down, and only use a LockID for a store if write operations are expected: the store should fail on write operations if writes are not expected.
Replace AFS.verifyReserved with a condition on the RESERVATION_COLUMN to verify that it is reserved
Make WorkFinder and the TransactionRunners critical to the Manager
Refactor how the RESERVATION_COLUMN works for UserFateStore: create/delete column on reserve/unreserve
Additional fate test case from #4524 (comment)

Here are some questions I still have:

This was an existing TODO that you had marked in your ""distributed FATE"" WIP PR:

// TODO 4131
// TODO make the max time a function of the number of concurrent callers, as the number of
// concurrent callers increases then increase the max wait time
// TODO could support signaling within this instance for known events
// TODO made the maxWait low so this would be responsive... that may put a lot of load in
// the case there are lots of things waiting...

I made the max wait = curr num callers in seconds. Does this address the first TODO? For the other two TODOs, I was not sure what was to be done, so I left them. Can these be safely deleted without creating follow on issue(s)? If follow on issues should be made, perhaps it would be best for you to write these or explain it here because I am not sure what should be done.


Re one of your review comments:
""Not a change to make in this PR. Thinking if we push all of the Fate data into a single node it would be nice to use JSON. That would make the data stored in ZK more human readable and it would make it easier to serialize and de-serialize.""
Could you elaborate on what you mean by ""all of the Fate data into a single node"". By ""all"" do you mean the Repos, TxInfo, Status, Reservation, and FateKey would be in one ZK node? Writing the issue, I want to be sure I'm understanding correctly.


Can #4524 (comment) be resolved? Or should an issue be made?


There was a TODO regarding whether the ZooUtil.LockID was the best type for the lock. Should I make a follow on issue regarding this or is ZooUtil.LockID okay?


Can this #4524 (comment) be resolved or should a follow on issue be made?


Can these be resolved: #4524 (comment), #4524 (comment)


The DeadReservationCleaner runs every 30 seconds. Is this okay? Should this be longer? Shorter?


Once the above questions are answered, I can complete my full list of new follow-on issues. This will allow me to remove all the TODO 4131 in this PR and all the comments in this PR can be resolved as all of these will be addressed or have a follow on issue created for them. | I resolved a few of the comments.


There was a TODO regarding whether the ZooUtil.LockID was the best type for the lock. Should I make a follow on issue regarding this or is ZooUtil.LockID okay?


That type seems like the best existing type to use, so I do not think an issue is needed.

The DeadReservationCleaner runs every 30 seconds. Is this okay? Should this be longer? Shorter?

30 seconds seems too short to me.  Would be doing a lot of work and usually never finding anything, should probably be longer maybe 2 or 3 minutes.

Could you elaborate on what you mean by ""all of the Fate data into a single node"". By ""all"" do you mean the Repos, TxInfo, Status, Reservation, and FateKey would be in one ZK node? Writing the issue, I want to be sure I'm understanding correctly.

I was thinking of putting all data that is related to a single fate id into a single zk node.  This will allow atomic updates.  It will be slower (always have to rewrite all repos when adding/removing).  We are only using it for the metadata table.  So thinking it will make things more correct at the expense of speed but that tradeoff is good for metadata related fates.

Can these be safely deleted without creating follow on issue(s)? If follow on issues should be made, perhaps it would be best for you to write these or explain it here because I am not sure what should be done.

Yeah you can delete those.  I will look into it and open an issue if needed. | @kevinrr888 it seems like the only unresolved conversations are covered by your list of issues to open.  I just left those open for now. | @keith-turner - The TODO 4131 comments have now been removed and I believe this PR is ready to be merged 🎉
I was planning on creating the follow-on issues after this is merged in as a batch of new issues.
I will post a list of all the outstanding changes to be made below. The trivial ones, I will just make a PR for without creating an issue, the more involved follow-ons will be tracked in an issue.
Here is the list of changes to be made:
To be tracked by issue:

 Refactor MultipleStoresIT to function more similarly to other Fate tests like FateIT
 Deprecate AbstractFateStore.createDummyLockID(): this includes creating a path in ZK for utilities to get a ZK lock, changing admin fate fail and admin fate delete commands to get a LockID at this path and no longer require the Manager to be down, and only use a LockID for a store if write operations are expected: the store should fail on write operations if writes are not expected.
 A single ZK node for all fate data for each fate id
 Replace AFS.verifyReserved with a condition on the RESERVATION_COLUMN to verify that it is reserved
 Make WorkFinder and the TransactionRunners critical to the Manager
 Refactor how the RESERVATION_COLUMN works for UserFateStore: create/delete column on reserve/unreserve

Will do in one or a few small PRs:

 Add a toString() to FateKey
 Move MetaFateStore to org.apache.accumulo.core.fate.zookeeper
 Periodic clean up of dead reservations should be increased from every 30 seconds to every few minutes
 Add additional fate test case suggested in #4524 (comment) | Resolved the merge conflicts with AbstractFateStore using the new CountDownTimer.
Also noticed a sync block that should have been removed so removed that.
AbstractFateStore should probably be looked at again before this is merged. | @kevinrr888 -

This was an existing TODO that you had marked in your ""distributed FATE"" WIP PR:
// TODO 4131
// TODO make the max time a function of the number of concurrent callers, as the number of
// concurrent callers increases then increase the max wait time
// TODO could support signaling within this instance for known events
// TODO made the maxWait low so this would be responsive... that may put a lot of load in
// the case there are lots of things waiting...
I made the max wait = curr num callers in seconds. Does this address the first TODO? For the other two TODOs, I was not sure what was to be done, so I left them. Can these be safely deleted without creating follow on issue(s)? If follow on issues should be made, perhaps it would be best for you to write these or explain it here because I am not sure what should be done.

@keith-turner -

Yeah you can delete those. I will look into it and open an issue if needed.

This may still need to be looked into just posting this as a reminder just in case","Fate reservations moved out of memory

- Reservations for MetaFateStore were moved out of memory into ZooKeeper
- Reservations for UserFateStore were moved out of memory into the Accumulo Fate table
- Added test MultipleStoresIT
This commit is one part needed for having Fate be distributed | Changes:

- Combined the LockID and reservation attempt UUID into one new object: FateReservation
- Now cover edge case with tryReserve() where a write may make it to the server, but the server dies before a response is received
- New FATE Thread: DeadReservationCleaner which deletes reservations held by Managers that have since died
- Created new classes: ColumnValueMappingIterator and ReservationMappingIterator. ColumnValueMappingIterator abstracts out the common functionality of ReservationMappingIterator and StatusMappingIterator. ReservationMappingIterator is an iterator used for determining if the reservation column for a FateId has a FateReservation set or not
- Expanded/improved/simplified MultipleStoresIT tests | Merge branch 'elasticity' into elasticity-feature-4131 | Changes:

Formatting, fixed a test failure occurring with a newly added test that
was merged in (logic error in my existing code) | Changed a method call which is available in hadoop 3.3.6 but not in the

hadoop version used in the github build (3.0.3), causing a build failure | formatting | Changed a method call which is available in hadoop 3.3.6 but not in the

hadoop version used in the github build (3.0.3), causing a build
failure. This was not fixed with
0809fd399fce27e35d73e518ef9cf34b2e4eb90e. Should be fixed now. | Mockito -> EasyMock in MultipleStoresIT

Changed MultipleStoresIT from using a shaded library in hadoop (Mockito)
to using EasyMock | Merge branch 'elasticity' into elasticity-feature-4131 | Addressed review comments:

- Simplified MetaFateStore.isReserved(FateId) (unnecessary error handling)
- Added check in MetaFateStore.FateTxStoreImpl.setStatus() that was missing: needed to ensure that the reservation stored in the store and in ZK were equivalent
- Added some logging to MetaFateStore.FateTxStoreImpl.unreserve()
- No longer using a ConditionalMutation to check if a FateId is reserved for UserFateStore, just scan the table instead
- Made the DeadReservationCleaner a critical thread to the Manager (if it dies, so does the Manager)
- Stores now take a Predicate<ZooUtil.LockID> isLockHeld which is used to determine if the given LockID is live. This considerably simplified the new MultipleStoresIT and the stores themselves. | Merge branch 'elasticity' into elasticity-feature-4131 | Build fix | Changes:

- Stricter check of the column value for fate reservations. If anything unexpected is seen, an error is now thrown.
- Simplified MetaFateStore.getActiveReservations() to only read from ZooKeeper once.
- Combined the two scans that were done in AbstractFateStore.runnable() into one. This meant adding FateReservation to FateIdStatus and refactoring Meta/UserFateStore.getTransactions().
- No longer use/store a string representation of the FateReservation (was used in UserFateStore). Now, only the serialized value is used. This keeps the usage of FateReservation consistent across Meta and UserFateStore. It was also unneccessary, so simplifies code.
- Moved AbstractFateStore.createAndReserve() implementation into Meta and UserFateStore, and rewrote the impl for each to work with the new way reservations are stored. This also allowed me to delete methods that were only used by AFS.createAndReserve(): create(FateId, FateKey), getStatusAndKey(FateId), create(FateKey).
- Fixed how concurrentStatusChangeCallers was decremented in AbstractFateStore.waitForStatusChange()
- Small change to MetaFateStore.deleteDeadReservations() to avoid reading from ZK unnecessarily
- Added isReservedBy() method to MetaFateStore.NodeValue to avoid code duplication and make the code more clear.
- Since the FateIdStatus now has the FateReservation, realized Meta and UserFateStore.getActiveReservations() could now be simplified to just call list(). This also made the impls the same, so moved to AbstractFateStore. This also made me realize that I had put getActiveReservations() method signature in FateStore, but would be better suited for ReadOnlyFateStore, so moved it there.
- Deleted FateStore.isReserved(FateId)... No longer needed/used
- Moved UNKNOWN status check in AbstractFateStore.reserve() into waiting loop
- Now log when a dead reservation is detected and deleted
- Minor change to Fate: no longer create the executor for the dead reservation cleaner if it's not going to be used | Merge branch 'elasticity' into elasticity-feature-4131 | Fixed UserFateStore.createAndReserve()

- New createAndReserve was not working as expected for UserFateStore, fixed the bug | Bug fixes and code quality improvements:

- Replaced static vars used to determine whether to run the dead reservation cleaner in Fate with a new parameter to Fate
- Changed AbstractFateStore constructor to only accept non-null values for the LockID, and instead have the caller create a dummy LockID if needed
- Fixed a case for MetaFateStore.tryReserve() where an exception would be thrown when instead an empty optional should be returned. This also keeps the functionality of User/MetaFateStore.tryReserve() the same, which they weren't entirely before this change. Also added check for this equivalent functionality in MultipleStoresIT.testReserveUnreserve().
- Fixed issue with MetaFateStore.deleteDeadReservations() where a NoNodeException was thrown when it shouldn't be
- Added fate id to error message for MetaFateStore.setStatus()
- Trivial simplification to MetaFateStore.getTransactions(): return whole node instead of a pair of the needed fields
- Added check to MetaFateStore.NodeValue.deserializeFateKey() and deserializeFateReservation() to ensure valid non-negative length
- Reworked FateStatusFilter (now RowFateStatusFilter) to work with the whole row instead of individual Key/Value pairs
- Fixed UserFateStore.createAndReserve() to only write one mutation instead of two. This required adding a new method FateMutator.putReservedTxOnCreation()
- Added check to UserFateStore.createAndReserve() needed to avoid a potential race condition
- Fixed FateStoreIT to work with new implementation of createAndReserve(). A method was deleted that was no longer used which FateStoreIT relied on.
- Upon fixing FateStoreIT, noticed that the original implementation of createAndReserve expected an error to be thrown in a certain situation, but in the new implementation, an empty optional was returned instead. Fixed the new implementation to match the previous implementation for this case. | Merge branch 'elasticity' into elasticity-feature-4131 | Added check to FateStoreIT.testAbsent()

Added check that both stores have the same expected functionality when
trying to reserve a non-existent fate id | Trivial change to RowFateStatusFilter | Verify err msg contains fate id in MultipleStoresIT | Removed TODOs for this PR: 'TODO 4131' | Merge branch 'elasticity' into elasticity-feature-4131 | removed synchronization block:

- AbstractFateStore still had a synchronization block for accessing `deferred` which was changed to a synchronized map in this PR; removed since there shouldn't be any more sync blocks in this class. | Merge branch 'main' into elasticity-feature-4131"
apache/accumulo,2512,https://github.com/apache/accumulo/pull/2512,Replace n * 1000 with n_000 and n * 1000 * 1000 with n_000_000,Close #2510 ,2022-02-21T05:57:18Z,ctubbsii,KikiManjaro,"readability, readable, clarity","Looks like the build is failing due to formatting issues. Can you rune mvn clean package -DskipTests locally and then push up the changes from that. That will run the formatter. | Doesn't seem that the formatting took place. | I've runned clean and package with maven... Does the fact that i'm using windows could be causing the issue ? | I am not quite sure. Are you adding the changes before pushing them up (no need for a force push)?
It seems the specific issue is in KerberosRenewalIT and is a very small spacing issue in a comment. | I've added all changes and pushed them,
The 'package' command seems to throw an error :
Details
`
C:\Users\kylia.gradle\jdks\jdk-17.0.1+12\bin\java.exe -Dmaven.multiModuleProjectDirectory=C:\Users\kylia\Documents\Projects\accumulo -Dmaven.home=C:\Users\kylia\AppData\Local\JetBrains\Toolbox\apps\IDEA-U\ch-0\213.6777.52\plugins\maven\lib\maven3 -Dclassworlds.conf=C:\Users\kylia\AppData\Local\JetBrains\Toolbox\apps\IDEA-U\ch-0\213.6777.52\plugins\maven\lib\maven3\bin\m2.conf -Dmaven.ext.class.path=C:\Users\kylia\AppData\Local\JetBrains\Toolbox\apps\IDEA-U\ch-0\213.6777.52\plugins\maven\lib\maven-event-listener.jar -javaagent:C:\Users\kylia\AppData\Local\JetBrains\Toolbox\apps\IDEA-U\ch-0\213.6777.52\lib\idea_rt.jar=52583:C:\Users\kylia\AppData\Local\JetBrains\Toolbox\apps\IDEA-U\ch-0\213.6777.52\bin -Dfile.encoding=UTF-8 -classpath C:\Users\kylia\AppData\Local\JetBrains\Toolbox\apps\IDEA-U\ch-0\213.6777.52\plugins\maven\lib\maven3\boot\plexus-classworlds-2.6.0.jar;C:\Users\kylia\AppData\Local\JetBrains\Toolbox\apps\IDEA-U\ch-0\213.6777.52\plugins\maven\lib\maven3\boot\plexus-classworlds.license org.codehaus.classworlds.Launcher -Didea.version=2021.3.2 package
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO]
[INFO] Apache Accumulo Project                                            [pom]
[INFO] Apache Accumulo Start                                              [jar]
[INFO] Apache Accumulo Core                                               [jar]
[INFO] Apache Accumulo Server Base                                        [jar]
[INFO] Apache Accumulo Compaction Coordinator                             [jar]
[INFO] Apache Accumulo Compactor                                          [jar]
[INFO] Apache Accumulo GC Server                                          [jar]
[INFO] Apache Accumulo Manager Server                                     [jar]
[INFO] Apache Accumulo Monitor Server                                     [jar]
[INFO] Apache Accumulo Tablet Server                                      [jar]
[INFO] Apache Accumulo MiniCluster                                        [jar]
[INFO] Apache Accumulo Native Libraries                                   [pom]
[INFO] Apache Accumulo Shell                                              [jar]
[INFO] Apache Accumulo Iterator Test Harness                              [jar]
[INFO] Apache Accumulo Testing                                            [jar]
[INFO] Apache Accumulo Hadoop MapReduce                                   [jar]
[INFO] Apache Accumulo                                                    [pom]
[INFO] Apache Accumulo Master Server (Deprecated)                         [pom]
[INFO]
[INFO] ----------------< org.apache.accumulo:accumulo-project >----------------
[INFO] Building Apache Accumulo Project 2.1.0-SNAPSHOT                   [1/18]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO]
[INFO] --- build-helper-maven-plugin:3.3.0:parse-version (parse-project-version) @ accumulo-project ---
[INFO]
[INFO] --- maven-enforcer-plugin:3.0.0-M3:enforce (enforce-maven-version) @ accumulo-project ---
[INFO]
[INFO] --- maven-enforcer-plugin:3.0.0-M3:enforce (enforce-java-version) @ accumulo-project ---
[INFO]
[INFO] --- maven-enforcer-plugin:3.0.0-M3:enforce (enforce-output-timestamp-property) @ accumulo-project ---
[INFO]
[INFO] --- maven-enforcer-plugin:3.0.0-M3:enforce (enforce-accumulo-rules) @ accumulo-project ---
[INFO]
[INFO] --- mavanagaiata:1.0.0:commit (git-commit) @ accumulo-project ---
[INFO]
[INFO] --- sortpom-maven-plugin:3.0.0:sort (sort-pom) @ accumulo-project ---
[INFO] Sorting file C:\Users\kylia\Documents\Projects\accumulo\pom.xml
[INFO] Pom file is already sorted, exiting
[INFO]
[INFO] --- formatter-maven-plugin:2.17.1:format (format-java-source) @ accumulo-project ---
[INFO]
[INFO] --- impsort-maven-plugin:1.6.2:sort (sort-imports) @ accumulo-project ---
[INFO] Processed 0 files in 00:00.000 (Already Sorted: 0, Needed Sorting: 0)
[INFO]
[INFO] --- maven-remote-resources-plugin:1.7.0:process (process-resource-bundles) @ accumulo-project ---
[INFO] Preparing remote bundle org.apache:apache-jar-resource-bundle:1.4
[INFO] Copying 3 resources from 1 bundle.
[INFO]
[INFO] --- license-maven-plugin:4.1:format (license-headers) @ accumulo-project ---
[INFO] Updating license headers...
[INFO]
[INFO] --- modernizer-maven-plugin:2.2.0:modernizer (modernizer) @ accumulo-project ---
[INFO]
[INFO] --- warbucks-maven-plugin:1.1.2:check (check-junit-categories-on-its) @ accumulo-project ---
[INFO] Rule 0: Class Matches: 0
[INFO] Rule 0: Class Failures: 0
[INFO] Total class failures: 0
[INFO]
[INFO] --- apache-rat-plugin:0.13:check (check-licenses) @ accumulo-project ---
[INFO] Enabled default license matchers.
[INFO] Will parse SCM ignores for exclusions...
[INFO] Parsing exclusions from C:\Users\kylia\Documents\Projects\accumulo.gitignore
[INFO] Finished adding exclusions from SCM ignore files.
[INFO] 91 implicit excludes (use -debug for more details).
[INFO] 7 explicit excludes (use -debug for more details).
[INFO] 14 resources included (use -debug for more details)
[INFO] Rat check: Summary over all files. Unapproved: 0, unknown: 0, generated: 0, approved: 9 licenses.
[INFO]
[INFO] --- maven-site-plugin:3.9.1:attach-descriptor (attach-descriptor) @ accumulo-project ---
[INFO] Attaching 'src\site\site.xml' site descriptor with classifier 'site'.
[INFO]
[INFO] -----------------< org.apache.accumulo:accumulo-start >-----------------
[INFO] Building Apache Accumulo Start 2.1.0-SNAPSHOT                     [2/18]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO]
[INFO] --- build-helper-maven-plugin:3.3.0:parse-version (parse-project-version) @ accumulo-start ---
[INFO]
[INFO] --- maven-enforcer-plugin:3.0.0-M3:enforce (enforce-maven-version) @ accumulo-start ---
[INFO]
[INFO] --- maven-enforcer-plugin:3.0.0-M3:enforce (enforce-java-version) @ accumulo-start ---
[INFO]
[INFO] --- maven-enforcer-plugin:3.0.0-M3:enforce (enforce-output-timestamp-property) @ accumulo-start ---
[INFO]
[INFO] --- maven-enforcer-plugin:3.0.0-M3:enforce (enforce-accumulo-rules) @ accumulo-start ---
[INFO]
[INFO] --- mavanagaiata:1.0.0:commit (git-commit) @ accumulo-start ---
[INFO]
[INFO] --- sortpom-maven-plugin:3.0.0:sort (sort-pom) @ accumulo-start ---
[INFO] Sorting file C:\Users\kylia\Documents\Projects\accumulo\start\pom.xml
[INFO] Pom file is already sorted, exiting
[INFO]
[INFO] --- formatter-maven-plugin:2.17.1:format (format-java-source) @ accumulo-start ---
[INFO] Processed 24 files in 235ms (Formatted: 0, Unchanged: 24, Failed: 0, Readonly: 0)
[INFO]
[INFO] --- impsort-maven-plugin:1.6.2:sort (sort-imports) @ accumulo-start ---
[INFO] Processed 24 files in 00:00.026 (Already Sorted: 24, Needed Sorting: 0)
[INFO]
[INFO] --- maven-remote-resources-plugin:1.7.0:process (process-resource-bundles) @ accumulo-start ---
[INFO] Preparing remote bundle org.apache:apache-jar-resource-bundle:1.4
[INFO] Copying 3 resources from 1 bundle.
[INFO]
[INFO] --- maven-resources-plugin:3.2.0:resources (default-resources) @ accumulo-start ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Using 'UTF-8' encoding to copy filtered properties files.
[INFO] skip non existing resourceDirectory C:\Users\kylia\Documents\Projects\accumulo\start\src\main\resources
[INFO] Copying 3 resources
[INFO]
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ accumulo-start ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 17 source files to C:\Users\kylia\Documents\Projects\accumulo\start\target\classes
[INFO]
[INFO] --- maven-resources-plugin:3.2.0:testResources (default-testResources) @ accumulo-start ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Using 'UTF-8' encoding to copy filtered properties files.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO]
[INFO] --- license-maven-plugin:4.1:format (license-headers) @ accumulo-start ---
[INFO] Updating license headers...
[INFO]
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ accumulo-start ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 7 source files to C:\Users\kylia\Documents\Projects\accumulo\start\target\test-classes
[INFO]
[INFO] --- modernizer-maven-plugin:2.2.0:modernizer (modernizer) @ accumulo-start ---
[INFO]
[INFO] --- warbucks-maven-plugin:1.1.2:check (check-junit-categories-on-its) @ accumulo-start ---
[INFO] Rule 0: Class Matches: 0
[INFO] Rule 0: Class Failures: 0
[INFO] Total class failures: 0
[INFO]
[INFO] --- exec-maven-plugin:3.0.0:exec (Build Test jars) @ accumulo-start ---
[ERROR] Command execution failed.
java.io.IOException: Cannot run program ""C:\Users\kylia\Documents\Projects\accumulo\start\src\test\shell\makeTestJars.sh"" (in directory ""C:\Users\kylia\Documents\Projects\accumulo\start""): CreateProcess error=193, %1 n’est pas une application Win32 valide
at java.lang.ProcessBuilder.start (ProcessBuilder.java:1143)
at java.lang.ProcessBuilder.start (ProcessBuilder.java:1073)
at java.lang.Runtime.exec (Runtime.java:594)
at org.apache.commons.exec.launcher.Java13CommandLauncher.exec (Java13CommandLauncher.java:61)
at org.apache.commons.exec.DefaultExecutor.launch (DefaultExecutor.java:279)
at org.apache.commons.exec.DefaultExecutor.executeInternal (DefaultExecutor.java:336)
at org.apache.commons.exec.DefaultExecutor.execute (DefaultExecutor.java:166)
at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:982)
at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:929)
at org.codehaus.mojo.exec.ExecMojo.execute (ExecMojo.java:457)
at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)
at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)
at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)
at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)
at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)
at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke (Method.java:568)
at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
at org.codehaus.classworlds.Launcher.main (Launcher.java:47)
Caused by: java.io.IOException: CreateProcess error=193, %1 n’est pas une application Win32 valide
at java.lang.ProcessImpl.create (Native Method)
at java.lang.ProcessImpl. (ProcessImpl.java:494)
at java.lang.ProcessImpl.start (ProcessImpl.java:159)
at java.lang.ProcessBuilder.start (ProcessBuilder.java:1110)
at java.lang.ProcessBuilder.start (ProcessBuilder.java:1073)
at java.lang.Runtime.exec (Runtime.java:594)
at org.apache.commons.exec.launcher.Java13CommandLauncher.exec (Java13CommandLauncher.java:61)
at org.apache.commons.exec.DefaultExecutor.launch (DefaultExecutor.java:279)
at org.apache.commons.exec.DefaultExecutor.executeInternal (DefaultExecutor.java:336)
at org.apache.commons.exec.DefaultExecutor.execute (DefaultExecutor.java:166)
at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:982)
at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:929)
at org.codehaus.mojo.exec.ExecMojo.execute (ExecMojo.java:457)
at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)
at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)
at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)
at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)
at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)
at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke (Method.java:568)
at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
at org.codehaus.classworlds.Launcher.main (Launcher.java:47)
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Apache Accumulo Project 2.1.0-SNAPSHOT:
[INFO]
[INFO] Apache Accumulo Project ............................ SUCCESS [  3.031 s]
[INFO] Apache Accumulo Start .............................. FAILURE [  3.534 s]
[INFO] Apache Accumulo Core ............................... SKIPPED
[INFO] Apache Accumulo Server Base ........................ SKIPPED
[INFO] Apache Accumulo Compaction Coordinator ............. SKIPPED
[INFO] Apache Accumulo Compactor .......................... SKIPPED
[INFO] Apache Accumulo GC Server .......................... SKIPPED
[INFO] Apache Accumulo Manager Server ..................... SKIPPED
[INFO] Apache Accumulo Monitor Server ..................... SKIPPED
[INFO] Apache Accumulo Tablet Server ...................... SKIPPED
[INFO] Apache Accumulo MiniCluster ........................ SKIPPED
[INFO] Apache Accumulo Native Libraries ................... SKIPPED
[INFO] Apache Accumulo Shell .............................. SKIPPED
[INFO] Apache Accumulo Iterator Test Harness .............. SKIPPED
[INFO] Apache Accumulo Testing ............................ SKIPPED
[INFO] Apache Accumulo Hadoop MapReduce ................... SKIPPED
[INFO] Apache Accumulo .................................... SKIPPED
[INFO] Apache Accumulo Master Server (Deprecated) ......... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  6.775 s
[INFO] Finished at: 2022-02-20T18:28:00+01:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:3.0.0:exec (Build Test jars) on project accumulo-start: Command execution failed.: Cannot run program ""C:\Users\kylia\Documents\Projects\accumulo\start\src\test\shell\makeTestJars.sh"" (in directory ""C:\Users\kylia\Documents\Projects\accumulo\start""): CreateProcess error=193, %1 n’est pas une application Win32 valide -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn  -rf :accumulo-start
Process finished with exit code 1
` | Yeah, appears to be an issue due to Windows. I went ahead and added a commit that fixes the tiny format issue. | Thanks :) | As for building on Windows, we definitely don't support that, but you can probably get it to work if you build in a terminal on Windows Subsystem for Linux (WSL). | There are two approvals - my previous suggestions could be done as a follow on if this is merged as is. | There are two approvals - my previous suggestions could be done as a follow on if this is merged as is.

I think the suggestions you made are good, but they are more than I intended with the initial ticket. My main goal was to have these cleaned up in the timeouts for the test annotations to smooth the transition to JUnit5, and this PR does what I wanted. I think additional changes should go in a subsequent PR, if @KikiManjaro wants to work on them, or @EdColeman can create a new issue to have somebody else follow up later. @KikiManjaro , would you be interested in doing the changes Ed suggests? | There are two approvals - my previous suggestions could be done as a follow on if this is merged as is.

I think the suggestions you made are good, but they are more than I intended with the initial ticket. My main goal was to have these cleaned up in the timeouts for the test annotations to smooth the transition to JUnit5, and this PR does what I wanted. I think additional changes should go in a subsequent PR, if @KikiManjaro wants to work on them, or @EdColeman can create a new issue to have somebody else follow up later. @KikiManjaro , would you be interested in doing the changes Ed suggests?

Yep :) I'll do it | Yep :) I'll do it

Cool. Just create a new PR with the changes, then. You don't need to create a new issue. | @KikiManjaro - there is another form suggested by @ctubbsii that may be even cleaner to read using Duration.ofHours, Minutes,...(x).toMillis()
Duration.ofHours(24).toMillis() | @KikiManjaro - there is another form suggested by @ctubbsii that may be even cleaner to read using Duration.ofHours, Minutes,...(x).toMillis()
Duration.ofHours(24).toMillis()


There might be subtle differences in overflow behavior between those that probably won't matter, but might be worth paying close attention just in case. If I remember correctly, most lines are shorter with TimeUnit, even more so if you static import the type, as in HOURS.toMillis(24), but Duration might be better in some cases. Duration might also use more object creation whereas TimeUnit is an enum, basically a bag of singletons.",Replace n * 1000 with n_000 and n * 1000 * 1000 with n_000_000 | Update KerberosRenewalIT.java
apache/accumulo,2068,https://github.com/apache/accumulo/pull/2068,Update and improvement in README,"- CONTRIBUTING.md
    - Aggregation to the word 'issues-bug' the link to the Issues with 'Bug' label
    - Improvement of the format
- Moving 'Contributing.md' from the main directory to the '.github/' folder
- README.md
    - Image of the focused logo
    - Badges centered.
    - Division of the text in new sections
    - Aggregation of the section :
        - 'License'
        - 'Contributing'
        - 'Social media'
          - 'Mailing Lists'
- Creation of '.github / code_of_conduct.md'",2021-05-05T20:02:53Z,ctubbsii,nicolasalarconrapela,"readability, readable","@nicolasalarconrapela For the code of conduct page, see my proposal on the ComDev mailing list | For the README, consider these guidelines:

Keep it minimal; only show basics, and defer to the project website for more
Keep it readable in both plain text Markdown and in the Markdown-rendered HTML
Make it look nice on GitHub, with badges, but don't go overboard so that it hurts plain text readability
Use footnote links instead of inline links so that the reader doesn't have to interrupt reading to view a URL
Don't use relative links to files not present in the tarball distribution
Don't use links to resources whose address is likely to change
Don't link to the source code repository
Avoid emojis, unnecessary graphics, and unnecessary special characters


I see fit that these guidelines should be saved or mentioned somewhere in case some other user decides to modify the README. | I see fit that these guidelines should be saved or mentioned somewhere in case some other user decides to modify the README.

To be clear, these aren't official guidelines. All of these are subject to change, based on future community decisions. Nothing is set in stone, and we don't want these to be set in stone. We certainly don't want to have more documentation about how to maintain the README than the actual content contained within it. These are just my attempt to compile and share a list of relevant concepts from a decade of project history, in order to help guide you. Others in the community may have different opinions, values, and suggestions to offer.","Update and improvement in README

- CONTRIBUTING.md
    - Aggregation to the word 'issues-bug' the link to the Issues with 'Bug' label
    - Improvement of the format
- Moving 'Contributing.md' from the main directory to the '.github/' folder
- README.md
    - Image of the focused logo
    - Badges centered.
    - Division of the text in new sections
    - Aggregation of the section :
        - 'License'
        - 'Contributing'
        - 'Social media'
          - 'Mailing Lists'
- Creation of '.github / code_of_conduct.md' | Elimination of '.github/code_of_conduct.md'

See https://github.com/apache/accumulo/pull/2068#issuecomment-830654865 | Elimination in Readme of the 'Social Media' section | FIX logo links | FIX 'Contributing' links

- Elimination of section 'ISSUES' and 'PR'
- Fix links | Changelog removal | Reversion to the original Readme:

- It keeps :
  - apache-accumulo download text from the web
  - section of 'Contributing' | FIX : Add point | Update README.md

Suggestion of @ctubbsil

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update README.md

Suggestiohn of @ctubbsii

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | FIX : Keep the existing 2 lines | Reversing 'br' change per line | Update README.md

Suggestion of @ctubbsii

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org>"
apache/accumulo,5989,https://github.com/apache/accumulo/pull/5989,Added summary of queued and running compactions to coordinator,"This commit adds periodic logging of queued and running external compaction information to the coordinator. The logging is emitted by a new class, CoordinatorSummaryLogger, so that users can easily redirect this log to a new file in the logging configuration.

At each interval this new class will log the number of compactions running for each table, and will log the number of compactors, queued compactions and running compactions for each compaction queue.

The number of queued compactions is an estimate as each tablet server only reports up to 100 different compaction priorities to conserve memory space in the Coordinator (see ExternalCompactionExecutor.summarize).

The metrics are a more accurate source of the number of queued external compactions, but that requires aggregating all of the METRICS_MAJC_QUEUED Meters from all of the TabletServers.

Related to #5965",2025-11-25T12:43:47Z,dlmarion,dlmarion,readable,,"Added summary of queued and running compactions to coordinator

This commit adds periodic logging of queued and running external
compaction information to the coordinator. The logging is emitted
by a new class, CoordinatorSummaryLogger, so that users can easily
redirect this log to a new file in the logging configuration.

At each interval this new class will log the number of compactions
running for each table, and will log the number of compactors,
queued compactions and running compactions for each compaction queue.

The number of queued compactions is an estimate as each tablet server
only reports up to 100 different compaction priorities to conserve
memory space in the Coordinator (see ExternalCompactionExecutor.summarize).

The metrics are a more accurate source of the number of queued external
compactions, but that requires aggregating all of the METRICS_MAJC_QUEUED
Meters from all of the TabletServers.

Related to #5965 | Fix findbugs error by calling method on superclass | Updated logging of queued compactions | Update server/compaction-coordinator/src/main/java/org/apache/accumulo/coordinator/CompactionCoordinator.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update server/compaction-coordinator/src/main/java/org/apache/accumulo/coordinator/CompactionCoordinator.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update server/compaction-coordinator/src/main/java/org/apache/accumulo/coordinator/CoordinatorSummaryLogger.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Fix formatting"
apache/accumulo,5906,https://github.com/apache/accumulo/pull/5906,Replaces pair with record in putSplits impl,"This was done as an experiment to see if replacing usage of Pair with records is worthwhile.  A lot of the usage of Pair in the code is because it used to be so cumbersome to create a quick data class w/ two elements (had to implement hashCode, equals, toString).  Going forward, new code would probably use a record instead of Pair for this case.  

Was curious what modifying existing code would be like.  So picked a place in the code that used Pair and tried changing it record.  The main benefit I found from this change is replacing the getFirst and getSecond methods made the code more readable.  When making this change I had to refer back to the type deceleration to know what getFirst meant.  The drawbacks I observed was that the change was tedious to make and that it has the potential to introduce bugs.  The potential to introduce bugs is probably higher when there is a pair of the same type that is used widely.  Not sure if this is worthwhile to pursue through the codebase.  If we did want to make these changes to existing code, its probably best to do it piece meal and focus on an individual usage of Pair in a PR to lower the chance of introducing bugs.

I started off replacing one usage of Pair and found a second usage in the adjacent code that was actually really easy to replace, so did that one too.  For that second case declared a record in method as the usage of pair was only scoped to that method.

",2025-09-23T14:39:31Z,keith-turner,keith-turner,readable,"I wonder if replacing Pair with a Record is worth the churn in the code. I think the only real difference is that the method names to get the elements would be more descriptive. Pair itself could probably be converted into a Record type though. | I wonder if replacing Pair with a Record is worth the churn in the code.

I do not think its worthwhile to do everywhere after working on this.  Depends on the situation and how hard it is discern what getFirst and getSecond actually are.",Replaces pair with record in putSplits impl | use new record type in more places | use record for simple gson type
apache/accumulo,5887,https://github.com/apache/accumulo/pull/5887,shortens help description for accumulo upgrade,When running `accumulo -help` the description of the upgrade command is really long compared to other commands.  Shortened the description by moving information into the options so that the more detailed information is displayed when running `accumulo upgrade --help`.  Some of the information in the description was redundant w/ the existing help for `--prepare` and did not need to be moved.,2025-09-18T16:57:40Z,keith-turner,keith-turner,readable,,"shortens help description for accumulo upgrade

When running `accumulo -help` the description of the upgrade command is
really long compared to other commands.  Shortened the description by
moving information into the options so that the more detailed
information is displayed when running `accumulo upgrade --help`.  Some
of the information in the description was redundant w/ the existing help
for `--prepare` and did not need to be moved. | format code | use text block | Update server/base/src/main/java/org/apache/accumulo/server/util/UpgradeUtil.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | code review update"
apache/accumulo,5786,https://github.com/apache/accumulo/pull/5786,adds ranges to table locks,"Added ranges to table locks using the ranges from fate operations.  For write locks these ranges are widened to the nearest splits, see comments in Utils.reserveTable for details.  See comments and code in DistributedReadWriteLock.WriteLock.tryLock() and
DistributedReadWriteLock.ReadLock.tryLock() for details about how these ranges change lock behavior.  Added tests to excercise this new locking behavior. Modified the `accumulo admin fate -l` command to display lock ranges.  Also modified how this command determines if a lock is held or waiting.

fixes #2483",2025-08-28T15:12:00Z,keith-turner,keith-turner,readable,"This change is important for the new automatic merge feature.  Even though merges are fast now, if a merge gets stuck waiting on a long running compaction then it will block all subsequent compactions and bulk imports from running.  Adding ranges prevents blocking for the case where the ranges do not overlap.
This PR can also close #1324 #3823 | I was running bulk random walk test against this branch and found #5833.  The test is a good test for this change because it does lots of concurrent table operations with different ranges. | I didn't follow all of the bulk import changes

The cause of the bulk changes was the code was computing the range after it got the lock.  Needed to reorg the code to compute the range prior to getting the lock.","adds ranges to table locks

Added ranges to table locks using the ranges from fate operations.  For
write locks these ranges are widened to the nearest splits, see comments
in Utils.reserveTable for details.  See comments and code in
DistributedReadWriteLock.WriteLock.tryLock() and
DistributedReadWriteLock.ReadLock.tryLock() for details about how these
ranges change lock behavior.  Added tests to excercise this new locking
behavior. Modified the `accumulo admin fate -l` command to display lock
ranges.  Also modified how this command determines if a lock is held or
waiting.

fixes #2483 | Merge remote-tracking branch 'upstream/main' into table-range-lock | use LockRange.toString() | shutdown executor in test | fix spelling | removed todos after opening #5788 | fix copy paste comment problem and improve comment | Update core/src/main/java/org/apache/accumulo/core/fate/zookeeper/FateLock.java

Co-authored-by: Kevin Rathbun <kevinrr888@gmail.com> | Update core/src/main/java/org/apache/accumulo/core/fate/zookeeper/FateLock.java

Co-authored-by: Kevin Rathbun <kevinrr888@gmail.com> | Update core/src/main/java/org/apache/accumulo/core/fate/zookeeper/LockRange.java

Co-authored-by: Kevin Rathbun <kevinrr888@gmail.com> | fix compile | use constant | Add test | Merge remote-tracking branch 'upstream/main' into table-range-lock | fixed bug found running random walk Bulk.xml | Update server/manager/src/main/java/org/apache/accumulo/manager/tableOps/Utils.java

Co-authored-by: Kevin Rathbun <kevinrr888@gmail.com> | format code | remove unused methods"
apache/accumulo,5702,https://github.com/apache/accumulo/pull/5702,Constructed API methods for creating Mutations from key/value pairs,"Created put(Key, Value) and putDelete(Key) methods for easily creating and modifying Mutations from key/value pairs. Made JUnit tests for these methods as well.

Closes #1056",2025-07-17T16:33:42Z,keith-turner,ibilley7,readable,"Happened to see these places in the code where this new API could eventually be used.

  
    
      accumulo/test/src/main/java/org/apache/accumulo/test/functional/TabletManagementIteratorIT.java
    
    
         Line 383
      in
      99dd858
    
  
  
    

        
          
           m.putDelete(entry.getKey().getColumnFamily(), entry.getKey().getColumnQualifier(), 
        
    
  



  
    
      accumulo/test/src/main/java/org/apache/accumulo/test/functional/TabletManagementIteratorIT.java
    
    
         Line 518
      in
      99dd858
    
  
  
    

        
          
           m.put(k.getColumnFamily(), k.getColumnQualifier(), k.getColumnVisibilityParsed(),","Constructed API methods for creating Mutations from key/value pairs

Created put(Key, Value) and putDelete(Key) methods for easily creating and modifying Mutations from key/value pairs. Made JUnit tests for these methods as well.

Closes #1056 | Creating new columns method within FamilyOptions to add a Key rather than a put method | Created new method keyColumns to replace put(Key, Value) and putDelete(Key) and make use of the existing API. Changes javadocs and update JUnit tests. | Created new method keyColumns to replace put(Key, Value) and putDelete(Key) and make use of the existing API. Changes javadocs and update JUnit tests. | Allow differing row IDs | Improved javadocs and edited types. | Improved documentation"
apache/accumulo,4297,https://github.com/apache/accumulo/pull/4297,Replace boolean with enum for table locks in 3.1,"Instead of using a boolean value for obtaining table locks, use an enum which makes the code more readable and makes it easier to search and catch errors with the lock type. The enum used already existed in DistributedReadWriteLock. 

This closes #4276",2024-02-26T18:20:22Z,cshannon,cshannon,readable,"This current PR is for main, I'll fix the conflicts when merging into elasticity after this is merged to main | Actually in elasticity virtually everything is a conflict so it will be simpler to merge this forward as an empty commit and then just do a separate PR for elasticity, #4298","Replace boolean with enum for table locks

Instead of using a boolean value for obtaining table locks, use an enum
which makes the code more readable and makes it easier to search and
catch errors with the lock type.

This closes #4276 | Merge branch 'main' into accumulo-4276 | Address comments"
apache/accumulo,5478,https://github.com/apache/accumulo/pull/5478,Merged FileTypePrefix into FilePrefix,Closes #5476,2025-04-16T16:58:13Z,dlmarion,dlmarion,readable,,"Merged FileTypePrefix into FilePrefix

Closes #5476 | Addressed PR comments"
apache/accumulo,5261,https://github.com/apache/accumulo/pull/5261,Remove zookeeper-jute from minicluster,Use ZooReaderWriter to do ZK updates in MiniAccumuloClusterImpl instead of requiring zookeeper-jute on the classpath (also makes the code shorter and more readable),2025-01-17T00:01:29Z,ctubbsii,ctubbsii,readable,"MAC changes look good. Can't speak on the pom changes though.

The dependency analyzer in the build checks that, and those passed, so we're good here.","Remove zookeeper-jute from minicluster

Use ZooReaderWriter to do ZK updates in MiniAccumuloClusterImpl instead
of requiring zookeeper-jute on the classpath (also makes the code
shorter and more readable)"
apache/accumulo,5246,https://github.com/apache/accumulo/pull/5246,Made ITs that restart MAC faster,"Modified MAC so that it cleaned up lock paths
in ZooKeeper and ZooCache when stopping. Noticed
that in tests that restarted MAC the Manager
process would wait for the previous lock to be
removed on the session timeout. The lock paths
would also be cached in ZooCache and not updated
right away because the Watcher would not fire
when MAC was stopped, a ConnectionLoss error
would be returned when MAC started, and it would
take a while for ZooCache to fix itself.",2025-01-15T20:41:31Z,dlmarion,dlmarion,readable,"These changes shaved about 30s off of VolumeIT locally for me in 2.1. In main, with more tests, it shaved off about 50s.","Made ITs that restart MAC faster

Modified MAC so that it cleaned up lock paths
in ZooKeeper and ZooCache when stopping. Noticed
that in tests that restarted MAC the Manager
process would wait for the previous lock to be
removed on the session timeout. The lock paths
would also be cached in ZooCache and not updated
right away because the Watcher would not fire
when MAC was stopped, a ConnectionLoss error
would be returned when MAC started, and it would
take a while for ZooCache to fix itself. | Implemented PR suggestions | Merge branch '2.1' into faster-mac-restart | Implemented PR suggestions"
apache/accumulo,3761,https://github.com/apache/accumulo/pull/3761,No-chop merge and delete,"This PR is to merge in the no-chop merge and no-chop/no-split feature into main. There are still some follow on tasks to do which I will list and create issues for but the main functionality is complete and now that 3.0 has been release I think it's time to get what is here reviewed and merged into main so we can continue on.

The goal here is to prevent having to run chop compactions when merging (or deleting) tablets which can be quite slow. The other goal is to get rid of having to split tablets during deletions to simplify the manager code. Instead, the changes here allow RFiles to be fenced off by by valid ranges so that we don't need to compact/split anymore and when scanning only the valid range is visible. Files can then be cleaned up later with the normal compaction process.

**Notes:** 

1. This PR squshes all the commits from the [no-chop-merge](https://github.com/apache/accumulo/tree/no-chop-merge) branch into one commit to make it much easier to review. I left that original branch alone for now and pushed this to a different branch.
2. I plan to create a wiki page or website post at some point with full details of how no-chop merge works and the changes (as it would be quite large) but the highlights are below for reviewing the PR:

### Description of Changes:

**TabletFile**

Tablet files used to be unique by just the path. Now a TabletFile also includes a Range. This allows Accumulo to track the same file more than one time, each with a different associated range. So the same file can have several valid ranges and be treated as different files. The benefit here is most of the TabletFile handling code can stay the same. If a Range is not provided (null start/end) then the entire file is visible and this is how backwards compability is handled.

**RFiles**

There is now a new RFile iterator/reader that can fence files by taking a collection of ranges and only returning rows that are part of the given range when scanning. Rows outside of the range are skipped and not visible.

**Metadata**

The File metadata that is stored has been changed to now include a range. Originally each file just contained the path in the Data File column. Now this has been changed so that the value is Json and includes both a file and a range. This means that the same file can be stored more than one time in metadata as the combination of file and range makes it unique.

**Merge Strategy**

During a merge, instead of compacting tablets, each tablet that is being merged will instead have its files fenced by the tablet range. This is a quick operation as this just requires going through the files in metadata and adding the range to the entry and then adding that entry to the target tablet where files are being merged to . If an existing range already exists for the file when processing then that file has its range clipped with the tablet range to get the valid overlapping range. Each file will only have 1 range created.

**Delete Strategy**

Deletes are a little more complicated because besides chop compactions when there are shared boundaries (tablets are stretched to cover the deleted range so we need to compact), we also need to handle splits. Normally splits are first done at the deletion boundaries so entire tablets can be deleted but now we can also fence off tablets instead of splitting. Deletion now works by figuring out the first/last tablets in the deletion range and instead of splitting them or compacting the algorithm will fence the tablets (if needed) and create 0, 1 or 2 ranges. Then any tablets that exist in the middle are still just deleted like before.

When the deletion range overlaps part of the first tablet or part of the last tablet we need to create 1 valid range for the files in those tablets (the part of the files outside the deletion range and not being deleted). This is done instead of a split. If a deletion range is entirely inside of a single tablet then we need to create two ranges (two files are added for each existing file) as we need to fence all the files by the valid range before and after the delete range.

**Future Issues to be Done**

1. All of the existing chop compaction related code can be removed (or deprecated if public API). There's a lot to remove and would make the PR much larger so I figured it was best to do a follow on.
3. We still need some more testing to make sure all edge cases are handled
4. Upgrade code needs to be written once the changes are approved
5. There might be some simpler things we could do to improve things as a follow on. One example @keith-turner mentioned was maybe instead of deleting tablets during deletion we could just remove the files and call merge later. We may also be able to simplify the metadata updates into 1 method/loop vs 2 for deletes. I plan to explore some of this going forward.
6. All of the changes will need to be merged/refactored into Elasticity branch.

**Included PRs:**

https://github.com/apache/accumulo/pull/3418, https://github.com/apache/accumulo/pull/3480, https://github.com/apache/accumulo/pull/3614, https://github.com/apache/accumulo/pull/3621, https://github.com/apache/accumulo/pull/3640, https://github.com/apache/accumulo/pull/3710, https://github.com/apache/accumulo/pull/3728

This closes #1327 ",2023-09-24T18:13:51Z,cshannon,cshannon,readable,"I restarted the full IT build with this new PR | I merged in main to get #3760 to fix the tests, I just re-kicked the full IT again. | I merged in main to get #3760 to fix the tests, I just re-kicked the full IT again.

Full IT build finished and passed. | I have been running this on a single node instance and experimenting with it, its really neat and is working really well. I created this gist showing some of the things I tried. One problem I ran into is there is no easy way to see the ranges, I think we need a follow on issue for this. Need something in place for debugging purposes that makes it easy to see the ranges in the metadata table.

I can create a follow on issue. For debug logging (if we don't already have the StoredTabletFile object loaded) we could deserialize the metadata back into either a StoredTabletFile or just to the json object that is used to serialize which is TabletFileCqMetadataGson and use that to print it as human readable form. Reading it back into TabletFileCqMetadataGson would be quicker and more efficient if we just wanted the Range than loading an entire StoredTabletFile.
For seeing the ranges when writing ITs the simplest thing was to deserialize the metadata entry back into the entire StoredTabletFile object and then print the range object and other things. I did this for all of the ITs I modified for merge/delete so I could easily see what was stored. The helper method I used to print the metadata with human readable ranges is part of this PR in FileMetadata. I just added a log statement where I print the Range and other info: 
  
    
      accumulo/test/src/main/java/org/apache/accumulo/test/util/FileMetadataUtil.java
    
    
        Lines 61 to 63
      in
      ab3e551
    
  
  
    

        
          
           log.debug(""Extent: "" + tabletMetadata.getExtent() + ""; File Name: "" + file.getFileName() 
        

        
          
               + ""; Range: "" + file.getRange() + ""; Entries: "" + dfv.getNumEntries() + "", Size: "" 
        

        
          
               + dfv.getSize()); 
        
    
  


This outputs like the following: Extent: 5<<; File Name: F000000c.rf; Range: (-inf,m%00; : [] 9223372036854775807 false); Entries: 2, Size: 212

I manually exported and imported a table where the files had non infinite ranges and it worked, the ranges made it. Do you know if there are test for this? If not I think we should open a follow on issue to create test for this.

I haven't specifically done any testing with import/export so I can create a follow on for testing. My assumption is that it should work (along with bulk import) due to everything is just treated as a StoredTabletFile in the rest of the code base and the fact that it has a range shouldn't really matter but we should definitely test it.

I manually cloned a table that had files with non infinite ranges and that worked great. If that does not have test, we probably want to open a follow on issue for that.

Same thing here, I don't think there's a test (at least I didn't write one). I am thinking I can create an overall top level issue for adding testing for no-chop merge and we can just keep a running list of things to add and add PRs to cover each case as there's likely several things we want to test with ranges.

Noticed MergeState still has WAITING_FOR_CHOPPED state, is that something you were thinking of removing as a follow on? Have you created any issues for known follow on work yet?

Right, I figured I would just clean up all ""chopped"" related things in a follow on. I started to work on removing all of that stuff, like WAITING_FOR_CHOPPED from MergeState, removing things from Thrift, etc and the diff was starting to hit another 1000 lines of code changes to remove all the chop compaction code. So I figured for now I'd just leave it all as it doesn't hurt anything (just not used) and I was going to create an issue and follow on PR to remove all of it at one time. | @keith-turner - I added the 4 tests you suggested to the list here #3766 . I figured the simplest thing to do was just have 1 issue that tracks all the tests to add and create PRs for each one so we don't end up with a ton of separate issues.
Your external compaction test suggestion made me think of something else. I realized that in some of the tests for Merge/Delete I am counting and verifying the metadata files are the expected count to make sure things look ok in the table. I realized a compaction running could throw this off so I am wondering if I need to update and make sure the ITs won't ever run a compaction or if it's not necessary as there's not enough data being written. Probably wouldn't hurt to disable just so things are deterministic. | I realized a compaction running could throw this off so I am wondering if I need to update and make sure the ITs won't ever run a compaction or if it's not necessary as there's not enough data being written. Probably wouldn't hurt to disable just so things are deterministic.

Yeah it is nice to avoid non deterministic test failures.  Could up the compaction ratio on the table to higher than the number of files created to avoid this.   If creating less files than the default for the compaction ratio it should not be a problem. | I just kicked off one more full IT and assuming that passes I am planning to merge this today. There are several follow on issues to complete (including a major one of merging into elasticity) and they can't be completed until this is merged. 3.1 is not going to be released for a while so there is plenty of time to put in follow on issues and PRs to address any issues with no-chop merge that comes up during testing or later before release. | I realized a compaction running could throw this off so I am wondering if I need to update and make sure the ITs won't ever run a compaction or if it's not necessary as there's not enough data being written. Probably wouldn't hurt to disable just so things are deterministic.

Yeah it is nice to avoid non deterministic test failures. Could up the compaction ratio on the table to higher than the number of files created to avoid this. If creating less files than the default for the compaction ratio it should not be a problem.

Looks like I did do this in one test but need to go back and make sure other tests have it too, will add it to  #3766 | I just kicked off one more full IT and assuming that passes I am planning to merge this today. There are several follow on issues to complete (including a major one of merging into elasticity) and they can't be completed until this is merged. 3.1 is not going to be released for a while so there is plenty of time to put in follow on issues and PRs to address any issues with no-chop merge that comes up during testing or later before release.

The only thing that failed was GarbageCollectorIT due to updates from #3738 which have now been fixed","No chop merge and delete

This PR is to switch merge and delete to no-chop merge and no-chop and
no-split deletions. There are some follow on tasks to do still but the
main functionality is complete.

The goal here is to prevent having to run chop compactions when merging
(or deleting) tablets which can be quite slow. The other goal is to get
rid of having to split tablets during deletions to simplify the manager
code. Instead, the changes here allow RFiles to be fenced off by by
valid ranges so that we don't need to compact/split anymore and when
scanning only the valid range is visible. Files can then be cleaned
up later with the normal compaction process.
This commit contains changes from the following PRs:

https://github.com/apache/accumulo/pull/3418
https://github.com/apache/accumulo/pull/3480
https://github.com/apache/accumulo/pull/3614
https://github.com/apache/accumulo/pull/3621
https://github.com/apache/accumulo/pull/3640
https://github.com/apache/accumulo/pull/3710
https://github.com/apache/accumulo/pull/3728 | Merge branch 'main' into no-chop-merge-squashed | Improve Base64 file metadata serialization

This makes the serialization of the ranges smaller by not serilalizing
the entire buffer if not all the array was used. Also disable escaping
html characters.

Co-authored-by: Keith Turner <kturner@apache.org> | Merge branch 'main' into no-chop-merge-squashed | Merge branch 'main' into no-chop-merge-squashed | Address multiple PR comments

Clean up various things and add tests

Co-authored-by: Keith Turner <kturner@apache.org> | memoize fencedEndKey so it is only loaded if needed | Remove equals call from compareTo in ReferencedTabletFile | fix tests in GarbageCollectorIT"
apache/accumulo,3712,https://github.com/apache/accumulo/pull/3712,Refactor assertTrue(!=) with assertNotEquals & Add explanatory messages,"I am working on research that investigates test smell refactoring in which we identify alternative implementations of test cases, study how commonly used these refactorings are, and assess how acceptable they are in practice.

The first smell is when inappropriate assertions are used, while there exist better alternatives. For example, in [SummaryReportTest.java](https://github.com/apache/accumulo/commit/a657b6ee75d7c6a0556d0059dc3bc1317594d34b#diff-d0c6de0d5f496d76d1c71a7b39cb5ddc4060d8c68ddcf9893fd1f7fd5395d626), I refactored `assertTrue(report.getReportTime() != 0);` using `assertNotEquals` instead.

The second smell is known as _Assertion Roulette_, where a test method has multiple asserts without explanatory messages, which can sometimes make it difficult to identify which assert has failed. Therefore, I added exploratory messages to the asserts in the two test methods in [SummaryReportTest.java](https://github.com/apache/accumulo/commit/a657b6ee75d7c6a0556d0059dc3bc1317594d34b#diff-d0c6de0d5f496d76d1c71a7b39cb5ddc4060d8c68ddcf9893fd1f7fd5395d626).

I would like to get your feedback on these particular test smells and their refactorings. Thanks in advance for your input.",2023-08-21T21:18:52Z,ctubbsii,Taher-Ghaleb,"readable, clarity","Sorry for failing the build, but I don't get why it complains about ""Imports are not sorted"", while they look fine. Please advise. Thanks. | Sorry for failing the build, but I don't get why it complains about ""Imports are not sorted"", while they look fine. Please advise. Thanks.

Running mvn clean package -DskipTests should sort the imports and make sure the code is properly formatted. After running that, you should be able to push the changes and have that build error go away. | You can run the following locally to run the same checks:
mvn clean verify package javadoc:jar -Perrorprone -DskipITs=true -DskipTests=true 

But, I think the issue may be that this import is not needed:
import static org.junit.jupiter.api.Assertions.assertTrue; | Thanks for your responses. I assumed I have already sorted the imports manually, but will also check locally. I removed the unneeded import for now.
What is your opinion about the changes made in this pull request, please? I would appreciate your feedback for my research work. Thanks. | The use of assertNotEquals instead of assertTrue..!= ...!0 seems fine.  I am not sure about usefulness of the explanatory messages in all cases.  The premise that the message adds clarify might not always be true.
When an assert fails, it provides the line as well as the actual / expected value.  So
assertNotNull(report, ""The report should not be null"");

Is not adding much to the failure message.  The same with
assertNotEquals(0, report.getReportTime(), ""Report time should not be 0"");

If will fail with a value not equal to 0 and show expected / received value.  The fact that now it would include that it was report time in the message? Might help, but it is pretty obvious from the code and would need to examine the code anyway, so it does not seem like it adds much.
The case of
assertEquals(Map.of(), report.getStatusCounts(), ""Status counts should be empty"");

is not  clear cut.  One, it is expecting an empty map - so it might be more accurate to state that `Status counts should be an empty map - but that is also redundant with the assertEquals, so again may not be adding clarity.
Using the message when it adds clarity seems like something to keep in mind and should be encouraged.  When it is just repeating the obvious that something was null, not null or equal / not equal, well, it may marginally make the reported test failure easier to read ""foo was not null""... if you only read the test output, but to understand the error in context, you still need to go to the code and then the extra text is not really adding much and is now one more thing to keep in sync / correct for something that is rather self documenting in the first place.
Just my opinion. | Thanks, @EdColeman and @ctubbsii, for your feedback.
I have removed the assertion messages as suggested. I wanted to raise the point on having multiple assert statements in a single test method, which is known in the research literature as a test smell called _ Assertion Roulette_. One of the suggested refactoring of such practice was to add explanatory messages to the assertions. I understand that my messages were not informative enough, but I just wanted to get your opinion about the refactoring approach itself, whether you usually do it in practice or if you have other better alternatives, or if you only do it occasionally when necessary.
Thank you again for your input. | Thanks, @EdColeman and @ctubbsii, for your feedback.
I have removed the assertion messages as suggested. I wanted to raise the point on having multiple assert statements in a single test method, which is known in the research literature as a test smell called _ Assertion Roulette_.

I'm not familiar with that literature, but after a quick Google search, I understand it to be not about multiple assertions themselves, but multiple assertions without sufficient description to identify which of the assertions failed for a given test. However, the general concerns about that are already addressed by using a test framework like JUnit, where the assertion failures already identify the specific line in the code where the assertion failed.
For example, the following would be an example of Assertion Roulette:
  public boolean testMethodAssertionRouletteBadSmall() {
    if (condition1) {
      return true;
    }
    if (condition1) {
      return true;
    }
    if (condition2) {
      return true;
    }
    if (condition3) {
      return true;
    }
    return false;
  }
  
  public static void main(String[] args) {
    if (!testMethodAssertionRouletteBadSmall()) {
      throw new AssertionError();
    }
  }
But, the following would not be, because JUnit adds sufficient information about which specific assertion failed and for what reason:
  @Test
  public void testMethodIsOkay() {
    assertThat(condition1);
    assertThat(condition2);
    assertThat(condition3);
  }
We can add more details in JUnit's API, but often, we don't need to, because it's already implied by the choice of which assertion method to use from JUnit.
So, I don't think our code is a good example of Assertion Roulette.
There are some places in our code that might be, though, where we do something like:
  public void testMethodAssertionRouletteBadSmall() {
    manyConditions.forEach(condition -> assertThat(condition));
  }
These are probably better off with something more like:
  public void testMethodAssertionRouletteIsBetter() {
    manyConditions.forEach(condition -> assertThat(condition, condition.description()));
  }
But even then, we don't do that kind of thing very often, and if something does fail in a loop like that, it's still plenty valuable enough to track down the problem, without needing to bloat the test code with more complexity to avoid the bad smell.",Refactor assertTrue(!=) with assertNotEquals & Add explanatory messages | Remove unneeded import related to assertTrue | Remove verbose assertion explanatory messages
apache/accumulo,5350,https://github.com/apache/accumulo/pull/5350,"""Chroot"" ZooKeeper to base directory instead of specifying full path","This PR initializes ZooKeeper to a new root that allows the full path to be omitted elsewhere in ZooKeeper code.

The original specified path for ZooKeeper functions was typically `/accumulo/[instanceId]/[remainingPathToDesiredNode]`. In this PR, ZooKeeper is initialized with the root after `/accumulo/[instanceId]`, allowing ZooKeeper functions in the code to only need to specify the `/[remainingPathToDesiredNode]` portion, eliminating redundancy. In other words, instead of needing to specify an absolute path, we can now specify a relative path.

Original Issue: #4809 ",2025-03-17T21:11:52Z,ctubbsii,jkucera0,readable,"@meatballspaghetti I pushed a few commits onto this that merged the latest 3.1 onto this branch, and also fixed some additional issues with some of the prop store and prop store related tests. | Oh cool, we are looking at this! 😎 | All the tests are now passing, including all the ITs. I'm going to give this another look over, but I think it's very close to being ready to merge, if not already ready. | 2. Creating Constants.ZRELATIVE_ROOT to replace leading ""/"" in various places in the code.

I'm not sure this one should be done. I think hiding the ""/"" behind a constant in the few places where it is needed makes those places less easy to read. I'd also avoid using words like ""relative"" or ""absolute"" when talking about paths, because those mean different things than what's happening here. Typically, an absolute path is one that starts with a slash and contains no relative path elements (e.g. / or /path/to/someplace), and relative paths contain dots or start with path names (e.g. ./, .., path/to/something, or /path/to/../something). ZooKeeper does not use relative paths at all. However, the absolute paths it does use can be ""relative"" to a ""chroot"" absolute base path element. Referring to absolute paths as ""relative"" to another path can be confusing, even if one takes great care to be precise in the wording. I think it's better to just avoid it. The places where / are used as a leading path prefix, or complete path, are not common, but where they do occur, they are typically more readable and obvious when seen as a string literal than when used as a constant. | I'd also avoid using words like ""relative"" or ""absolute"" when talking about paths, because those mean different things than what's happening here.

Yeah, maybe ZINSTANCE_ROOT is a better  name | I updated this PR with the staged changes in the branch labeled zkchroot-merge-main and also merged in main, and set the base branch to main. | ExternalCompaction_2_IT keeps failing for me, but not every time. So, I don't think it's related to this PR. I think it's a flaky test that has a race condition or something. | I'd also avoid using words like ""relative"" or ""absolute"" when talking about paths, because those mean different things than what's happening here.

Yeah, maybe ZINSTANCE_ROOT is a better name

I agree that is a much better name. If we have need of it, I think that's a good choice to use. However, I still don't see much value in making a Constants.ZINSTANCE_ROOT when ""/"" will suffice and is almost always more clear.","Change ZK paths for Chroot compatibility

- Delete ClientContext.getZooKeeperRoot()
- Remove uses of ZooUtil.getRoot() in which Chroot would apply
- Inline TabletsMetadata.getRootMetadata()
- Remove AdminTest.testZooKeeperTserverPath()
- Remove other functionality whose purpose revolved around the
  root path which will be Chroot-ed.

Resolves: #4809 | Clean up compilation errors from Chroot changes

- Fix misc errors and refactors from previous ZooKeeper Chroot
  path changes
- Add expect line to PropStoreEventTest temporarily

Resolves: #4809 | Post rebase fixes | Refactor ZK Init Name and ID | Reverting some Chroot paths | WIP fix root in ClientContext and ZKInitialize | Modify and revert ZK root paths | WIP | WIP | Remove the need for instanceId in SystemPropKey

Add an intermediate abstract PropStoreKey class for ID-based prop keys
to extend (specifically, TablePropKey and NamespacePropKey), and drop
the generic from the base PropStoreKey, from which SystemPropKey extends
directly. | Merge branch '3.1' into zkchroot-postzkclean | Remove some unused items | Fix ZooBasedConfigIT | Add chroot option to ZooKeeperTestingServer

* Add chroot option to testing server
* Make use of chroot option and fix ZooBasedConfigIT and other prop
  store tests better, using chrooted path for easy cleanup when
  appropriate
* Drop extra context and instanceId from some prop store tests that
  aren't needed
* Stop passing instanceId to ZooPropStore.initialize and new
  ZooPropStore constructor, since it no longer is used | Apply formatting and fix javadoc | Fix remaining broken ITs, Clean constants use
- Fix PropCacheCaffeineImplZkIT, ZooPropEditorIT
- Remove constants from method arguments
- Inline some constants | Add ServiceStatus ReportKey to ZKPath mapping | Fix some tests

* Clean up some mini code (use the server context rather than do extra
  lookups)
* Use ServerContext with ZooZap instead of SiteConfiguration
* Remove unused variables | Merge branch '3.1' into zkchroot-postzkclean | Fix chroot in upgrade tracker code | Revert removal of error catch and clarify message | Merge branch '3.1' into zkchroot-postzkclean | Merge branch '3.1' into zkchroot-postzkclean | Merge branch '3.1' into zkchroot-postzkclean | Merge in progress | Add IT to verify watched event paths are chrooted | Merge branch '3.1' into zkchroot-postzkclean | Require ZooKeeper 3.6 to fix build

The addition of an integration test to verify persistent recursive
watcher behavior with chroot paths requires 3.6. | Merge branch 'zkchroot-postzkclean' into main | Fix broken unit tests post-merge | Fix some build errors

* Drop unused root path from ServiceLockPaths
* Fix some constants to use RootTable.ZROOT_TABLET instead of
  Constants.ZFATE
* Fix ZooCache handling of persistent watched path overlaps | Remove unneeded MetaFateStore path param | Merge branch 'main' into zkchroot-merge-main | Merge branch '3.1' into zkchroot-postzkclean | Merge branch 'zkchroot-postzkclean' into zkchroot-merge-main | Fix one IT, and attempt to fix another

Fix FateOpsCommandsIT, but failed to fix ExternalCompaction_1_IT | Fix mini lock code | Fix ZooCacheIT | Merge branch 'main' into zkchroot-postzkclean | Fix some warnings | Clarify log messages when failure to read root | Fix MetaFateExecutionOrderIT | Remove unnecessary extra details on log message

Failure to read /recovery is sufficient. It is not situated differently
than any other path relative to the chroot. | Merge branch 'main' into zkchroot-postzkclean | Fix ZooInfoViewer acl read | Merge branch 'main' into zkchroot-postzkclean"
apache/accumulo,4850,https://github.com/apache/accumulo/pull/4850,Dynamically generate metrics documentation,"Fixes #4815

* Creates a new class - `Metrics`, which is similar to `Property` where we can add documentation for each metric
* Creates a new `MetricsDocGen` class which pulls the data from `Metrics` and constructs a markdown document from it",2024-09-12T19:45:19Z,DomGarguilo,DomGarguilo,readable,"Opening this PR as draft to hopefully get some feedback on this approach to documenting the metrics. This code is largely modeled after and works the same as the Property and ConfigurationDocGen classes. If this approach looks good I suspect we could probably refactor some shared logic into a parent class but wanted to make sure things look good before spending more time on it.
Here is an example of the output from the few example metrics I added to Metrics in 880411b:

  Click to expand

Below are the metrics used to monitor various components of Accumulo.
General Server Metrics
accumulo.detected.low.memory
Type: GAUGE
Description: reports 1 when process memory usage is above threshold, 0 when memory is okay
accumulo.server.idle
Type: GAUGE
Description: Indicates if the server is idle or not. The value will be 1 when idle and 0 when not idle.
Compactor Metrics
Scan Server Metrics
accumulo.scan.busy.timeout.count
Type: COUNTER
Description: Count of the scans where a busy timeout happened
Fate Metrics | Marking this as ready for review since I think its mostly done.
One thing I thought of is if we want to use the description from the enum when registering the corresponding metric. For example:

  
    
      accumulo/server/tserver/src/main/java/org/apache/accumulo/tserver/BlockCacheMetrics.java
    
    
        Lines 57 to 58
      in
      3a265ed
    
  
  
    

        
          
           FunctionCounter.builder(BLOCKCACHE_INDEX_HITCOUNT.getName(), indexCache, getHitCount) 
        

        
          
               .description(""Index block cache hit count"").register(registry); 
        
    
  


could be changed to
 FunctionCounter.builder(BLOCKCACHE_INDEX_HITCOUNT.getName(), indexCache, getHitCount) 
     .description(BLOCKCACHE_INDEX_HITCOUNT.getDescription()).register(registry); 
this would make the description that appears in the docs the same as the one connected to the micrometer instrument. I am guessing we want to keep the description attached to the micrometer instrument more brief than the one that will appear on the docs but I figured I would ask. | @kevinrr888 All of your suggestions should be addressed now. | FunctionCounter.builder(BLOCKCACHE_INDEX_HITCOUNT.getName(), indexCache, getHitCount) 
  .description(BLOCKCACHE_INDEX_HITCOUNT.getDescription()).register(registry); 
this would make the description that appears in the docs the same as the one connected to the micrometer instrument.

@DomGarguilo  that would be really nice, is there a follow on issue for that? | FunctionCounter.builder(BLOCKCACHE_INDEX_HITCOUNT.getName(), indexCache, getHitCount) 
  .description(BLOCKCACHE_INDEX_HITCOUNT.getDescription()).register(registry); 
this would make the description that appears in the docs the same as the one connected to the micrometer instrument.

@DomGarguilo that would be really nice, is there a follow on issue for that?

Not yet. I can create one though. Two things I am thinking of that should be considered before consolidating these two descriptions though:

Would we want to keep the description attached to the Micrometer instrument shorter than we would want the description that will appear in the markdown doc? This may be an argument for always keeping the description very breif (yet descriptive) but also adding more fields that further describe or explain the metric (as @dlmarion and @ctubbsii were discussing above).
Similarly, if we want to take advantage of markdown styling within the descriptions, would that work okay for the descriptions attached to the Micrometer instrument? | Two things I am thinking of that should be considered before consolidating these two descriptions though:

For point number one we need to figure out what the implications are if any of a ""long"" description for micrometer.  I am assuming long means more than a sentence or two.  If there are no known negative implications, then should probably not worry about it.
For point number two, markdown is meant to be readable in source form. So as long as the characters used by markdown do not cause problems for micrometer then it seems ok to include it. | @keith-turner sounds good. I created #4870 to track this. | I made a PR to the website to add the doc generated by these changes: apache/accumulo-website#442.
Also, here is a screenshot of what things look like currently on the User manual page of the website when I run it from the linked branch: | I think this PR is ready to be merged as far as I can tell. Once this is merged I plan to open a follow on PR in the hopes that everyone can contribute to adding/editing the descriptions for each metric.
I have also verified that running mvn package -DskipTests correctly generates the markdown doc and puts in alongside the other generated markdown docs for client and server properties. | Was there already a discussion about the table format that is used for the properties? If so, I'll go read that. | Was there already a discussion about the table format that is used for the properties? If so, I'll go read that.

Here is a link to where I proposed the current format: #4815 (comment) along with the comment that follows it. That was pretty much the whole discussion on the formatting.
There is still time for more input/opinions as that is something that is pretty easy to change at this point. Do you a table better than how it is now with different sections per metric? @dlmarion | @DomGarguilo - Looking at the server configuration properties table, I like how the entries are more condensed and that there is a separator between each one. If we end up having a short description, and a longer explanation, then I can see how that might not work well in a table. Can the current format be adjusted to put a line separator between each metric, and to reduce the whitespace within the lines of the elements for each metric? | @DomGarguilo - Looking at the server configuration properties table, I like how the entries are more condensed and that there is a separator between each one. If we end up having a short description, and a longer explanation, then I can see how that might not work well in a table. Can the current format be adjusted to put a line separator between each metric, and to reduce the whitespace within the lines of the elements for each metric?

Yea I think those would be good improvements. I'll try to do that.
I think once this is merged and the follow on PR is created to fill out or improve all the descriptions, that will also give us a better idea of whether a table or ""list"" format would be better. | In e818f6d I added some more style changes so that the metrics page will render with alternating background colors per metric as well as reduced line break between fields:

I think this looks nice and did this as opposed to a line separator since there is one after each metrics section title (e.g. ""General Server Metrics"", ""Compactor Metrics"", etc.) | @DomGarguilo  - this looks better, thanks for adding this change.","Dynamically generate metrics documentation | Add all metrics from MetricsProducer docs | Replace Strings from MetricsProducer with Metric Enum | Add fromName() method to Metric | Simplify map to set | fix error prone error | Add metrics markdown generate section to pom | Code review | Code review | Generate sections for all metric categories | Merge remote-tracking branch 'upstream/3.1' into metricsDocGen | Fix typos when converting to new Metrics

Co-authored-by: Kevin Rathbun <krathbun@apache.org> | Fix markdown formatting for underlines | Add styling to metric sections"
apache/accumulo,5127,https://github.com/apache/accumulo/pull/5127,Single node META FATE data,"closes #4905 
- Moved all the fate data for a single `META` transaction into a single ZK node
	- Pushed all the data into `NodeValue` (renamed to `FateData`)
	- Previously, `FateData` stored `TStatus`, `FateReservation`, and `FateKey`. Now additionally stores the `REPO` stack and `TxInfo`.
- Status enforcement added to `MetaFateStore` (previously only possible for `UserFateStore`).
- Moved `testFateInitialConfigCorrectness()` from `UserFateStoreIT` to `UserFateIT`
- Renamed `UserFateStoreIT` to `UserFateStatusEnforcementIT` (now extends a new class `FateStatusEnforcementIT`)
	- Now only tests status enforcement (previously status enforcement + `testFateInitialConfigCorrectness()`)
- Created `MetaFateStatusEnforcementIT` (extends `FateStatusEnforcementIT`)
	- Tests that the new status enforcement in `MetaFateStore` works
- Created `FateStoreUtil`, moving the `createFateTable()` util here, created MetaFateZKSetup inner class here (the counterpart to `createFateTable()` for `UserFateStore` but sets up ZooKeeper for use in `MetaFateStore`)
- Deleted `UserFateStoreIT`s (now `UserFateStatusEnforcementIT`) method `injectStatus` replacing with the existing `setStatus` which does the same thing
- Ensured sunny day and all FATE tests still pass with these changes

I did not end up using JSON for the fate data. I was not sure what the benefit of this would be since the data needs to be stored in ZK as a byte array anyways and it seemed simpler to me to just keep the same serialization/deserialization strategy for the fate data. Maybe this could be changed to JSON in a follow on if there would be a benefit of using JSON over the current strategy.",2024-12-16T16:06:12Z,kevinrr888,kevinrr888,readable,"Addressed in 29edaa2:

Got rid of END_REPO_DATA marker, replacing with simply writing the number of repos to read
Avoid AtomicBoolean in MetaFateStore.FateTxStoreImpl.push(repo) by changing StackOverflowException to instead be a RuntimeException (previously Exception)
Deleted unnecessary preexisting catch and immediate re-throw of a StackOverflowException in MetaFateStore.FateTxStoreImpl.push(repo)
Cleaned up and refactored MetaFateStore methods which mutate existing FateData; now reuse same pattern across these methods: all call new method MetaFateStore.mutate() | Resolved the conflicts from the recent seed transaction speedup. Thankfully, those changes were mostly for UserFateStore and these are for MetaFateStore, so conflicts were very easy","Single node META FATE data

- Moved all the fate data for a `META` transaction into a single ZK node
	- Pushed all the data into `NodeValue` (renamed to `FateData`)
	- Previously, `FateData` stored `TStatus`, `FateReservation`, and `FateKey`. Now additionally stores the `REPO` stack and `TxInfo`.
- Status enforcement added to `MetaFateStore` (previously only possible for `UserFateStore`).
- Moved `testFateInitialConfigCorrectness()` from `UserFateStoreIT` to `UserFateIT`
- Renamed `UserFateStoreIT` to `UserFateStatusEnforcementIT` (now extends a new class `FateStatusEnforcementIT`)
	- Now only tests status enforcement (previously status enforcement + `testFateInitialConfigCorrectness()`)
- Created `MetaFateStatusEnforcementIT` (extends `FateStatusEnforcementIT`)
	- Tests that the new status enforcement in `MetaFateStore` works
- Created `FateStoreUtil`, moving the `createFateTable()` util here, created MetaFateZKSetup inner class here (the counterpart to `createFateTable()` for `UserFateStore` but sets up ZooKeeper for use in `MetaFateStore`)
- Deleted `UserFateStoreIT`s (now `UserFateStatusEnforcementIT`) method `injectStatus` replacing with the existing `setStatus` which does the same thing | Merge branch 'main' into 4.0-feature-4905 | review changes:

- Got rid of END_REPO_DATA marker, replacing with simply writing the number of repos to read
- Avoid AtomicBoolean in `MetaFateStore.FateTxStoreImpl.push(repo)` by changing StackOverflowException to instead be a RuntimeException (previously Exception)
- Deleted unnecessary preexisting catch and immediate re-throw of a StackOverflowException in `MetaFateStore.FateTxStoreImpl.push(repo)`
- Cleaned up and refactored `MetaFateStore` methods which mutate existing FateData; now reuse same pattern across these methods: all call new method `MetaFateStore.mutate()` | Merge branch 'main' into 4.0-feature-4905"
apache/accumulo,2906,https://github.com/apache/accumulo/pull/2906,convert text to printable string before logging in merge tablets,"I went with `toPrintableString` since that format is pretty readable, but if some other encoding is preferred for some reason, I don't have a preference. Just want to avoid getting vt100 control characters that reconfigure my terminal. ",2022-09-01T17:15:34Z,DomGarguilo,skirklin,readable,"PR fails file formatting validation. You can run mvn clean package -DskipTests locally which will format the file, among other things. | Thanks for the PR @skirklin . It looks like this is your first contribution to this project. If you wish to be added as a contributor to https://accumulo.apache.org/people/ , please open a pull request to add yourself (in alphabetical order) at https://github.com/apache/accumulo-website/edit/main/pages/people.md and leave a reference to apache/accumulo#2906 in your commit log.
If you intend to be a regular contributor to Accumulo projects, please consider subscribing to our developer mailing list (https://accumulo.apache.org/contact-us/) and introducing yourself. 😺",convert text to printable string before logging in merge tablets | pr feedback: start formatter should take start.getLength | autoformat
apache/accumulo,4298,https://github.com/apache/accumulo/pull/4298,Replace boolean with enum for table locks in Elasticity,"Instead of using a boolean value for obtaining table locks, use an enum which makes the code more readable and makes it easier to search and catch errors with the lock type. The enum used already existed in `DistributedReadWriteLock`. This PR is for the same changes to elasticity as in main that is being done in #4297. Because everything conflicts #4297 will need to be merged forwards from main to elasticity as an empty commit and then this can be merged.

This closes #4276",2024-02-26T18:28:19Z,cshannon,cshannon,readable,,"Replace boolean with enum for table locks

Instead of using a boolean value for obtaining table locks, use an enum
which makes the code more readable and makes it easier to search and
catch errors with the lock type.

This closes #4276 | Merge branch 'elasticity' into accumulo-4276-elasticity | Address comments"
apache/accumulo,2928,https://github.com/apache/accumulo/pull/2928,LargeSplitRowIT.automaticSplitLater - increase timeout and other improvements,"This IT has failed a few times due to timeout ([here](https://ci-builds.apache.org/job/Accumulo/job/main/org.apache.accumulo$accumulo-test/382/testReport/junit/org.apache.accumulo.test/LargeSplitRowIT/automaticSplitLater/) and [here](https://ci-builds.apache.org/job/Accumulo/job/main/org.apache.accumulo$accumulo-test/396/testReport/junit/org.apache.accumulo.test/LargeSplitRowIT/automaticSplitLater/)). This PR primarily increases the timeout for the failing IT.

While editing this file i saw a few places for improvements so I also added the following changes to this PR:
* adds resources to try-with-resources blocks
* make use of `Arrays.fill()`
* create and use variables with descriptive names
* make use of `NewTableConfiguration` to set properties where possible
* simplify logic where possible",2022-09-13T19:49:54Z,DomGarguilo,DomGarguilo,readable,"I'm all in favor of simplicity of user new API methods. I have no issue with what you did here, I was just curious if there was a reason you couldn't do the same thing.

in 17c37e7 I opted to use Map.of() in the places mentioned. @dlmarion, Thanks for the review, I'll merge this in once the checks pass.","Increase timeout, other misc improvements | Use Map.of() in more places"
apache/accumulo,2646,https://github.com/apache/accumulo/pull/2646,Replace iterator usage,"I was looking through places where the use of `Scanner.Iterator` might be improved by using the new `Scanner.stream` method and found a few patterns to replace that seemed worth-while. 

* In a lot of places, we create an interator, assert that the iterator `hasNext`, get the next entry, compare that entry to what we expect it to be, then assert that `hasNext` is false. All this to make sure there is just one entry. To simplify I changed these places to `.stream().collect(onlyElement())` and then perform the `assertEquals` on the single returned value.
* Convert `Iterators.size(s.iterator()) == 0` -> `s.stream().findAny().isEmpty()`
* Convert `Iterators.size(s.iterator()) > 0` -> `s.stream().findAny().isPresent()`

I also wanted to see if anyone had an opinion on which of the following equivalent options is better:
1. `scanner.stream().map(Entry::getValue).map(Value::get).map(String::new).map(Integer::parseInt).collect(onlyElement());`
2. `scanner.stream().map(entry -> Integer.parseInt(new String(entry.getValue().get()))).collect(onlyElement());`

I have both in these changes. To me, one does not seem significantly, different/more readable than the other so I thought I would ask.",2022-04-28T13:05:52Z,DomGarguilo,DomGarguilo,readable,,Use streams to simplify iterator usage | Add more short-circuit empty/non-empty checks | Formatting | Use getOnlyElement() where possible | Convert to .map and change order of asserts
apache/accumulo,3544,https://github.com/apache/accumulo/pull/3544,Add secure random to LazySingletons,"A new class, `LazySingletons` was added in #3531 which hosts singleton suppliers. This PR adds a supplier to that class for a `SecureRandom` object and replaces the creation of new `SecureRandom` objects with that supplier.",2023-06-28T14:26:15Z,DomGarguilo,DomGarguilo,readable,"The fact that it's a SecureRandom vs. just a Random is expressed in the type, either way, so I don't think putting the SECURE in the name (or even just S) helps make the calling code any more readable over RNG

I agree. I think that RANDOM and RNG are my top choices for renaming but I'm leaning towards RANDOM just so it is a bit clearer what the variables type is. Also it seems like when we created SecureRandom objects throughout the code, we often use ""random"" as the variable name so it would be more consistent with what is already present. | The fact that it's a SecureRandom vs. just a Random is expressed in the type, either way, so I don't think putting the SECURE in the name (or even just S) helps make the calling code any more readable over RNG

I agree. I think that RANDOM and RNG are my top choices for renaming but I'm leaning towards RANDOM just so it is a bit clearer what the variables type is. Also it seems like when we created SecureRandom objects throughout the code, we often use ""random"" as the variable name so it would be more consistent with what is already present.

I'm fine with RANDOM, but if you wanted to, you could also go with SRANDOM, just to make it super clear. While the local uses may have not cared what type of random it was (they were probably just using SecureRandom to avoid a spotbugs warning), we have to also consider how the LazySingletons class presents itself, even when the callers don't necessarily care.","Add SecureRandom to LazySingletons | Replace AccumuloITBase SecureRandom | Remove SecureRandom from AccumuloITBase and fix unrelated comment | Update core/src/main/java/org/apache/accumulo/core/util/LazySingletons.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Rename to 'RANDOM'"
apache/accumulo,3225,https://github.com/apache/accumulo/pull/3225,replace sleepUninterruptibly() with Thread.sleep() in tests,"This is a subset (and supersedes) https://github.com/apache/accumulo/pull/3202 and focuses on replacements only in test code.

- replace occurrences of sleepUninterruptibly with Thread.sleep()
- prefer static import of TimeUnits instead of using TimeUnit.[UNIT] in files that had changes.
",2023-03-07T13:39:16Z,EdColeman,EdColeman,readable,,replace sleepUninterruptibly in tests | address PR comments
apache/accumulo,3038,https://github.com/apache/accumulo/pull/3038,Trivial improvements to block cache code,"Make trivial improvements seen while investigating #2697

* Get rid of unnecessary generic method in internal implementation
* Drop redundant static modifier on interface
* Add explicit private constructor for BlockIndex",2022-10-24T20:01:48Z,ctubbsii,ctubbsii,readable,"Does this fix a bug in 2.1.0? If not, I'd rather we punt this to a bugfix release and get RC1 underway. | Does this fix a bug in 2.1.0? If not, I'd rather we punt this to a bugfix release and get RC1 underway.

It does not fix a bug. It just makes a few lines very slightly more readable and easier to follow. Merging it won't block an RC1, though. I have another patch to improve the assembly process itself, that I'll be merging before RC1 anyway. I don't care if we include this or not, but it's trivial.","Trivial improvements to block cache code

Make trivial improvements seen while investigating #2697

* Get rid of unnecessary generic method in internal implementation
* Drop redundant static modifier on interface
* Add explicit private constructor for BlockIndex"
apache/accumulo,4561,https://github.com/apache/accumulo/pull/4561,Improve ThriftTransportPool shutdown speed,"Wake the ""Thrift Connection Pool Checker"" thread from sleeping during shutdown, so that it is joined quicker.",2024-05-20T15:38:14Z,dlmarion,findepi,readable,"I think this change could target the 2.1 branch so that it gets included in the next 2.1 patch release. | I think this change could target the 2.1 branch so that it gets included in the next 2.1 patch release.

thanks, will rebase | @findepi  - we have a code formatter that runs as part of our build, and we verify that here, which is currently failing. Running mvn compile should fix the source. | I kicked off a full IT build for this branch. | Full IT build passed successfully","Improve ThriftTransportPool shutdown speed

Wake the ""Thrift Connection Pool Checker"" thread from sleeping during
shutdown, so that it is joined quicker."
apache/accumulo,2333,https://github.com/apache/accumulo/pull/2333,Modify shutdown test in ManagerApiIT,"* Closes #2327
* Drop the second RPC call in the last test of the ManagerApiIT
* ThriftTransportPool is now a part of ClientContext and only allows for
a single remaining connection after shutting down",2021-11-01T19:24:30Z,milleruntime,milleruntime,readable,"Actually, I think I figured out a way to do both calls in the test after shutting down. Putting up a change shortly. | The IT is passing with my last changes:
[INFO] Running org.apache.accumulo.test.functional.ManagerApiIT
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.635 s - in org.apache.accumulo.test.functional.ManagerApiIT | The IT is passing with my last changes:

I can confirm the test is passing now.","Drop second RPC call after shutdown in ManagerApiIT

* Closes #2327
* Drop the second RPC call in the last test of the ManagerApiIT
* ThriftTransportPool is now a part of ClientContext and only allows for
a single remaining connection after shutting down | Rework to create connections before shutting down | Use try-with-resources"
apache/accumulo,4348,https://github.com/apache/accumulo/pull/4348,Throw error when non-standard chars exist,"Adds an explicit error message when base64 encoding should have been used. 
Removes special character modification in favor of the user piping the output to `base64 -d`

closes: 4347",2024-03-12T15:12:53Z,ddanielr,ddanielr,readable,,"Throw error when non-standard chars exist

Adds an explicit error message when base64 coding should have been used.
Adds override flag to generate human-readable output. | Update error message with solution | Removes char modifications and arg flag

Removes the new ""human-readable"" flag in favor of user using `base64
-d`.

Removes specical handling of `\` character and replaces it with an
allowed character set. | Update core/src/main/java/org/apache/accumulo/core/file/rfile/GenerateSplits.java

Co-authored-by: Keith Turner <kturner@apache.org>"
apache/accumulo,2549,https://github.com/apache/accumulo/pull/2549,Migrate Minicluster module to JUnit5,Part of issue #2441 This PR contains changes for the conversion from JUnit4 to JUnit5 for the accumulo-minicluster module.,2022-03-09T17:25:11Z,DomGarguilo,AlbertWhitlock,readable,LGTM. I have verified all tests pass.,Minicluster JUnit5 conversion | Minicluster JUnit5 conversion updates | Minicluster JUnit5 conversion cleanup
apache/accumulo,3582,https://github.com/apache/accumulo/pull/3582,adds logging when tablet close is waiting on a scan,this is related to #3512,2023-07-12T16:58:06Z,keith-turner,keith-turner,readable,,"adds logging when tablet close is waiting on a scan

this is related to #3512 | code review update | Update server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/ScanDataSource.java

Co-authored-by: Dom G. <domgarguilo@apache.org> | fixe build"
apache/accumulo,1851,https://github.com/apache/accumulo/pull/1851,fixes #1320 adds a stress test for ZooReaderWriter.mutateOrCreate(),,2021-01-06T20:29:07Z,keith-turner,keith-turner,readable,"I'm having a hard time following what the test is actually doing. Can you add more explanation inline about how it's supposed to work?

@ctubbsii I added docs that give a high level overview and cover intent. Did this in 97967bf","fixes #1320 adds a stress test for ZooReaderWriter.mutateOrCreate() | Update test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update test/src/main/java/org/apache/accumulo/test/functional/ZooMutatorIT.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | make test compile | use futures | hash another way | removed printlns | added some javadocs | use guava hashing | update test documentation | add more docs"
apache/accumulo,1861,https://github.com/apache/accumulo/pull/1861,Fixes #1665 use old compaction thread pool config if set,,2021-01-12T18:42:40Z,keith-turner,keith-turner,readable,"I tested this change manually using Uno.  In one test I set only the the old deprecated property and inspected the logs to see that the expected warning was logged and the an executor w/ the expected number of threads was created.  In another test I set the old deprecated prop and also modified the new prop.  For that test I could see in the logs that it logged a warning, but did not use the old prop. | I manually retested after making code review changes and everything worked as expected.",Fixes #1665 use old compaction thread pool config if set | Code review updates
apache/accumulo,2987,https://github.com/apache/accumulo/pull/2987,Fixes for new Admin fate command,* Closes #2974,2022-09-30T19:16:18Z,milleruntime,milleruntime,readable,Another advantage to have the fate command run in Admin is that it doesn't require Accumulo to be running. So an admin can shut the cluster down and perform actions using the new command. The only exception is cancel requires the Manager to be running.,"Fixes for new Admin fate command

* Closes #2974 | Make Admin fate conform to Git command usage guidelines"
apache/accumulo,3583,https://github.com/apache/accumulo/pull/3583,Cache existence of tablet directories in CompactionCoordinator,"Cache existence of tablet directories to avoid checking to see if they exist, which can be expensive.

Fixes #3475",2023-07-19T11:21:33Z,dlmarion,dlmarion,readable,"Were you able to test this in any way? I suspect some of the volume related ITs may exercise the dir creation aspect of this, but not sure. Depends on if we have a multi volume IT that compacts data.

No, I basically copied over the code from Tablet.java. I don't see any ITs that test this condition. It also looks like CompactionCoordinatorTest got nuked. Looking at the comment in that file, we might need a ticket to re-evaluate that","Cache existence of tablet directories in CompactionCoordinator

Cache existence of tablet directories to avoid checking to
see if they exist, which can be expensive.

Fixes #3475 | Fix incorrect check | Merge branch 'elasticity' into 3475-cache-check-tablet-dir | Modified Cache to use Weigher to keep 10MB of entries | Removed static modifier from added code"
apache/accumulo,1918,https://github.com/apache/accumulo/pull/1918,Drop debug logs to trace in ZooLock,* Seeing ~300 logs per second per tserver with just these few debug statements,2021-02-10T15:11:15Z,milleruntime,milleruntime,readable,"I am working to convert ZooLockIT's use of MAC to ZooKeeperTestingServer (via #1885 ) and am seeing the same thing with the log:
2021-02-26T16:23:31,422 [zookeeper.ZooLock] DEBUG: [zlock#81282aef-d8a3-4bf1-a376-23a31e1d7b12#] Renewing watch on prior node  /zltestDeleteWaiting-1025168844-l1/zlock#eadf5a4c-680e-411f-92b5-62a55c615031#0000000000
At what looks like about 100 times per second. | I am working to convert ZooLockIT's use of MAC to ZooKeeperTestingServer (via #1885 ) and am seeing the same thing with the log:

@DomGarguilo It's probably better to track that in #1885 or a new issue, since this work is complete and the changes in this PR did not cause the issue you are seeing. It is a different log message, unrelated to these changes.","Drop debug logs to trace in ZooLock

* Was seeing ~300 logs per second per tserver just on debug | Update core/src/main/java/org/apache/accumulo/fate/zookeeper/ZooLock.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org>"
apache/accumulo,4049,https://github.com/apache/accumulo/pull/4049,Add a new AccumuloStore for FATE,"This adds support for storing FATE data in an Accumulo table instead of storing in Zookeeper for #3559

This is currently a work in progress and is not finished but the main operations are implemented for the `AcccumuloStore`. Common code has been refactored and moved out of `ZooStore` and into `AbstractFateStore`. Most of the common code moved has to do with the code that handles signaling and tracking reserved/defered transactions. The store just uses a batch writer for now but we will likely need to use conditional mutations at some point.

There is a test called `AccumuloStoreReadWriteIT` which tests out the various operations on the new store. The test only tests read/writing to the store...the store hasn't been tested with the overall FATE framework yet as some more work probably needs to be done to test it for real (such as handling which store to use so we can still use the ZK store for root/metadata). 

The next steps (besides addressing any review comments) are to try testing it for real with FATE (and finish up any supporting tasks for that) and then eventually support conditional mutations.",2023-12-22T12:50:21Z,cshannon,cshannon,readable,"@keith-turner - I did some refactoring and now FateIT is abstract and there are test classes to run the 4 tests for both Zookeeper and Accumulo stores. One thing I noticed was one of the tests seems to have a race condition that I think needs fixing as I saw an occasional failure with the accumuo store version and i added a comment here.
This should be close to being able to go out of draft status if we are ok doing some follow ons for things like conditional mutations.","Add a new AccumuloStore for FATE data

This supports storing FATE data in an Accumulo table intead of storing in
Zookeeper | fix warning | Merge branch 'elasticity' into accumulo-3559 | Use getRow() for range

Co-authored-by: Keith Turner <kturner@apache.org> | Address comments and invert repo position to store in reverse order | Add range validation on repos and set batch size to 1 for repo scans when finding top | Move FateIT | Refactor FateIT to be used for Accumulo and ZK store | Merge branch 'elasticity' into accumulo-3559 | Merge branch 'elasticity' into accumulo-3559 | code cleanup"
apache/accumulo,3255,https://github.com/apache/accumulo/pull/3255,Convert Bulk Import metadata actions to use Ample,"Refactor's the Bulk Import metadata operations from the MetadataTableUtil class and migrate them to use Ample instead. 

This is a part of #3208. ",2023-03-30T22:27:07Z,keith-turner,ddanielr,readable,,"Convert BulkImportInProgressFlag methods to Ample

Refactors the addBulkImportInProgressFlag and removeBulkImportInProgressFlag
methods and moves them to ServerAmpleImpl from the MetadataTableUtil class. | Refactor removeBulkLoadEntries metadata actions

Moves removeBulkLoadEntries from the MetadataTableUtil class to
ServerAmpleImpl to centralize all metadata modification code to Ample. | Add javadoc comments

Add javadoc comments on Ample interface for new methods. | Switch exception type in default method definition

Switch the default expection type from RunTime to UnsupportedOperation
for the default method definitions in the Ample interface.

Refactor exception handling in `removeBulkLoadEntries` to throw
RuntimeExceptions instead. | Implement PR review changes

Switched metadataActions to ample for style/readability reasons.
Changed BatchWriter variable creations to use
context.createBatchWriter() for better readability and cleaner logic. | Removes private createWriter method

Removes the private createWriter method in favor of just calling
context directly with the datalevel. | Add table checks and changed table comparisons

Added precondition check for user tables and switched root table
comparision to use Datalevel vs direct ID. | fix build issue"
apache/accumulo,1980,https://github.com/apache/accumulo/pull/1980,Add created column to Fate Print command,"From https://issues.apache.org/jira/browse/ACCUMULO-4161. An age/created column was added to the `fate print ` command in the shell. 

Image below is what the timestamp looks like in its current state. 

![image](https://user-images.githubusercontent.com/29436247/112877128-617be080-9094-11eb-943c-4ca3f3ca853c.png)

",2021-03-31T17:36:10Z,Manno15,Manno15,readable,"From a quick look, some general things you may want to consider:

Format the printed time in UTC with ISO-8601 format instead of java Date default.
Provide more descriptive naming in functions instead of timestamp, maybe topCreated() or timeTopCreated()?

The rational for a more descriptive function name than getTimestamp - there are multiple timestamps important in overall FATE transaction life cycle, some of which might be useful if additional FATE timing information was ever extended. If the function naming reflected the operation from an ""external"" perspective then it may be easier differentiate what time this is retrieving (FATE top creation) rather than some of the other possibilities. | Good ideas, especially on the naming convention. I wasn't sure on the time zone part which is why I stuck with the default. I will look into making that change. | Format the printed time in UTC with ISO-8601 format instead of java Date default.

The original ticket specified a more human-readable print statement. My latest commit does that. What are your thoughts on this @EdColeman? I did keep it in the same order as expect with iso_8601 (I believe). Picture below: | @EdColeman I think this is finalized for review again. Is there anything else you recommend I fix? | I went ahead and added a format function people can use and also one that just returns the long value for it.","add timestamp to fate print command | rename functions to be more descriptive | format print to be human readable | iso format for print message | return ERROR for timestamp when exception occurs | rename function to something less confusing | change from String to long, create format function"
apache/accumulo,3109,https://github.com/apache/accumulo/pull/3109,"Check ACLs on ZNode contain expected regardless of order, improve error msg",,2022-12-16T17:54:43Z,dlmarion,dlmarion,readable,"Related to #3108 | User tested this patch when upgrading from 1.10 to 2.1 to help identify ZK nodes with unexpected ACLs. This patch identified 4 ZK nodes (system config node, and 3 user nodes). User manually set the ACLs on these nodes to what was expected, then upgrade worked successfully.","Check ACLs on ZNode contain expected regardless of order, improve error msg | Modify validateACLs to test ZK node ACLs length and contents | Merge branch '2.1' into fix-2.1-zoo-upgrader-issue | Rework upgrade acl validation to just ensure that accumulo can write to each node | Merge branch '2.1' into fix-2.1-zoo-upgrader-issue | Addressed PR comments | Updates from PR feedback | Change method name"
apache/accumulo,3549,https://github.com/apache/accumulo/pull/3549,Optimize GC getReferences iteration,"* Creates TabletFile path with URI, which avoids a regex validation.
* Updates validateFileName to check explicit characters.
* Unit test for ValidationUtil.

Fixes #3499

-- 

The following are pre/post benchmarks for updates:

```
Accumulo 2.1.2-SNAPSHOT (current):
Benchmark                                                   (splitsRfile)  Mode  Cnt    Score     Error  Units
GarbageCollectorPerformanceIT.benchmarkReferences                   1,100  avgt    3    1.665 ±   0.211  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op          1,100  avgt    3    0.017 ±   0.002  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                  10,100  avgt    3   10.139 ±  16.299  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op         10,100  avgt    3    0.010 ±   0.016  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                 100,100  avgt    3   62.035 ±  24.129  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op        100,100  avgt    3    0.006 ±   0.002  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                1000,100  avgt    3  521.994 ± 228.079  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op       1000,100  avgt    3    0.005 ±   0.002  ms/op


Accumulo 2.1.2-SNAPSHOT (PR changes):
Benchmark                                                   (splitsRfile)  Mode  Cnt    Score    Error  Units
GarbageCollectorPerformanceIT.benchmarkReferences                   1,100  avgt    3    1.374 ±  0.160  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op          1,100  avgt    3    0.014 ±  0.002  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                  10,100  avgt    3    6.562 ±  2.008  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op         10,100  avgt    3    0.007 ±  0.002  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                 100,100  avgt    3   38.382 ± 10.472  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op        100,100  avgt    3    0.004 ±  0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                1000,100  avgt    3  233.692 ± 14.204  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op       1000,100  avgt    3    0.002 ±  0.001  ms/op
```",2023-06-30T11:59:09Z,dlmarion,dtspence,readable,"@dtspence I fixed the trivial JUnit5 imports so we could get the build checks going, but I'll leave my substantive suggestions there for your consideration/further discussion. | @dtspence I fixed the trivial JUnit5 imports so we could get the build checks going, but I'll leave my substantive suggestions there for your consideration/further discussion.

Thank you for updating the JUnit5 imports. | For what it's worth, I got substantially wider error bars a few times when I ran the gc-test-update branch (at commit b27d9fa56b68e7ab58a9e6c4d9b894ae35724c80) locally. Each run took about 10 minutes (using mvn clean verify -Dit.test=GarbageCollectorPerformanceIT -Dversion.accumulo=2.1.2-SNAPSHOT), and all were very spammy with regard to hadoop log messages about decompressors.
Click each to expand/collapse details.

Current 2.1.2-SNAPSHOT at 544ef01:
Benchmark                                                   (splitsRfile)  Mode  Cnt    Score    Error  Units
GarbageCollectorPerformanceIT.benchmarkReferences                   1,100  avgt    3    0.892 ±  0.181  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op          1,100  avgt    3    0.009 ±  0.002  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                  10,100  avgt    3    6.250 ±  2.679  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op         10,100  avgt    3    0.006 ±  0.003  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                 100,100  avgt    3   39.701 ±  2.739  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op        100,100  avgt    3    0.004 ±  0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                1000,100  avgt    3  314.899 ± 34.392  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op       1000,100  avgt    3    0.003 ±  0.001  ms/op



Current PR merged onto 2.1.2-SNAPSHOT at 544ef01:
Benchmark                                                   (splitsRfile)  Mode  Cnt    Score     Error  Units
GarbageCollectorPerformanceIT.benchmarkReferences                   1,100  avgt    3    0.812 ±   0.150  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op          1,100  avgt    3    0.008 ±   0.002  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                  10,100  avgt    3    4.501 ±   2.058  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op         10,100  avgt    3    0.005 ±   0.002  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                 100,100  avgt    3   28.047 ±  12.950  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op        100,100  avgt    3    0.003 ±   0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                1000,100  avgt    3  176.200 ± 289.206  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op       1000,100  avgt    3    0.002 ±   0.003  ms/op



Slight change to move the IntPredicate outside the method to a private static final variable at the top of the file:
Benchmark                                                   (splitsRfile)  Mode  Cnt    Score     Error  Units
GarbageCollectorPerformanceIT.benchmarkReferences                   1,100  avgt    3    0.710 ±   0.083  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op          1,100  avgt    3    0.007 ±   0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                  10,100  avgt    3    4.269 ±   1.178  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op         10,100  avgt    3    0.004 ±   0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                 100,100  avgt    3   25.945 ±   6.227  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op        100,100  avgt    3    0.003 ±   0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                1000,100  avgt    3  177.723 ± 427.999  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op       1000,100  avgt    3    0.002 ±   0.004  ms/op



Ran that same one again, because the error bars looked crazy to me (`±428` is crazy for a value of 178... there must've been some kind of outlier due to some other activity on the box):
Benchmark                                                   (splitsRfile)  Mode  Cnt    Score    Error  Units
GarbageCollectorPerformanceIT.benchmarkReferences                   1,100  avgt    3    0.706 ±  0.186  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op          1,100  avgt    3    0.007 ±  0.002  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                  10,100  avgt    3    4.315 ±  1.793  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op         10,100  avgt    3    0.004 ±  0.002  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                 100,100  avgt    3   26.623 ± 12.256  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op        100,100  avgt    3    0.003 ±  0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                1000,100  avgt    3  168.236 ± 47.627  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op       1000,100  avgt    3    0.002 ±  0.001  ms/op



Second slight change to use for loop with indexed pointer, keeping the predicate and calling `predicate.test(ch)`:
Benchmark                                                   (splitsRfile)  Mode  Cnt    Score    Error  Units
GarbageCollectorPerformanceIT.benchmarkReferences                   1,100  avgt    3    0.713 ±  0.089  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op          1,100  avgt    3    0.007 ±  0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                  10,100  avgt    3    4.440 ±  1.746  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op         10,100  avgt    3    0.004 ±  0.002  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                 100,100  avgt    3   27.450 ± 13.024  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op        100,100  avgt    3    0.003 ±  0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                1000,100  avgt    3  165.311 ± 68.361  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op       1000,100  avgt    3    0.002 ±  0.001  ms/op



Third slight change to use for loop with indexed pointer, and getting rid of the predicate in favor of inlining the checks into the `if` statement's condition:
Benchmark                                                   (splitsRfile)  Mode  Cnt    Score    Error  Units
GarbageCollectorPerformanceIT.benchmarkReferences                   1,100  avgt    3    0.714 ±  0.019  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op          1,100  avgt    3    0.007 ±  0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                  10,100  avgt    3    4.318 ±  0.779  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op         10,100  avgt    3    0.004 ±  0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                 100,100  avgt    3   27.488 ±  9.656  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op        100,100  avgt    3    0.003 ±  0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                1000,100  avgt    3  166.778 ± 60.033  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op       1000,100  avgt    3    0.002 ±  0.001  ms/op



Fourth slight change to cover case where the string is empty (this case was initially omitted from this PR, even though the regex checked for it):
Benchmark                                                   (splitsRfile)  Mode  Cnt    Score     Error  Units
GarbageCollectorPerformanceIT.benchmarkReferences                   1,100  avgt    3    0.788 ±   0.658  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op          1,100  avgt    3    0.008 ±   0.007  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                  10,100  avgt    3    4.453 ±   2.814  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op         10,100  avgt    3    0.004 ±   0.003  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                 100,100  avgt    3   27.361 ±  12.971  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op        100,100  avgt    3    0.003 ±   0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                1000,100  avgt    3  168.248 ± 105.847  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op       1000,100  avgt    3    0.002 ±   0.001  ms/op



Instead of doing the change in this PR, merely using a precompiled Pattern rather than calling `String.matches` results in:
Benchmark                                                   (splitsRfile)  Mode  Cnt    Score    Error  Units
GarbageCollectorPerformanceIT.benchmarkReferences                   1,100  avgt    3    0.875 ±  0.162  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op          1,100  avgt    3    0.009 ±  0.002  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                  10,100  avgt    3    6.026 ±  0.598  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op         10,100  avgt    3    0.006 ±  0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                 100,100  avgt    3   38.700 ±  7.029  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op        100,100  avgt    3    0.004 ±  0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                1000,100  avgt    3  292.542 ± 44.951  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op       1000,100  avgt    3    0.003 ±  0.001  ms/op



Using the precompiled Pattern, I forgot to include the change to StoredTabletFile to use the URI-based constructor for Path. It seems that's a big part of the improvements here. If I include that change, using the precompiled Pattern results in:
Benchmark                                                   (splitsRfile)  Mode  Cnt    Score    Error  Units
GarbageCollectorPerformanceIT.benchmarkReferences                   1,100  avgt    3    0.734 ±  0.148  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op          1,100  avgt    3    0.007 ±  0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                  10,100  avgt    3    4.509 ±  0.249  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op         10,100  avgt    3    0.005 ±  0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                 100,100  avgt    3   27.809 ±  8.549  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op        100,100  avgt    3    0.003 ±  0.001  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences                1000,100  avgt    3  176.476 ± 15.462  ms/op
GarbageCollectorPerformanceIT.benchmarkReferences:rfile/op       1000,100  avgt    3    0.002 ±  0.001  ms/op


So, these lower numbers look promising, but the outliers that widen the error bars make me skeptical of how consistent these are, as they seem to show some sensitivity to the environment, even though I wasn't actively using my machine for much else while they ran.
My conclusions (changed after I saw the impact of the use of URI-based Path constructor):

the for loop does not matter over the codePoints IntStream, so the current IntStream is fine
the predicate object does not matter over inlining the if condition into the method, so the current use of the predicate is fine
moving the predicate out of the method doesn't hurt or help, but is better code structure
adding coverage for the empty string case is needed
the regex pattern is just as good as using the custom predicate, and maybe slightly better (the error bars were certainly the narrowest with that run, leading to more confidence that it's going to perform better consistently)

I'm going to open a separate PR for precompiling patterns in the couple of places we use them, and then I think this PR should be re-evaluated after that. At the very least, after that PR is merged in, we'll still want to keep the URI-based Path constructor in StoredTabletFile, and the ValidationUtilTest coverage. | I moved the new unit test to the correct package and pushed it to 2.1 in 2ec2321 so it can provide test coverage, regardless of what we want to do here.
I updated the title of this PR since it's no longer about removing regexes, which perform similarly to the alternatives previously proposed in this PR according to the benchmarks, once the Pattern is compiled once, instead of each time it is evaluated. Of course, you can still update this PR to remove the regex now and replace it with one of the alternatives, but I don't think it's necessary anymore, and I dropped it when I resolved this PR's merge conflicts with the 2.1 branch after #3554 was merged.
Based on the benchmarking, the main optimization that was included here was the use of the URI-based constructor for Path. So now, after resolving merge conflicts, that's the only thing left in this PR. I would have already merged that in, as well, but I am not sure about the TabletFile changes in 3.0, and this doesn't merge forward cleanly, since the TabletFile code changed. I'm not sure where this equivalent code moved to, if it even still applies to the main brnch. I don't want to merge it in to 2.1 until there's a plan for merging it forward into the main branch, even if that plan is to just make the change in 2.1, and do benchmarking and further changes separately for the main branch. | @ctubbsii - For 3.0, I believe it's AbstractTabletFile and any descendents that create a Path without calling URI.create. @cshannon  - any thoughts on what changes need to be made in the merge to 3.0 for this change? | @ctubbsii - For 3.0, I believe it's AbstractTabletFile and any descendents that create a Path without calling URI.create. @cshannon - any thoughts on what changes need to be made in the merge to 3.0 for this change?

I will take care of merging into main when this is in 2.1 | So this actually merged pretty easily as the creation of the Path is still in StoredTabletFile and wasn't moved to AbstractTabletFile.
The only thing I noticed when going through the different implementations to check for Path creation was that ScanServerRefTabletFile creates a Path as well so I am wondering if this change should be made there too. | I should add the harder place to merge is in the no chop merge stuff since that redoes metadata to be Json but I will also handle that of course. | The only thing I noticed when going through the different implementations to check for Path creation was that ScanServerRefTabletFile creates a Path as well so I am wondering if this change should be made there too.

Can you create a ticket for this in 2.1.2?","Avoid regex evaluation during GC getReferences iteration (#3499)

* Creates TabletFile path with URI, which avoids a regex validation.
* Updates validateFileName to check explicit characters.
* Unit test for ValidationUtil.

Fixes #3499 | Update core/src/test/java/org/apache/accumulo/core/util/ValidationUtilTest.java | Fix JUnit5 assertions | Update ValidationUtilTest to contain parameter test and update pom.xml for junit dependency | Update validateFileName to match with predicate | Merge branch '2.1' into accumulo-3499-gc-update | Add empty string case | Merge branch '2.1' into accumulo-3499-gc-update"
apache/accumulo,3056,https://github.com/apache/accumulo/pull/3056,Properly format timestamps in Accumulo Monitor,"Fixes the issue in #3048 where the `dateFormat()` function has a bug and returns the original date and not a formatted timestamp.

~~An example of the new format is: `10/28/2022, 11:49:29 AM EDT`~~

Update: I switched to use a modified version of the toISOString() method so the date is YYYY-MM-DD based on comments.

Updated sample of the new format: `2022-10-28 18:51:36,072Z`",2022-11-01T14:28:11Z,cshannon,cshannon,readable,"@ctubbsii - Should we hold off merging this until 2.1.0 is done? It looks like the branch is still set to 2.1.0-SNAPSHOT and not 2.1.1-SNAPSHOT | @ctubbsii - Should we hold off merging this until 2.1.0 is done? It looks like the branch is still set to 2.1.0-SNAPSHOT and not 2.1.1-SNAPSHOT

Things can be merged into the 2.1 branch, but they generally should be limited to bug fixes (semver rules). If we need another release candidate, they will make it into 2.1.0. If not, they'll make it into 2.1.1. The current SNAPSHOT version doesn't matter. We'll update those after the release is finished, as needed.
However, see my note above. I am against this change. | I will again toss out that there is benifits of matching the timestamp format of the logs 2022-10-28 17:13:50,493 - yes, that can be changed by configs, but by default if we are consistent then it eases things.
An example would be grab a timestamp from the monitor and then grep a log - much easier to cut-n-paste than needing to convert and type.
But anything that makes the timestamp shorter is moving in the right direction. | I would strongly prefer not having MM/dd/YYYY format. It is very confusing to me, having a military background, and living overseas for a bit. YYYYMMdd is better, because it's sortable, and europeans use dd/MM/YYYY, which is often indistinguishable from MM/dd/YYYY, enough to cause confusion.
The human-readable long format is better in that regard... no ambiguity or confusion. Something like dd-MMM-YYYY might also be okay (e.g. 04-JUL-1776), but please avoid anything that results in MM/dd/YYYY.

We may have to go ahead and use the toISOString() method and then parse and customize that a bit. Using toLocaleString() doesn't let you change the year to be first, which makes sense, as the it's supposed to format for your locality. As you mentioned it would show up different for Europeans.
I could just show something like 2011-10-05 14:48:00.000Z which is what @EdColeman had mentioned. If we wanted to keep local timezone and AM/PM then we'd need to do more work or find another library as I don't think native Javascript has a lot of options out of the box for formatting timestamps so just using ISO in this case probably makes the most sense. | @ctubbsii and @EdColeman , My latest commit updates things so it's YYYY-MM-DD and Z since I used the toISOString() format and that's in Z, new sample screen shot:

Let me know what you think, I can always write something custom to tweak things more because as I said out of the box there's not a lot of formatting options in JavaScript (Java has a lot more customization options with its formatter naively) | I reviewed these changes. They look good. Displaying as: | @ctubbsii  - So there is actually one bug here either way. The current code is not actually using the  toLocaleString(). It calls that method but it returns the original date so what you are seeing is actually the format of the toString() method. I do agree that I liked having the locality better (your own time zone).
If we fixed the bug and used the toLocaleString() method for real it would show up as:
10/28/2022, 6:45:19 PM
If you like the current format then I can just fix the bug and remove the unnecessary call to toLocaleString() and then maybe drop the full time zone name as I still think that we should at least drop ""Eastern Daylight Time"" as it's so verbose.
New Proposed format:
Fri Oct 28 2022 18:47:02 GMT-0400
How does that sound? | I do prefer your proposed format. However, I'm also thinking it's probably best to just stick with the default locale string, even though I don't personally like it. It seems to be the option that gives the most deference to the user's environment/configuration. I hadn't realized that was an actual bug. I thought this was just about style preferences. My main concern is that it doesn't show the time zone (neither do the logs, apparently, since the version of ISO8601 log4j is printing doesn't seem to include it, so maybe it's fine). | Alright, tomorrow I can fix it to use toLocaleString() like it was supposed to be all along. I will also do some research/testing and see if we can customize the output at all. I think the function takes some options that we can set so we may be able to customize the output to show the time zone, but not sure until I try.
If we can customize it I can give some example outputs and pick the one everyone likes the best otherwise I'll just use what's given if we can't. | @ctubbsii - There are actually quite a few ways we can customize toLocaleString(). Documentation is located here for toLocaleString()  and the options for customization supported are the same as Intl.DateTimeFormat
This list is not exhaustive of course but I figure one of these should accomplish the goal of being more concise but still containing all the info we want (date, time, timezone, etc)
Here are several examples of options and outputs, what do you think of them? Any of these seem fine to me. Do we want to include the day of the week? Also do you prefer the timezone as an offset or the actual name? (ie EDT or GMT-4).
date.toLocaleString([], { timeStyle: 'long', dateStyle: 'medium' })
Output: Oct 29, 2022, 10:08:11 AM EDT

date.toLocaleString([], { timeStyle: 'long', dateStyle: 'long' })
Output: October 29, 2022 at 10:04:00 AM EDT

date.toLocaleString([], { weekday: 'short', month: 'short', day: 'numeric', year: 'numeric', 
   hour: 'numeric', minute: 'numeric', second: 'numeric', timeZoneName: 'shortOffset' })
Output: Sat, Oct 29, 2022, 10:08:11 AM GMT-4

date.toLocaleString([], { weekday: 'short', month: 'short', day: 'numeric', year: 'numeric', 
  hour: 'numeric', minute: 'numeric', second: 'numeric', timeZoneName: 'short' })
Output: Sat, Oct 29, 2022, 10:16:28 AM EDT

date.toLocaleString([], { weekday: 'short', month: 'numeric', day: 'numeric', year: 'numeric', 
  hour: 'numeric', minute: 'numeric', second: 'numeric', timeZoneName: 'short' })
Output: Sat, 10/29/2022, 10:16:28 AM EDT

date.toLocaleString([], { month: 'numeric', day: 'numeric', year: 'numeric', 
  hour: 'numeric', minute: 'numeric', second: 'numeric', timeZoneName: 'short' })
Output: 10/29/2022, 10:17:41 AM EDT

date.toLocaleString([], { weekday: 'short', month: '2-digit', day: '2-digit', year: '2-digit', 
  hour: 'numeric', minute: 'numeric', second: 'numeric', timeZoneName: 'shortOffset' })
Output: Sat, 10/29/22, 10:17:41 AM GMT-4 | date.toLocaleString([], { timeStyle: 'long', dateStyle: 'medium' })
Output: Oct 29, 2022, 10:08:11 AM EDT

Out of the ones listed, I'd put my vote on something like this. Clear on month and day to avoid any possible confusion but also short and concise. EDT over GMT +/- though not sure how useful timezone is in general for these logs. | date.toLocaleString([], { timeStyle: 'long', dateStyle: 'medium' })
Output: Oct 29, 2022, 10:08:11 AM EDT

Out of the ones listed, I'd put my vote on something like this. Clear on month and day to avoid any possible confusion but also short and concise. EDT over GMT +/- though not sure how useful timezone is in general for these logs.

I agree with this. | date.toLocaleString([], { timeStyle: 'long', dateStyle: 'medium' })
Output: Oct 29, 2022, 10:08:11 AM EDT

Out of the ones listed, I'd put my vote on something like this. Clear on month and day to avoid any possible confusion but also short and concise. EDT over GMT +/- though not sure how useful timezone is in general for these logs.

I agree with this.

I'll update the PR with this later today or tomorrow. | I pushed up the latest change with the new format. Screenshot of the latest:","Properly format timestamps in Accumulo Monitor

Fixes the dateFormat() function to return a formatted timestamp and not
the original date.

This closes #3048 | Change format of date to be close to ISO 8601 so year is first | Rework slightly to bring back use of join method instead of string concat | Merge remote-tracking branch 'accumulo/2.1' into accumulo-3055 | Update monitor dateFormat function to return date formatted with toLocaleString

Fixes bug where generated toLocaleString() was not used and also makes
timestamps more concise"
apache/accumulo,2912,https://github.com/apache/accumulo/pull/2912,Updates to ScanAttempts,"As a follow-up to #2880:

* Remove unused and undocumented endTime in ScanAttempt API
* Rename ScanAttemptImpl method and variables from ""mutationCounter"" to
  ""attemptNumber"" with corresponding comments to make it clear how this
  is used for the snapshot
* Set attemptNumber in ScanAttemptImpl constructor as an immutable
  value, rather than setting it immediately after construction
* Remove unnecessary synchronization and use an AtomicLong for the
  currentAttemptNumber
* IDE changes also removed some unneeded keywords in the interface and
  converted an anonymous inner class to a lambda
* Create the snapshot using Java streams instead of using ""forEach()""",2022-10-18T18:08:53Z,dlmarion,ctubbsii,readable,"I'll need to rebase this on the changes done in #2896 | The reason that the mutationCount/attemptNumber existed was because ScanAttemptsImpl used to use guava methods that filtered a concurrent hash map in place. If the map is being copied instead of filtered in place, I think mutationCount/attemptNumber could just be dropped.
I was trying to make multiple comments across the diff, but this just got too complicated for a small diff. So I copied the new changes and modified them in a text editor to illustrate what I was thinking. Below is an example of dropping mutationCount/attemptNumber.
public class ScanAttemptsImpl {

  private static final Logger LOG = LoggerFactory.getLogger(ScanAttemptsImpl.class);

  static class ScanAttemptImpl implements ScanAttempt {

    private final String server;
    private final Result result;

    private ScanAttemptImpl(Result result, String server) {
      this.result = result;
      this.server = Objects.requireNonNull(server);
    }

    @Override
    public String getServer() {
      return server;
    }

    @Override
    public Result getResult() {
      return result;
    }
  }

  private final Map<TabletId,Collection<ScanAttemptImpl>> attempts = new HashMap<>();

  public interface ScanAttemptReporter {
    void report(ScanAttempt.Result result);
  }

  ScanAttemptReporter createReporter(String server, TabletId tablet) {
    return result -> {
      LOG.trace(""Received result: {}"", result);
      synchronized(ScanAttemptImpl.this){
        attempts.computeIfAbsent(tablet, k -> new ArrayList<>())
          .add(new ScanAttemptImpl(result, server));
      }
    };
  }

  /**
   * Creates and returns a snapshot of ScanAttempt objects that were added before this call
   *
   * @return a map of TabletId to a collection ScanAttempt objects associated with that TabletId
   */
  synchronized Map<TabletId,Collection<ScanAttemptImpl>> snapshot() {
    //TODO If the map and collections created below are not immutable, then need to wrap w/ immutable. Snapshot should be immutable.
    return attempts.entrySet().stream().map(sourceEntry ->
      new SimpleEntry(sourceEntry.getKey(), List.copyOf(sourceEntry.getValue()))
    ).collect(Collectors.toMap(Entry::getKey, Entry::getValue));
  }
} | Okay, thanks @keith-turner . I'll see about simplifying my PR based on what you said. Since we're doing copy anyway now, it's not necessary to have that. I would like to to modify what you have above to make the concurrency a bit more readable, though. Maybe it would make sense to just use some kind of concurrent list, rather than ArrayList and synchronizing on ScanAttemptImpl.this, which I find to be not very intuitive. Or just block accesses with an explicit lock/mutex object? I'll have to think about it. | Maybe it would make sense to just use some kind of concurrent list

If the data structure is only accessed in a synchronization block then I would recommend avoiding concurrent data structs.  The concurrent data structs use lots of volatile pointers and therefore can not utilize processor cache as well.  That is why I used hash map and array list.

rather than ArrayList and synchronizing on ScanAttemptImpl.this, which I find to be not very intuitive. Or just block accesses with an explicit lock/mutex object?

The two synchronized blocks could sync on attempts.  Its a final hash map, so should be safe to sync on... this would avoid the sync on ScanAttemptImpl.this.  I think that would make the code easier to follow. | Maybe it would make sense to just use some kind of concurrent list

If the data structure is only accessed in a synchronization block then I would recommend avoiding concurrent data structs. The concurrent data structs use lots of volatile pointers and therefore can not utilize processor cache as well. That is why I used hash map and array list.

rather than ArrayList and synchronizing on ScanAttemptImpl.this, which I find to be not very intuitive. Or just block accesses with an explicit lock/mutex object?

The two synchronized blocks could sync on attempts. Its a final hash map, so should be safe to sync on... this would avoid the sync on ScanAttemptImpl.this. I think that would make the code easier to follow.

Why not sync on an explicit object for that purpose? What's the benefit for synchronizing on one of the data structure itself? I find that harder to reason about, because I don't know what else is synchronizing on it, without tracking down all its references. Tracking down the references to an explicit object for synchronizing seems easier. | Why not sync on an explicit object for that purpose? What's the benefit for synchronizing on one of the data structure itself? I find that harder to reason about, because I don't know what else is synchronizing on it, without tracking down all its references. Tracking down the references to an explicit object for synchronizing seems easier.

I was thinking that its a really small class (after these changes) that is easy to analyze in totality and that syncing on the central data struct helps keep it simple and small.  I am not opposed to adding more code for sync though. | Merging to main, can fix any issues laster.","Updates to ScanServerAttempts

As a follow-up to #2880:

* Remove unused and undocumented endTime in ScanAttempt API
* Separate out more inner classes/interfaces into their own files
* Simplify concurrency by synchronizing on the attempts and preparing
  the snapshot using immutable collections
* IDE changes also converted some anonymous inner classes to lambdas"
apache/accumulo,4247,https://github.com/apache/accumulo/pull/4247,Globally Unique FATE Transaction Ids - Part 3,"This addresses several previously deferred changes for issue #4044. Root of most of these new changes were from changing TabletMetadata and TabletUpdates (in Ample) - FateId is now stored in the Metadata table instead of just the formatted long id. Addresses deferred changes to TabletMetadata, TabletUpdates, CompactionConfigStorage, SelectedFiles, and Ample.",2024-02-14T21:31:44Z,keith-turner,kevinrr888,readable,"Completed review changes:

FateId refactored (better Pattern for FateId and simpler validation)
CompactionMetadata GSonData changed to use the canonical FateId string instead of FateId (note that old data may need to be considered here)
SelectedFiles renamed ""txid"" --> ""fateId"". SelectedFilesTest updated to be consistent with changes
CompactionConfigStorage added Preconditions check
Changed compactionHints in TabletManagementParameters from Map<FateId,Map<String,String>> --> Map<String,Map<String,String>>: required changes to TabletManagementParameters, CompactionConfigStorage, CompactionJobGenerator, TabletManagementParametersTest, Manager, CancelCompactions, and PreDeleteTable
AmpleConditionalWriterIT.testCompacted() reordering of FateIds | Changed compactionHints in TabletManagementParameters from Map<FateId,Map<String,String>> --> Map<String,Map<String,String>>: required changes to TabletManagementParameters, CompactionConfigStorage, CompactionJobGenerator, TabletManagementParametersTest, Manager, CancelCompactions, and PreDeleteTable

Sorry I did not communicate very clearly for the comment about that.  Was not looking to change the external map type parameters for TabletManagementParameters to have a key type of String.  Just thought it would be good to change internal map used for json so that the json encoding would cleaner.  So was thinking  of pattern like the following.

The public methods on TabletManagementParameters use Map<FateId,Map<String,String>>
Internally for json encoding Map<FateId,Map<String,String>> is converted to/from Map<String,Map<String,String>> | Changed compactionHints in TabletManagementParameters from Map<FateId,Map<String,String>> --> Map<String,Map<String,String>>: required changes to TabletManagementParameters, CompactionConfigStorage, CompactionJobGenerator, TabletManagementParametersTest, Manager, CancelCompactions, and PreDeleteTable

Sorry I did not communicate very clearly for the comment about that. Was not looking to change the external map type parameters for TabletManagementParameters to have a key type of String. Just thought it would be good to change internal map used for json so that the json encoding would cleaner. So was thinking of pattern like the following.

The public methods on TabletManagementParameters use Map<FateId,Map<String,String>>
Internally for json encoding Map<FateId,Map<String,String>> is converted to/from Map<String,Map<String,String>>


Ah gotcha. Will fix. | @kevinrr888 - The latest IT run is failing (stacktrace below). I think it might be due to this merge. Just an FYI...
org.opentest4j.AssertionFailedError: Test json should be identical to actual metadata at this point ==> expected: 
<{""fateId"":""FATE:USER:0000000000000002"",""selAll"":true,""files"":[""
{\""path\"":\""hdfs://localhost:8020/accumulo/tables/2a/default_tablet/F0000070.rf\"",\""startRow\"":\""\"",\""endRow\"":\""\""}"",""
{\""path\"":\""hdfs://localhost:8020/accumulo/tables/2a/default_tablet/F0000071.rf\"",\""startRow\"":\""\"",\""endRow\"":\""\""}"",""
{\""path\"":\""hdfs://localhost:8020/accumulo/tables/2a/default_tablet/F0000072.rf\"",\""startRow\"":\""\"",\""endRow\"":\""\""}""]}> 
but was: <{""txid"":""FATE:USER:0000000000000002"",""selAll"":true,""files"":[""
{\""path\"":\""hdfs://localhost:8020/accumulo/tables/2a/default_tablet/F0000070.rf\"",\""startRow\"":\""\"",\""endRow\"":\""\""}"",""
{\""path\"":\""hdfs://localhost:8020/accumulo/tables/2a/default_tablet/F0000071.rf\"",\""startRow\"":\""\"",\""endRow\"":\""\""}"",""
{\""path\"":\""hdfs://localhost:8020/accumulo/tables/2a/default_tablet/F0000072.rf\"",\""startRow\"":\""\"",\""endRow\"":\""\""}""]}> | @kevinrr888 - The latest IT run is failing (stacktrace below). I think it might be due to this merge. Just an FYI...
org.opentest4j.AssertionFailedError: Test json should be identical to actual metadata at this point ==> expected: 
<{""fateId"":""FATE:USER:0000000000000002"",""selAll"":true,""files"":[""
{\""path\"":\""hdfs://localhost:8020/accumulo/tables/2a/default_tablet/F0000070.rf\"",\""startRow\"":\""\"",\""endRow\"":\""\""}"",""
{\""path\"":\""hdfs://localhost:8020/accumulo/tables/2a/default_tablet/F0000071.rf\"",\""startRow\"":\""\"",\""endRow\"":\""\""}"",""
{\""path\"":\""hdfs://localhost:8020/accumulo/tables/2a/default_tablet/F0000072.rf\"",\""startRow\"":\""\"",\""endRow\"":\""\""}""]}> 
but was: <{""txid"":""FATE:USER:0000000000000002"",""selAll"":true,""files"":[""
{\""path\"":\""hdfs://localhost:8020/accumulo/tables/2a/default_tablet/F0000070.rf\"",\""startRow\"":\""\"",\""endRow\"":\""\""}"",""
{\""path\"":\""hdfs://localhost:8020/accumulo/tables/2a/default_tablet/F0000071.rf\"",\""startRow\"":\""\"",\""endRow\"":\""\""}"",""
{\""path\"":\""hdfs://localhost:8020/accumulo/tables/2a/default_tablet/F0000072.rf\"",\""startRow\"":\""\"",\""endRow\"":\""\""}""]}>


Thanks for pointing this out. I created #4265","Globally Unique FATE Transaction Ids - Part 3

This addresses several previously deferred changes for issue #4044. Root of most of these new changes were from changing TabletMetadata and TabletUpdates (in Ample) - FateId is now stored in the Metadata table instead of just the formatted long id. Addresses deferred changes to TabletMetadata, TabletUpdates, CompactionConfigStorage, SelectedFiles, and Ample. | Merge branch 'elasticity' into elasticity-feature-4044-deferrals | Review changes:

- FateId refactored (better Pattern for FateId and simpler validation)
- CompactionMetadata GSonData changed to use the canonical FateId string instead of FateId (note that old data may need to be considered here)
- SelectedFiles renamed ""txid"" -> ""fateId"". SelectedFilesTest updated to be consistent with changes
- CompactionConfigStorage added Preconditions check
- Changed compactionHints in TabletManagementParameters from Map<FateId,Map<String,String>> -> Map<String,Map<String,String>>: required changes to TabletManagementParameters, CompactionConfigStorage, CompactionJobGenerator, TabletManagementParametersTest, Manager, CancelCompactions, and PreDeleteTable
- AmpleConditionalWriterIT.testCompacted() reordering of FateIds | Changed TabletManagementParameters as outlined. Restored changes made to CompactionConfigStorage, CompactionJobGenerator, TabletManagementParametersTest, Manager, CancelCompactions, and PreDeleteTable from the previous commit involving changes to 'compactionHints'"
apache/accumulo,2214,https://github.com/apache/accumulo/pull/2214,Remove continue point from Garbage Collector,"Updated Garbage Collection code to no longer use a continue point when processing deletion candidates. The GC  now uses an iterator that lasts during the lifetime of a GC cycle.

The GarbageCollectionTest was updated to work with the update, as was the GC integration test.

Closes #1351",2021-07-29T12:07:56Z,jmark99,jmark99,readable,"Thanks for taking a look @milleruntime. Looking more closely is a good idea since we definitely want the GC to work as expected. I've run the unit and IT tests as well as the sunny profile. Additionally, I ran the GC test in the accumulo-testing repo, although I'd like to run it for a longer period of time. I may wait and see if @keith-turner would like to take a look before merging since he had created the initial ticket. | @jmark99 I just took an initial look at this.  I plan to take a more in-depth look later this afternoon. | @mjwall might have additional insight if this overlaps with the issue that he has been tracking down. | @milleruntime , @keith-turner I did some re-factoring based upon @keith-turner suggestion concerning the getCandidates method. It now returns an iterator. With that change, the flow now more closely resembles the original flow of the code but without the continue point. | I can look at this tonight if it is still open | @mjwall I will wait until tomorrow to merge if you wish to take a look tonight. | This looks good to me @jmark99 .  It is unrelated to what I am tracking in #1916.  First I have looked at Ample, cool abstraction.  Hope it is enough <pun intended>","Remove continue point from Garbage Collector

Updated Garbage Collection code to no longer use a continue point when
processing deletion candidates. The GC  now uses an iterator that lasts
during the lifetime of a GC cycle.

The GarbageCollectionTest was updated to work with the update, as was
the GC integration test.

Closes #1351 | Remove continue point from Garbage Collector

Updated Garbage Collection code to no longer use a continue point when
processing deletion candidates. The GC  now uses an iterator that lasts
during the lifetime of a GC cycle.

The GarbageCollectionTest was updated to work with the update, as was
the GC integration test.

Closes #1351 | Remove continue point from Garbage Collector

Correcting some formatting issues. | Update server/gc/src/test/java/org/apache/accumulo/gc/GarbageCollectionTest.java

Remove an extra character in the comment.

Co-authored-by: Mike Miller <mmiller@apache.org> | Update server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java

Typo correction.

Co-authored-by: Mike Miller <mmiller@apache.org> | Remove continue point from Garbage Collector

Formatting update. | Remove continue point from Garbage Collector

Refactor after updating getCandidates to return an iterator.

Flow is now more closely linked to original code but without use of
continue point. | Remove continue point from Garbage Collector

Changed collectBatch access from public to private. | Remove continue point from Garbage Collector

Renamed collectBatch method to deleteBatch."
apache/accumulo,1995,https://github.com/apache/accumulo/pull/1995,Auto Generate Accumulo Client in JShell,"This commit contains implementations to auto-generate
an Accumulo client during JShell startup.",2021-04-21T21:05:47Z,ctubbsii,slackwinner,readable,,"Auto Generate Accumulo Client in JShell

This commit contains implementations to auto-generate
an Accumulo client on JShell startup. | Improve Accumulo Client Build in JShell

This commit contains code enhancements to
auto-generate an Accumulo Client. | Fix Formatting in JShell Creation File

This commit contains minor updates to
improve formatting of file. | Enhance Accumulo Client Build Script

This commit contains minor updates to improve
the JShell script process. | Revise Accumulo Client Build Method

This commit contains minor changes to replace
the builder method with a different and efficient
method. | Update assemble/src/main/scripts/create-jshell.sh"
apache/accumulo,3445,https://github.com/apache/accumulo/pull/3445,Add tool to modify properties in ZooKeeper without a running cluster,"The tool provides an way to modify properties stored in a single ZooKeeper node as of 2.1, without a running cluster.  The options provided are similar to the shell config command.  The tool can be used to view properties in a node, (with filters) set or delete a single property.  The tool is intended for emergency use when a cluster is not available - otherwise, users should prefer to use the shell.

- creates keyword executable zoo-prop-set-tool and tests
- moves common functions from ZooInfoViewer in ZooPropUtils for use by both tools.  
- contains changes in PR #3444, removing -s safemode from ConfigOpts. (That PR should be merged first)",2023-06-12T23:24:48Z,ctubbsii,EdColeman,"readable, clarity","Sample print output:
System:
> accumulo zoo-prop-set-tool --instanceId 856dadc5-d072-45e9-94f9-c6c4acf8ce55

- Instance name: uno
- Instance id: 856dadc5-d072-45e9-94f9-c6c4acf8ce55
- Property scope: - SYSTEM
- id: 856dadc5-d072-45e9-94f9-c6c4acf8ce55, data version: 1, timestamp: 2023-06-02T16:13:48.690791Z
table.bloom.enabled=true


Namespace:
> zoo-prop-set-tool --instanceId 856dadc5-d072-45e9-94f9-c6c4acf8ce55 -ns ns1

- Instance name: uno
- Instance id: 856dadc5-d072-45e9-94f9-c6c4acf8ce55
- Property scope: - NAMESPACE
- id: 1, data version: 1, timestamp: 2023-06-02T16:13:52.63487Z
table.bloom.enabled=true


Table:
> accumulo zoo-prop-set-tool --instanceId 856dadc5-d072-45e9-94f9-c6c4acf8ce55 -t ns1.tbl1

- Instance name: uno
- Instance id: 856dadc5-d072-45e9-94f9-c6c4acf8ce55
- Property scope: - TABLE
- id: 2, data version: 2, timestamp: 2023-06-02T16:14:00.504751Z
table.bloom.enabled=true
table.constraint.1=org.apache.accumulo.core.data.constraints.DefaultKeySizeConstraint
table.iterator.majc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator
table.iterator.majc.vers.opt.maxVersions=1
table.iterator.minc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator
table.iterator.minc.vers.opt.maxVersions=1
table.iterator.scan.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator
table.iterator.scan.vers.opt.maxVersions=1


With no custom properties (system as example)
> accumulo zoo-prop-set-tool --instanceId 856dadc5-d072-45e9-94f9-c6c4acf8ce55

- Instance name: uno
- Instance id: 856dadc5-d072-45e9-94f9-c6c4acf8ce55
- Property scope: - SYSTEM
- id: 856dadc5-d072-45e9-94f9-c6c4acf8ce55, data version: 2, timestamp: 2023-06-02T16:32:26.712568Z
none | Updated output:
> accumulo zoo-prop-editor -t ns1.tbl1

+ Instance name: uno
+ Instance id: acccdd62-bb86-43c8-91ff-e57fb9e1fe85
+ Property scope: - TABLE
+ id: 2, data version: 2, timestamp: 2023-06-09T16:03:04.008849Z
table.bloom.enabled=true
table.constraint.1=org.apache.accumulo.core.data.constraints.DefaultKeySizeConstraint
table.iterator.majc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator
table.iterator.majc.vers.opt.maxVersions=1
table.iterator.minc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator
table.iterator.minc.vers.opt.maxVersions=1
table.iterator.scan.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator
table.iterator.scan.vers.opt.maxVersions=1

To strip the header:

> accumulo zoo-prop-editor -t ns1.tbl1 | grep -v +

table.bloom.enabled=true
table.constraint.1=org.apache.accumulo.core.data.constraints.DefaultKeySizeConstraint
table.iterator.majc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator
table.iterator.majc.vers.opt.maxVersions=1
table.iterator.minc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator
table.iterator.minc.vers.opt.maxVersions=1
table.iterator.scan.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator
table.iterator.scan.vers.opt.maxVersions=1 | Updated output:

Looks better, but instead of a blank line, can we prefix that with the same prefix as the other informational output, so it can be grepped out also? Can also add another one after the info, as in:
sh$ accumulo zoo-prop-editor -t ns1.tbl1
+
+ Instance: uno (acccdd62-bb86-43c8-91ff-e57fb9e1fe85)
+ Property scope: TABLE(name: tableName, id: tableId)
+ id: 2, data version: 2, timestamp: 2023-06-09T16:03:04.008849Z
+
table.bloom.enabled=true
table.constraint.1=org.apache.accumulo.core.data.constraints.DefaultKeySizeConstraint
I also think the Instance info can be put on one line, and the dash before the scope can be removed, and it would be good to print the name of the table/namespace and ID.
For set operations, could display what was replaced, if anything, so it can be reverted easily. If you use a different prefix for the info at the top, you could use - and + to show both what was added and what was removed, in the same output. | I'm changing it to use ':', but there is no blank line.  That is an artifact of me pasting the command and output and manually separating them for clarity so the command stood out.  From the command line, there is some log4j output, but it does not appear when piped. | Output with ':'
: Instance name: uno
: Instance id: b92e515d-0277-4ea0-a5dd-404c98678c65
: Property scope: - TABLE
: id: 2, data version: 2, timestamp: 2023-06-09T16:36:32.222483Z
table.bloom.enabled=true
table.constraint.1=org.apache.accumulo.core.data.constraints.DefaultKeySizeConstraint
table.iterator.majc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator
table.iterator.majc.vers.opt.maxVersions=1
table.iterator.minc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator
table.iterator.minc.vers.opt.maxVersions=1
table.iterator.scan.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator
table.iterator.scan.vers.opt.maxVersions=1 | web-site documentation for tool is provided with apache/accumulo-website#386 | I choose to leave the set / delete commands silent on success, think that may be more friendly if the actions were scripted. | I choose to leave the set / delete commands silent on success, think that may be more friendly if the actions were scripted.

The main use case I was thinking was if a user ran a set or delete operation, but wanted to revert it, they wouldn't have any info about what the old value was. They also don't get any feedback from the process exit code on whether it was successful or not, unless there's an exception thrown (which should cause a non-zero exit code).
I was thinking just add DEBUG logging, which can easily be sent to STDERR if the user wants to ignore it.
It's not critical, though. The tool is an emergency tool, and is not expected to be used carelessly. These would be minor improvements... if we get around to it.","Add tool to modify properties in ZooKeeper without a running cluster | update print header | modify output format | Merge remote-tracking branch 'upstream/2.1' into 3423_offline_props | change tool name to zoo-prop-editor | Address PR comments

 - renamed ZooPropSetTool to ZooPropEditor (and associated tests)
 - removed main method and updated KeywordStartIT
 - removed instance id and instance name options
 - removed filter options | modify output separator from ""+"" to "":"" | improve tool output header information | Merge branch '2.1' into 3423_offline_props | Fix merge conflicts | Fix KeywordStartIT"
apache/accumulo,1950,https://github.com/apache/accumulo/pull/1950,Clean up CompactionDirectives default impl,,2021-03-04T12:27:58Z,milleruntime,milleruntime,readable,"@milleruntime I'm trying to review this, but it's taking time, because I'm finding I have soooo many questions about the compaction stuff, and having a hard time understanding where to begin. I'm sure I could figure some of this out if I spent some more time with it, but my initial thoughts/questions are:

What's a compaction directive, vs. a compaction service, vs. a compaction dispatcher? (these appear to be the different categories of things touched by this change)
Why was CompactionDirectiveImpl a package-private class stored in the SPI package instead of a separate implementation package?
What was the compaction directives builder that is being deleted in this PR even for (it had no javadoc)? Why did it carry a service ID, and what even is a service ID?
Does the retention of CompactionDirectiveImpl.setService in your PR imply that CompactionsDirectiveImpl is mutable? Should it be mutable? Is it okay to be mutable?
Is the loop in SimpleCompactionDispatcher that calls defaultService.setService(v) a good idea? What's the end goal being pursued here if the service that is being held in the defaultService object is being swapped out repeatedly in a loop?
Naming-wise, what does it mean for a ""defaultService"" to have a ""setService"" method? What is the relationship between the ""service"" that the variable name is referring to and the ""service"" that the method name is referring to? | CompactionDispatcher and CompactionDirective follow the same pattern as ScanDispatcher and ScanDirectives.  In my opinion it would be good to keep them consistent.  ScanDispatcher used to return a single string for its dispatch method in 2.0.0.  Then in 2.1.0 cache information was added to the scan dispatch method resulting in the need to return an object.  That is why I chose to start off returning an object for the compaction dispatch method. | CompactionDispatcher and CompactionDirective follow the same pattern as ScanDispatcher and ScanDirectives. In my opinion it would be good to keep them consistent. ScanDispatcher used to return a single string for its dispatch method in 2.0.0. Then in 2.1.0 cache information was added to the scan dispatch method resulting in the need to return an object. That is why I chose to start off returning an object for the compaction dispatch method.

There seems to be similar issues with ScanDispatcher, mainly having a builder where only one method should be called to set a single object on another class. My concern is that it seems like a lot of extra code for just calling new ScanDirectivesImpl().setX() and the builder returned allows multiple setX() calls, which would only create throw away objects. | There seems to be similar issues with ScanDispatcher, mainly having a builder where only one method should be called to set a single object on another class.

For the ScanDispatcher three things can be set.  In 2.0.0 only one thing could be set for the ScanDispatcher.  For 2.1.0 to additional things were added.  Because ScanDispatcher did not follow the pattern in 2.0.0 that it does now, the addition of new things to return was a painful.  The design of this interface is the way it is to support SPI evolution where new things can be returned in the future.  When writing the ScanDispatcher in 2.0.0 I never could have imagined additionally returning cache directives in 2.1.0.

My concern is that it seems like a lot of extra code for just calling new ScanDirectivesImpl().setX() and the builder

Someone writing a ScanDispatcher or CompactionDispatcher would never interact with ScanDirectivesImpl or CompactionDirectivesImpl.  Those are internal implementations hidden from users.  When making changes I would suggest to start with the interfaces CompactionDirectives and ScanDirectives and changes those to how you think they should be w/o regard for ScanDirectivesImpl and CompactionDirectivesImpl.  Also considering how a user writing a CompactionDispatcher would use CompactionDirectives, as the interfaces only exists to support CompactionDispatcher. | Perhaps a stupid question, but if it's designed as complex, extensible return type for the Dispatcher.dispatch() method, why not call the returned object a ""Dispatch"" rather than a ""Directive""? | Perhaps a stupid question, but if it's designed as complex, extensible return type for the Dispatcher.dispatch() method, why not call the returned object a ""Dispatch"" rather than a ""Directive""?

That name seems fine w/ me.  If changed I think it would make sense to consider ScanDispatcher also.  At this point I would personally prioritize consistency between the two. | @keith-turner After the last changes, I had to add a way to set the default in a977acc | I think it would be good to rename setService as @ctubbsii had suggested.

I am not sure what you are referring to. Sorry its hard to follow with all the comments. | I think it would be good to rename setService as @ctubbsii had suggested.

I am not sure what you are referring to. Sorry its hard to follow with all the comments.


If it's required at compile time then I would be ok w/ that change. Also I agreep setService is not a good name for a fluent builder, something else would be better like toService().

Nevermind found it. Sorry I missed your response.","Clean up CompactionDirectives default impl | CR Updates | Update javadoc | Update core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionDirectivesBuilder.java

Co-authored-by: Keith Turner <kturner@apache.org> | Update core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionDirectives.java

Co-authored-by: Keith Turner <kturner@apache.org> | Change how default CompactionDirectives is set | Updates to prevent NPE | Update core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionDirectivesBuilder.java

Co-authored-by: Keith Turner <kturner@apache.org> | CR updates"
apache/accumulo,2240,https://github.com/apache/accumulo/pull/2240,Prevent compactions from starting when deleting a table,Closes #2182,2021-09-07T17:00:20Z,dlmarion,dlmarion,understandable,"So, I wrote a new test in CompactionIT. The compaction fails with a TableNotFoundException, but at a different point in the code. I wonder if this is already handled.
org.apache.accumulo.core.client.TableNotFoundException: Table (Id=1) does not exist (Table does not exist)
	at org.apache.accumulo.core.clientImpl.TableOperationsImpl.doFateOperation(TableOperationsImpl.java:390)
	at org.apache.accumulo.core.clientImpl.TableOperationsImpl.compact(TableOperationsImpl.java:866)
	at org.apache.accumulo.test.functional.CompactionIT.testDeleteTableAbortsCompaction(CompactionIT.java:223)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: ThriftTableOperationException(tableId:1, tableName:, op:COMPACT, type:NOTFOUND, description:Table does not exist)
	at org.apache.accumulo.core.manager.thrift.FateService$waitForFateOperation_result$waitForFateOperation_resultStandardScheme.read(FateService.java:4731)
	at org.apache.accumulo.core.manager.thrift.FateService$waitForFateOperation_result$waitForFateOperation_resultStandardScheme.read(FateService.java:1)
	at org.apache.accumulo.core.manager.thrift.FateService$waitForFateOperation_result.read(FateService.java:4626)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)
	at org.apache.accumulo.core.manager.thrift.FateService$Client.recv_waitForFateOperation(FateService.java:157)
	at org.apache.accumulo.core.manager.thrift.FateService$Client.waitForFateOperation(FateService.java:142)
	at org.apache.accumulo.core.clientImpl.TableOperationsImpl.waitForFateOperation(TableOperationsImpl.java:303)
	at org.apache.accumulo.core.clientImpl.TableOperationsImpl.doFateOperation(TableOperationsImpl.java:372)
	... 16 more","Prevent compactions from starting when deleting a table

Closes #2182 | Remove unused Logger | Fixed NPE in Zoo.exists, added test which currently fails | Merge branch 'main' into 2182-delete-table-prevents-compactions | Fix findbugs issue | Applied suggestion from PR | Merge branch 'main' into 2182-delete-table-prevents-compactions | Removed CompactionIT test that did not work, added unit test | Remove unused dependency | Updates to code based on PR comments | Pass TableId to utility methods instead of String"
apache/accumulo,2755,https://github.com/apache/accumulo/pull/2755,Fix Fate print command and improve ShellServerIT test,"This is more for discussion of #2754, the the fate -print issue.  It seems that the code was filtering the transaction.

Comparing the logic in main and in https://github.com/apache/accumulo/pull/2215, the logic seems the same, so not sure what is going on.

Also, the original logic seemed correct, but rather than the negative / skip logic that was used, this code flips that.  Not sure this is a final solution - but maybe it help points to the issue.",2022-06-28T16:09:52Z,EdColeman,EdColeman,understandable,"I think this could be marked ready for review. There are some white space changes that could be dropped and the logic is confusing but if the test is passing then that is the important part to fix #2754. This PR is definitely closer to being merged than #2780, which is just an updated version of 2215.","modify filter conditions for status | auto format corrections | Merge remote-tracking branch 'origin/fate_print_debug' into fate_print_debug | Adds FATE test

 - Test provided by millerruntime, modified to add cluster accumulo.properties
 - modified logic for fate filters | minor clean-up | Clean-up logic and tests

 - modified filter logic
 - expanded IT test
 - modified unit test, command parsing didn't match IT results | Merge remote-tracking branch 'upstream/main' into fate_print_debug | minor clean-up in test | test improvements - added txid filter tests | minor test addition, merge update"
apache/accumulo,6069,https://github.com/apache/accumulo/pull/6069,Add Add RowRange filters to listtablets command,"Fixes #6035 
This PR adds new options to the `listtablets` command and also removed one from the `getavailability` command.

Added row range filtering to `listtablets` matching what `getavailability` already supports:
```
  -b,--begin-row <arg>            begin row (inclusive)
  -be,--begin-exclusive           make start row exclusive (by default it's inclusive)
  -e,--end-row <end-row>          end row (inclusive)
```

The `-ee`/`--end-exclusive` option was removed from `getavailability` since it was not being used anyways. Ultimately both `listtablets` and `getavailability` call this portion of the code which just sets end exclusive to true:
https://github.com/apache/accumulo/blob/3dfe28e8e1774a143c86edbeb645ea3ac301cdd5/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java#L415-L425",2026-01-27T17:58:30Z,DomGarguilo,DomGarguilo,clarity,,Add Add RowRange filters to listtablets command | add missing end paren | rename opt vars for clarity | Merge branch 'main' into listTabletsFilter
apache/accumulo,5336,https://github.com/apache/accumulo/pull/5336,Rename TxName to fateOp for clarity (#5230),"Replaced occurrences of TxName, txName, and similar references with fateOp.

Changes
    Renamed `txName` → `fateOp` where applicable
    Updated related method names

Fixes #5230 ",2025-02-18T15:13:02Z,kevinrr888,Suvrat1629,clarity,"@kevinrr888
I had renamed all the references that I could find, please take a look and tell me if have missed anything.
Thank you! | grep -riE --include *.java 'tx.?name' . I ran this command and only found a couple of comments containing tx name do you want me to refactor this also? | Yes, comments should be updated | @kevinrr888
I updated the comments and also since in one of the prior comments I had changed putName to putFateOp I changed all the references of putName to putFateOp. Please take a look.","renamed txName and other references to fateOp | Merge branch 'apache:main' into rename-txname-5230 | putName -> putFateOp

Co-authored-by: Kevin Rathbun <kevinrr888@gmail.com> | Update core/src/main/java/org/apache/accumulo/core/fate/user/schema/FateSchema.java

Co-authored-by: Kevin Rathbun <kevinrr888@gmail.com> | changed putName to putFateOp and similar references"
apache/accumulo,5502,https://github.com/apache/accumulo/pull/5502,Improves accumulo-service script,"Adds improved list output and cleanup of dead pid files.
Ported changes from #5439 ",2025-04-24T20:42:05Z,ddanielr,ddanielr,clarity,,"Improves accumulo-service script

Adds improved list output and cleanup of pid files

---------

Co-authored-by: Keith Turner <kturner@apache.org> | Update assemble/bin/accumulo-service

Co-authored-by: Keith Turner <kturner@apache.org> | Update assemble/bin/accumulo-service

Co-authored-by: Dave Marion <dlmarion@apache.org> | Fix reading file contents"
apache/accumulo,3989,https://github.com/apache/accumulo/pull/3989,Remove remaining references to deprecated prefix,"As part of #3915, a log message was added for instructing users to update their compaction properties if they were using an old prefix. 
https://github.com/apache/accumulo/blob/e12e033df1721fb8a851017ffe39c8a9a49b7a49/core/src/main/java/org/apache/accumulo/core/util/compaction/CompactionServicesConfig.java#L109-L113 

Unfortunately, there are also a number of accumulo tests that modified the accumulo config to set compaction service properties. 
So the test logs starting showing a `Please update property to use the compaction.service prefix` message which isn't ideal. 

This PR* fixes those tests by switching the remaining hardcoded references from `tserver.compaction.major.service` to `compaction.service`. and also updates some documentation references for clarity.

*Includes the `ExternalCompactionTestUtils.java` property updates from #3983 ",2023-11-27T22:22:41Z,ddanielr,ddanielr,clarity,,"Remove deprecated prefix references

Switch the remaining hardcoded references from
`tserver.compaction.major.service` to `compaction.service`."
apache/accumulo,3954,https://github.com/apache/accumulo/pull/3954,Use single mutation during no chop fence upgrade,"This is an optimization of the upgrade code from PR #3876.  This code processes each row and emits one mutation for all of the changes needed to upgrade that row.  At a minimum this should reduce the number of mutations needed by 50% by combining the `add updated file reference with fencing` and the `delete obsolete file reference` into a single mutation.  When there are multiple files, chopped or external compaction references for the row, the reduction will be greater. 

The changes simplified error handling so individual methods for each column type was not providing clarity and were removed.  The tests were modified using a mock scanner to provide equivalent coverage that was done for the individual methods.

This code was also testing manually upgrading an uno instance from 2.1.3-SNAPSHOT to this code.",2023-11-24T23:41:41Z,EdColeman,EdColeman,clarity,,"Use single mutation during no chop fence upgrade | Add guard to prevent trying empty muation, PR comment | Use TextUtil.getBytes instead of row.getBytes. Addresses PR comment | add ColVis precondition check | Merge remote-tracking branch 'upstream/main' into no_chop_upgrade_one_mutation | Update formatting for ColVis precondition check | minor test improvments, guard for trace. Address PR comments | remove unnecessary AccumuloClient creation. Addresses PR comment | minor style clean-up"
apache/accumulo,3842,https://github.com/apache/accumulo/pull/3842,Rename setgoal/getgoal to sethostinggoal/gethostinggoal,"Rename the setgoal and getgoal shell commands to sethostinggoal and gethostinggoal, respectively. This change is being made to provide more clarity as to what the commands are doing and make them less generic.

Several other instances if 'goal' were changed to 'hostinggoal' in order to add more clarity.

Closes #3800",2023-10-13T11:54:33Z,jmark99,jmark99,clarity,"@jmark99 - when this is merged, could you update https://cwiki.apache.org/confluence/display/ACCUMULO/A+More+Cost+Efficient+Accumulo and https://cwiki.apache.org/confluence/display/ACCUMULO/Example+OnDemand+Tablet+Operation?","Rename setgoal/getgoal to sethostinggoal/gethostinggoal

Rename the setgoal and getgoal shell commands to sethostinggoal and
gethostinggoal, respectively. This change is being made to provide more
clarity as to what the commands are doing and make them less generic.

Several other instances if 'goal' were changed to 'hostinggoal' in order
to add more clarity.

Closes #3800"
apache/accumulo,3659,https://github.com/apache/accumulo/pull/3659,"make option exclude-parent-properties, other minor clean-up","- Change command option from `exclude-parent` to `exclude-parent-properties` for clarity.
- clarify command description present to uses
- comment clean-up

From additional comment in PR #3562 made after merge.",2023-07-27T21:29:23Z,EdColeman,EdColeman,clarity,This PR doesn't seem to be triggering the GitHub Actions CI checks for some reason. Not really sure what's going on there. | I manually triggered a re-run just to double check (https://github.com/apache/accumulo/actions/runs/5662956604),"make option exclude-parent-properties, other minor clean-up | update changed option in tests | Apply suggestions from code review"
apache/accumulo,4115,https://github.com/apache/accumulo/pull/4115,Fix ImportConfiguration.builder().setKeepOffline(),"Fixes #4045 

This PR adds logic to properly transition the state of a new table created via `importTable()` to offline (or online) depending on the value supplied via the `ImportConfiguration`.

This was corrected by the change in `FinishImportTable` where the table was never transitioned from the NEW to OFFLINE table state when the (then `onlineTable`) param was set.

Other changes:
* added a check in the IT to make sure the new table is in the OFFLINE state instead of just ""not online""
* renamed the member variable from `onlineTable` to `keepOffline` for clarity and consistency",2024-01-05T15:55:29Z,DomGarguilo,DomGarguilo,clarity,,Fix ImportConfiguration.builder().setKeepOffline()
apache/accumulo,3148,https://github.com/apache/accumulo/pull/3148,Added Prefix enum class to use for assigning and evaluating files,,2023-01-06T15:47:09Z,dlmarion,dlmarion,clarity,,Added Prefix enum class to use for assigning and evaluating files | Add missing enum | Merge branch 'main' into file-prefix-enum | Renamed Prefix to FilePrefix
apache/accumulo,2241,https://github.com/apache/accumulo/pull/2241,Rename all ServerContext variables to context,,2021-08-24T16:09:40Z,milleruntime,milleruntime,clarity,"Would it be better if it was something more like srvContext ? It looks like we have other contexts (ClientContext, ClassLoaderContext) as well as M/R contexts. It may be that ServerContext and ClientContext are not used together - but with using 'context' anyone should not assume that all instances of context are server contexts.
'context' does read better, but wondering if there are benefits to being more specific to add clarity or if it just makes it harder to read for something that is already obvious.

The other context objects are only used in specific classes. I went with the name that was most popular but I am not opposed to the more specific name serverContext. It would just require more changes. | Just comparing what is already in the code... To change the name to serverContext would require changing 147 files vs context  in this PR only changes 23. The name context is used in MR classes for JobContext, for a private Context object in ContextManager, both of which are deprecated. But it is used for ClientContext in about 100 files and for a String type in ContextClassLoaderFactory, which is new.
The way that the String type is used in ContextClassLoaderFactory, makes me think that it is the only object that should use the context variable name. As opposed to everywhere else we have specific types of context objects (ClientContext, ServerContext, JobContext, etc) that we can be more specific with the variable names. | I started renaming context to serverContext but it is a lot more work. It would be nice to have both consistency and specificity, but I am going to go with this PR so we at least have consistency.",Rename all ServerContext variables to context
apache/accumulo,5256,https://github.com/apache/accumulo/pull/5256,"Reused ZooCache where possible, used persistent recursive watchers","Reused ZooCache from client / server context where possible. Removed overloaded ZooCache constructor to make it easier to find where new instances are constructed. Removed watcher that was being placed in each call to the underlying ZooKeeper in favor of long-lived persistent recursive watchers set on specific paths which will fire when any child under that path is modified.

Related to #5134
Closes #5154, #5157",2025-02-03T21:35:50Z,dlmarion,dlmarion,clarity,"As of f88e363, a ZooCache instance is being created in the following places:
ClientContext - shared instance
ClientContext.getTServerLockChecker - non-shared instance when creating the ZookeeperLockChecker
ListInstances - non-shared instance in a standalone utility, not sure this needs to use ZooCache TBH
and in tests. | ListInstances - non-shared instance in a standalone utility, not sure this needs to use ZooCache TBH

Created #5257 to look at this | It seems trying to use persistent recursive watches w/ ZooSession is a bit tricky because persistent recursive watches have a lifecycle that is 1:1 w/ a zookeeper object.  The nice thing we get from ZooSession is the automated retry, however it significantly complicates this 1:1 relationship.  The following is a half baked idea that is trying to reconcile these things in the code, in this model there are two objects.
   // This contains all of the current ZooCache code, but it uses ZooKeeper directly instead of ZooSession.  
   // Instances of this class use a single zookeeper object for their lifetime (so the Zookeeper obj could be 
   // final).  Also the set of watched paths could be final and immutable.
class ZooCacheDirect {

}
// All code in Accumulo uses this for caching zookeeper reads
class ZooCache {
   // All reads are proxied to the internal zoo cache w/ reconnect handling that will create an new internal
   //  zoocachedirect as needed.
   AtomicReference<ZooCacheDirect> internalZooCache;
   
   // However w/ this model it seems that ZooCache has to duplicate some of the retry logic in ZooSession?  Could this be refactored?
}
Not really sure about the above.  Its an attempt to move the zoocache code to having a 1:1 relationship w/ a zookeeper object in its internals because this may simplify its impl and simplify reasoning abouts its correctness.  However unsure how the retry logic plays out in this model and if it duplicates zoosession code. | I wonder if it would make more sense to have a ZooSession.getCache method. Then ZooSession could keep track of the ZooCache's that it creates, and call the appropriate methods on the cache when the connection state changes. | I wonder if it would make more sense to have a ZooSession.getCache method. Then ZooSession could keep track of the ZooCache's that it creates, and call the appropriate methods on the cache when the connection state changes.

Maybe a method like Supplier<ZooCache> createCache(Set<String> watchedPaths) on zoosession could work.  The supplier could recreate the zoocache when reconnect happens, passing the new zookeeper obj to the new cache it creates. | I wonder if it would make more sense to have a ZooSession.getCache method. Then ZooSession could keep track of the ZooCache's that it creates, and call the appropriate methods on the cache when the connection state changes.

Maybe a method like Supplier<ZooCache> createCache(Set<String> watchedPaths) on zoosession could work. The supplier could recreate the zoocache when reconnect happens, passing the new zookeeper obj to the new cache it creates.

Interesting, I can play with that and see where it leads. I think it could simplify some things. | The GitHub build step that keeps failing in this PR is failing with:
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space: failed reallocation of scalar replaced objects

in the spotbugs portion of the build in accumulo-core. | @keith-turner  @ctubbsii - I think there is one thing left to do , remove the calls to ZooKeeper.exists. I'm going to do that next. | @keith-turner  - I removed the calls to ZooKeeper.exists in ae7ace8. ZooCacheTest wasn't an easy fix, not sure it's 100% correct. It definitely needs some eyes on it. | This could be in a follow on change.  It would be nice to attempt to test the zookeeper object in zoosession changing.  Not sure if should be in an IT or in ZooCacheTest using EasyMock. If its possible to do in an IT would be nice, could maybe kill and restart the zookeeper server. | @keith-turner - I removed the calls to ZooKeeper.exists in ae7ace8. ZooCacheTest wasn't an easy fix, not sure it's 100% correct. It definitely needs some eyes on it.

@dlmarion  the changes to remove exists look good.  I found some code that should  be removed while looking at it. Re ZooCacheTest, was there a specifc method/part you wanted reviewed? | @keith-turner - I removed the calls to ZooKeeper.exists in ae7ace8. ZooCacheTest wasn't an easy fix, not sure it's 100% correct. It definitely needs some eyes on it.

@dlmarion the changes to remove exists look good. I found some code that should be removed while looking at it. Re ZooCacheTest, was there a specifc method/part you wanted reviewed?

There were a lot of changes there, and I had to make some assumptions based on removing exists calls. I also had to change ZcNode to allow data to be null. The documentation on some of the methods said that data could be null, but there was code in the class preventing it.
Edit: Also there was some code in ZooCacheTest checking some ephemeral node id or something, and I just removed all of it. | There were a lot of changes there, and I had to make some assumptions based on removing exists calls. I also had to change ZcNode to allow data to be null.

ZcNode should not allow data to be null because it only expects the data constructor be called when there is data.  When data is null means that data was never fetched, which could be the case when only the children were fetched.
I have run into problems w/ this test before where its mocking of zookeeper does not follow the behavior of zookeeper.  When asking for data for a node that does not exist in zookeeper it will throw a NoNodeException and not return null.  Made the following changes to add the non null check back and make the test throw NoNodeException instead of return null.  Also one test expected synconnected to clear the cache, so had to change that.  The syncconnected test may be wrong in prev versions of Accumulo and maybe that is masked by the incorrect mocking behavior.
diff --git a/core/src/main/java/org/apache/accumulo/core/zookeeper/ZcNode.java b/core/src/main/java/org/apache/accumulo/core/zookeeper/ZcNode.java
index 741f905b26..d1f75ae4b8 100644
--- a/core/src/main/java/org/apache/accumulo/core/zookeeper/ZcNode.java
+++ b/core/src/main/java/org/apache/accumulo/core/zookeeper/ZcNode.java
@@ -83,7 +83,7 @@ class ZcNode {
    * stat.
    */
   ZcNode(byte[] data, ZcStat zstat, ZcNode existing) {
-    this.data = data;
+    this.data = Objects.requireNonNull(data);
     this.stat = Objects.requireNonNull(zstat);
     if (existing == null) {
       this.children = null;
diff --git a/core/src/test/java/org/apache/accumulo/core/zookeeper/ZooCacheTest.java b/core/src/test/java/org/apache/accumulo/core/zookeeper/ZooCacheTest.java
index 02484b1692..6dd4e2dcc2 100644
--- a/core/src/test/java/org/apache/accumulo/core/zookeeper/ZooCacheTest.java
+++ b/core/src/test/java/org/apache/accumulo/core/zookeeper/ZooCacheTest.java
@@ -263,7 +263,7 @@ public class ZooCacheTest {
 
   @Test
   public void testWatchDataNode_NoneSyncConnected() throws Exception {
-    testWatchDataNode(null, Watcher.Event.EventType.None, false);
+    testWatchDataNode(null, Watcher.Event.EventType.None, true);
   }
 
   private void testWatchDataNode(byte[] initialData, Watcher.Event.EventType eventType,
@@ -285,11 +285,11 @@ public class ZooCacheTest {
     if (initialData != null) {
       expect(zk.getData(ZPATH, null, existsStat)).andReturn(initialData);
     } else {
-      expect(zk.getData(ZPATH, null, existsStat)).andReturn(null);
+      expect(zk.getData(ZPATH, null, existsStat)).andThrow(new KeeperException.NoNodeException(ZPATH));
     }
     replay(zk);
     zc.get(ZPATH);
-    assertEquals(initialData != null, zc.dataCached(ZPATH));
+    assertTrue(zc.dataCached(ZPATH));
   }
 
   @Test

I will look at the ephemeral thing and see if I know anything. | This could be in a follow on change. It would be nice to attempt to test the zookeeper object in zoosession changing. Not sure if should be in an IT or in ZooCacheTest using EasyMock. If its possible to do in an IT would be nice, could maybe kill and restart the zookeeper server.

Added a test for this in ZooCache IT (in ddeee6b) | Full IT build passed","Reused ZooCache where possible, used persistent recursive watchers

Reused ZooCache from client / server context where possible. Removed
overloaded ZooCache constructor to make it easier to find where new
instances are constructed. Removed watcher that was being placed in
each call to the underlying ZooKeeper in favor of long-lived persistent
recursive watchers set on specific paths which will fire when any
child under that path is modified.

Related to #5134
Closes #5154, #5157 | address PR comment | Modified ZooCache to support multiple ZooCacheWatchers, updated TableManager | Merge branch 'main' into zc-recursive-watchers | Merge branch 'main' into zc-recursive-watchers | Merge branch 'zc-recursive-watchers' of github.com:dlmarion/accumulo into zc-recursive-watchers | Made ZC.clear() private, fixed path issue in TableZooHelper | Addressed PR suggestions | Ensure that we don't put watchers on overlapping paths | Merge branch 'main' into zc-recursive-watchers | Fix overlapping path logic and add test | Update GitHub Actions to use ZK 3.6.4

The feature in this PR requires ZK 3.6 or later, so accepting this PR
requires us to bump our minimum supported ZK version that we can test
with. | Merge branch 'main' into zc-recursive-watchers | Recreate watchers on re-connect, address PR comments | Add ZooCache as a connection state observer on ZooSession | Update core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooCache.java

Co-authored-by: Keith Turner <kturner@apache.org> | Update core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooCache.java

Co-authored-by: Keith Turner <kturner@apache.org> | Fix GH code suggestion, implement PR suggestion | Moved ZooCache management into ZooSession | fix formatting | Add missing override annotation | Revert change to ClientContext ZooCache path change | Merge branch 'main' into zc-recursive-watchers | Fix missing import | Merge branch 'main' into zc-recursive-watchers | Removed ZooCache interface, removed ZooSession-ZooCache entanglement | Set spotbugs heap size to 1G to fix GitHub build action OOME | Keep track of multiple watchers per PR Watcher path | Clear cache on close | Remove loop in reconnect and add PR Watchers without calling verify | Don't call verifyConnected inside of reconnect | Remove setup of watchers from ZooSession, set up in ZooCache only | Can't use WatcherType.PersistentRecursive until ZK 3.9.0 | Updates from meeting with Keith and Christopher | Update core/src/main/java/org/apache/accumulo/core/lock/ServiceLock.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Update core/src/main/java/org/apache/accumulo/core/zookeeper/ZooCache.java

Co-authored-by: Christopher Tubbs <ctubbsii@apache.org> | Addressed PR comments | Update core/src/main/java/org/apache/accumulo/core/zookeeper/ZooCache.java

Co-authored-by: Keith Turner <kturner@apache.org> | Update core/src/main/java/org/apache/accumulo/core/zookeeper/ZooCache.java

Co-authored-by: Keith Turner <kturner@apache.org> | Removed ZooKeeper.exists calls that were being used to set Watcher | Merge branch 'zc-recursive-watchers' of github.com:dlmarion/accumulo into zc-recursive-watchers | Fix formatting | Use variable in log statement | Address pr suggestions, add new IT that restarts ZK | Merge branch 'main' into zc-recursive-watchers | Add test lock path to fix some failing ITs | Merge branch 'main' into zc-recursive-watchers | Re-added removed precondition check in ZcNode | Changes from trying to fix RestartIT | Merge branch 'main' into zc-recursive-watchers | Fixed issue with LiveTServerSet receiving paths with children of the server | Merge branch 'main' into zc-recursive-watchers | Reduce logging in TableManager for unhandled path | Update server/base/src/main/java/org/apache/accumulo/server/manager/LiveTServerSet.java

Co-authored-by: Keith Turner <kturner@apache.org> | fix formatting | Merge branch 'main' into zc-recursive-watchers"
apache/accumulo,4151,https://github.com/apache/accumulo/pull/4151,Adjusts compactor error logging because of race,"Noticed error logging from the compactor process and found it was caused by a race condition.  The logging indicated a compaction was not running when it was expected to, but the compaction started shortly after.  The background thread had not yet populated a set that the foreground thread was examining.  Adjusted the error logging to debug since it can happen spuriously.",2024-02-08T18:55:56Z,keith-turner,keith-turner,clarity,"Good catch. I wonder if instead of the conditional that is currently in the Compactor (below) we should just put while loop that waits for !running.isEmpty(). That way the main thread will wait for the background thread to be invoked and make initial progress.
            List<CompactionInfo> running =
                org.apache.accumulo.server.compaction.FileCompactor.getRunningCompactions();
            if (!running.isEmpty()) {
            ...
            } else {
              LOG.error(""Waiting on compaction thread to finish, but no RUNNING compaction"");
            }

For clarity, I'm suggesting something like:
            List<CompactionInfo> running =
                org.apache.accumulo.server.compaction.FileCompactor.getRunningCompactions();
            while (!running.isEmpty()) {
              LOG.error(""Waiting on compaction thread to start"");
              Thread.sleep(3000);
            } | Also, this could probably target 2.1 | I wonder if instead of the conditional that is currently in the Compactor (below) we should just put while loop that waits for !running.isEmpty(). That way the main thread will wait for the background thread to be invoked and make initial progress.

There are two conditions where running could be empty.  One is where the background thread has not started and the other is where it has finished.  Can not really tell which one empty means the way things currently are. | Force pushed a change for 2.1 and reset the PR branch to 2.1","Adjusts compactor error logging because of race

Noticed error logging from the compactor process and found it was caused
by a race condition.  The logging indicated a compaction was not
running when it was expected to, but the compaction started shortly
after.  The background thread had not yet populated a set that the
foreground thread was examining.  Adjusted the error logging to debug
since it can happen spuriously."
apache/accumulo,4562,https://github.com/apache/accumulo/pull/4562,Added ZK cleanup thread to Manager for Scan Server nodes,Closes #4559,2024-05-16T18:54:05Z,dlmarion,dlmarion,clarity,"I started to comment on the loop where the lock data was read in the loop from getChildren with the following:

Would it be worth it to wrap this call with another try...catch(Keeper.NO_NODE ex) to allow it to handle the case where the ephemeral lock was removed while in the main getChildren loop?  With no lock node, it could either delete the host:port then - or at least continue processing the other nodes.  As is, it will retry, but handling NO_NODE could  make it more responsive by processing the remaining nodes in the list.

Taking no action and allowing that node to be processed on the next try would be safer.
But, could there be a race condition between the server lock code and this cleaner.  If the server lock creates the host:port node and then writes the lock there will a period where the lock does not exist, but host:port is expected to be there.  What would happen if the cleaner deletes the host:port and then the server lock write is attempted?
It may be possible to use the creation time of the host:port node (ZK stat ctime) and check that it is older than the loop retry period.  This would delay the removal for at least one cleaner cycle.  Or, the service lock code could try to recreate the host:port node. | I did test the code using a small, single node instance with multiple scan servers - the scan server entries were removed on a cluster restart and when killing individual scan server processes as expected. | Another mitigation may be to delay the first run of the cleaners so that initialization and scan server assignments have a better chance to complete before the cleaner runs - that way seeing a lock as it is being built should not occur. | Another mitigation may be to delay the first run of the cleaners so that initialization and scan server assignments have a better chance to complete before the cleaner runs - that way seeing a lock as it is being built should not occur.

The Manager and CompactionCoordinator don't have an initial delay when calling LiveTServerSet.startListeningForTabletServerChanges or CompactionCoordinator.startCompactionCleaner. I could make this change, but if we do, then we should do it for all of them for the same reason. | I started to comment on the loop where the lock data was read in the loop from getChildren with the following:

Would it be worth it to wrap this call with another try...catch(Keeper.NO_NODE ex) to allow it to handle the case where the ephemeral lock was removed while in the main getChildren loop?  With no lock node, it could either delete the host:port then - or at least continue processing the other nodes.  As is, it will retry, but handling NO_NODE could  make it more responsive by processing the remaining nodes in the list.

Taking no action and allowing that node to be processed on the next try would be safer.
But, could there be a race condition between the server lock code and this cleaner. If the server lock creates the host:port node and then writes the lock there will a period where the lock does not exist, but host:port is expected to be there. What would happen if the cleaner deletes the host:port and then the server lock write is attempted?
It may be possible to use the creation time of the host:port node (ZK stat ctime) and check that it is older than the loop retry period. This would delay the removal for at least one cleaner cycle. Or, the service lock code could try to recreate the host:port node.

Are you suggesting wrapping the following line with a try/catch to catch Keeper.NO_NODE?
            byte[] lockData = ServiceLock.getLockData(getContext().getZooCache(), zLockPath, stat);

I don't think that method throws that Exception. | You are correct - it does not throw NO_NODE - I incorrectly assumed that getLockData would echo the ZK exceptions - but it does not,","Added ZK cleanup thread to Manager for Scan Server nodes

Closes #4559 | Change interval variable name and units"
apache/accumulo,2492,https://github.com/apache/accumulo/pull/2492,Add goal message to Fate,* Closes #1249,2022-02-14T19:38:19Z,milleruntime,milleruntime,clarity,"Here are some examples of log output:
2022-02-14T10:25:34,906 [fate.Fate] INFO : FATE TxId: 3799578337887111047 Op: TABLE_CREATE Create table ci ONLINE with 19 splits.
2022-02-14T10:37:48,674 [fate.Fate] INFO : FATE TxId: 5505785720937952578 Op: TABLE_COMPACT Compact table (1) with config []
2022-02-14T10:38:18,256 [fate.Fate] INFO : FATE TxId: 2554407174635375739 Op: TABLE_DELETE Delete table ci(1) | @milleruntime I think GitHub is supposed to auto-link PRs to their referenced issues. However, I don't think it works the way you're doing it. It should work when you add the closing keywords to the description or body of a commit message, but it does not seem to work when you add it to the subject line. For consistency (and for less confusion), it's probably best to put these in the body of the commit anyway, so that way when GitHub appends the PR number on merge, it looks a bit nicer:
Add goal message to Fate (#2492)

vs.
Add goal message to Fate. Closes #1249 (#2492) | From the sample log
2022-02-14T10:37:48,674 [fate.Fate] INFO : FATE TxId: 5505785720937952578 Op: TABLE_COMPACT Compact table (1) with config []
2022-02-14T10:38:18,256 [fate.Fate] INFO : FATE TxId: 2554407174635375739 Op: TABLE_DELETE Delete table ci(1)

What is the number (1) in parentheses? The tableId? | From the sample log
2022-02-14T10:37:48,674 [fate.Fate] INFO : FATE TxId: 5505785720937952578 Op: TABLE_COMPACT Compact table (1) with config []
2022-02-14T10:38:18,256 [fate.Fate] INFO : FATE TxId: 2554407174635375739 Op: TABLE_DELETE Delete table ci(1)

What is the number (1) in parentheses? The tableId?

Yes | This method was created because the code used to log fate txids in many different ways (hex and decimal for example)

I forgot about that, thanks @keith-turner. What do you think of changing the transaction ID in the next version to be something more straight forward? Maybe use something like java.util.UUID? | What is the number (1) in parentheses? The tableId?

In another comment on this thread I suggested a specific way to log FATE IDs that was consistent.  The comments about table ids made me think it would be nice to consitently log all ids in the logs.  Maybe something like <type>[<id>]. For example FATE[123], TABLE_ID[456], ECID[abd], NAMESPACE_ID[xyz].  I think I will open an issue for this.

What do you think of changing the transaction ID in the next version to be something more straight forward? Maybe use something like java.util.UUID?

@milleruntime I don't have an opinion.  What is the motivation for the change? | What do you think of changing the transaction ID in the next version to be something more straight forward? Maybe use something like java.util.UUID?

@milleruntime I don't have an opinion. What is the motivation for the change?

I was just thinking that a stronger type could prevent coding errors like the one I made forgetting to call the method to display the value in HEX. | I was just thinking that a stronger type could prevent coding errors like the one I made forgetting to call the method to display the value in HEX.

Oh yeah, the code would probably be 10x better if we had a FateTxId type that we used instead of long.  Its toString method could consistently format for logging maybe.  That could possible encapsulate or extend UUID, but I think we would benefit form a more specific type than UUID or long. | I was just thinking that a stronger type could prevent coding errors like the one I made forgetting to call the method to display the value in HEX.

Oh yeah, the code would probably be 10x better if we had a FateTxId type that we used instead of long. Its toString method could consistently format for logging maybe. That could possible encapsulate or extend UUID, but I think we would benefit form a more specific type than UUID or long.

That seems like a bigger change than updating logging. Should that be considered as part of #2473 ? | That seems like a bigger change than updating logging. Should that be considered as part of #2473 ?

I was not thinking that change should be made on this PR.  Just seems like a good general change to make.  I think introducing a fate tx id type to replace long would be a nice change to make by itself for the purposes of review.","Add goal message to Fate. Closes #1249 | Update core/src/main/java/org/apache/accumulo/fate/Fate.java

Co-authored-by: Keith Turner <kturner@apache.org> | updates"
apache/accumulo,2986,https://github.com/apache/accumulo/pull/2986,Fix PropStoreConfigIT test failure,"This method was removed in #2985 because it returned a Thrift object and appeared to be return the same properties as getSystemConfiguration(). This commit readds the method back (but removes the Thrift object) as it only returns the properties set in ZK which is different than getSystemConfiguration() which also returns defaults.

This also fixes the test failure in PropStoreConfigIT",2022-09-30T14:01:30Z,cshannon,cshannon,clarity,"Is there a use case for this method? Or is it just to return the same properties set by the new method? | Is there a use case for this method? Or is it just to return the same properties set by the new method?

The use case is getting back what properties were set in ZK explicitly which is going to be useful when making configuration changes. The other method returns the entire configuration so you get back a couple hundred properties but most are going to be either values stored in the properties file or defaults. This allows you to find out exactly what was overridden so can be used to mutate using the new modify properties api. | I ran the PropStoreConfigIT test with these changes and it now passes | If you are going to add a new method, why not give the user more control with something like:
/**
   * Get specific set of system configuration properties.
   */
  Map getSystemConfiguration(ConfigurationType type);

We already have a thrift type for ConfigurationType so it could be new API Enum:
enum ConfigurationType {
    CURRENT, SITE, DEFAULT
  } | Oops, I thought it would pop up with an option to amend the commit message but I didn't see it so the commit message is wrong but this is obviously been rebased and merged. | Ah I was trying to rebase/squash but must have just picked the rebase only option as all 3 commits showed up. Sorry about that, I guess it's too late to fix as we obviously don't want to force push on main. | Is the functionality of the atomic updates covered in unit tests? If that coverage is sufficient, then this test may not be adding value and it seems odd to need to add thrift dependencies in an IT test.
I would prefer that the test find another way to test this functionality without a direct thrift call if at all feasible.

I don't see why we can't use thrift methods in the IT test but I just wanted to get the build working again since the test was broken. We could open up another issue to discuss the API changes around this further now that the test is fixed. | @ctubbsii - Anything we can do here to fix the history after my mistake (squash the 3 commits back to 1) or at this point is it too late since it's in main? | @ctubbsii - Anything we can do here to fix the history after my mistake (squash the 3 commits back to 1) or at this point is it too late since it's in main?

We should leave the history as-is, since the commits have already been pushed.","Re-add getProperties() to InstanceOperations

This method was removed in #2985 because it returned a Thrift object and
appeared to be return the same properties as getSystemConfiguration().
This commit readds the method back (but removes the Thrift object)
as it only returns the properties set in ZK which is different than
getSystemConfiguration() which also returns defaults. | Rename getSystemProperties() to getStoredProperties() | Revert readding getSystemProperties() and instead just fix failed test

Instead of exposing properties stored in ZK just update the test to use
the Thrift api to get the properties instead"
apache/accumulo,2792,https://github.com/apache/accumulo/pull/2792,closes #1377 - ensure all tables are checked ...,"... when removing references to candidates that are still in use.

This is done by getting tableIds from zookeeper at the start of the
reference scan and at the end of the reference scan.  During the
reference scans, the list of tableID is tracked.  After all candidates
that are still in use have been removed, there is a one more consitency
check to ensure we looked at all tables.",2022-09-22T14:50:46Z,dlmarion,dlmarion,clarity,"This is a cherry-pick of @mjwall 's GC fix on the 1.10 branch applied to 2.1 | I think all the logic is sound for this improvement but there is just some minor cleanup that could be done, most likely due to cherry picking and/or multiple contributors. | @keith-turner - I implemented your changes. I kind of expected a test failure, but that didn't happen.
Update: I found that I had to make a similar change in GarbageCollectionTest, now it fails.... | @dlmarion  I made some updatesto the test  in this commit keith-turner/accumulo@78cf05d to use DataLevel and to not have to add the metadata refs | @keith-turner  - I like it. Can you create a PR against my branch and I will merge it in? | Nevermind, I grabbed the diff and applied manually | So, I'm not convinced this is the right approach.

What's the implication of this statement? Are you saying that we need to go back to the drawing board? If I implement your suggested changes, is this good to go? | So, I'm not convinced this is the right approach.

What's the implication of this statement? Are you saying that we need to go back to the drawing board? If I implement your suggested changes, is this good to go?

Unless the edge cases for table creation/deletion are addressed (like paying attention to table states, like Keith mentioned), then this should definitely not be merged as is. And, I wouldn't recommend its inclusion in any forked version of 1.10 either, because of the potential harmful side-effects I mentioned.
If the table creation/deletion edge cases are addressed, I would have to re-review. I've only reviewed what's there, not what code changes have yet to be proposed.
I'm not sure what ""go[ing] back to the drawing board"" would entail. It feels like we're shooting in the dark, adding a sanity check against a problem that is not well-defined. | It feels like we're shooting in the dark, adding a sanity check against a problem that is not well-defined.

I think the validation outlined in #1377 is very well defined: If table ids w/ a certain state were seen in ZK before and after metadata scan then we must  see them in metadata OR something is really really wrong.   The less the GC trust that the persisted data it reads is correct and the more it tries to verify what it can about that data the more robust GC will be.   The GC already does a good bit of checks on the metadata table as it reads it.  Checking the metadata table against ZK will strengthen those existing verifications.
In addition to the check outlined in #1377, another check was added in #2293 for the case where table ids were seen in metadata table and none were seen in ZK.   This situation can legitimately happen when the total number of tables goes to/from zero concurrently with a GC cycle.  This situation could also happen when ZK read is silently failing and returning no data.  There is no way to distinguish between the two causes, so it can have false positives.  If the system is actually working correctly, then the legitimate case would be transient.   We could defer this second check and open an issue to do it another PR where we try to figure out how to avoid the false positive or make the check less noisy (like always skip GC cycle but log info at first and warn/error on subsequent cycles if seen again).  Or we could leave the check as-is in the PR with the knowledge that it can always be modified/removed in a bug fix release.  I lean towards leaving it and dealing with it if its problematic in a bug fix release. | @ctubbsii  - do these recent changes resolve the changes you requested? | Full IT passed except for the test failure identified in #2948","closes #1377 - ensure all tables are checked ...

... when removing references to candidates that are still in use.

This is done by getting tableIds from zookeeper at the start of the
reference scan and at the end of the reference scan.  During the
reference scans, the list of tableID is tracked.  After all candidates
that are still in use have been removed, there is a one more consitency
check to ensure we looked at all tables. | Address verify error, push up alternate getTableIds implemenation | Merge branch 'main' into 1377-gc-table-consistency-check-for-2.1 | Addressed most PR comments | Merge branch 'main' into 1377-gc-table-consistency-check-for-2.1 | Addressed PR comments | Updated ensureAllTablesChecked() based on feedback from Keith | Implement PR suggestions | Fixed up GarbageCollectionTest

Added metadata file references to each GarbageCollectionEnvironment
instance, updated TestGCE.getCandidateTableIDs() to match GCRun. | Applied Keith's changes to PR to use DataLevel instead of table name | Updates from PR comments

I updated GarbageCollectionEnvironment.getTableIDs to
return a map of table id to table state. I updated
GarbageCollectionEnvironment.getCandidateTableIDs to
filter out tables in the USER level that were not in
the online or offline state. Modified javadoc and
exception handling | Merge branch 'main' into 1377-gc-table-consistency-check-for-2.1 | Remove tables after adding them to the collection, not before | Capture NoNode exception when looking up state, set to UNKNOWN | Update GCRun to catch NoNodeException instead of check the code | Update GarbageCollectionTest based on PR feedback | Addressed comments from PR | Merge branch 'main' into 1377-gc-table-consistency-check-for-2.1 | Fix issue causing NPE in GC, add comment about new exception"
apache/accumulo,4024,https://github.com/apache/accumulo/pull/4024,Fix MemoryStarved ITs,"Rewrite MemoryConsumingIterator's method to compute the amount of memory to consume, so that:

* The implementation is more comprehensible
* Replace exception with bounds checking
* Avoid allocating more than necessary (a single byte is sufficient)
* The log message includes the amount of used memory detected
* The waiting message appears, even when memory was allocated, because that's the behavior
* Give the GC more time to detect the changed GC condition before trying to detect the low memory condition

Also, remove hard-coded comments for size of heap and incorrect interval frequency, and increase the configured free memory threshold, so that the memory percentage isn't so low, it doesn't get lower than the minimum that G1GC needs to do its job by default on a 256K VM.

This fixes #3868

Also include trivial fixes:

* Fix deprecation warning issues for getSplitCreationTime by making impl class deprecated instead of suppressing the interface deprecation, and use regular deprecations, not forRemoval=true, which complicates the way deprecations get inherited (see comments on #3977)
* Remove unused Logger",2023-12-06T19:46:38Z,ctubbsii,ctubbsii,comprehensible,"These changes finished a build with the full ITs, and these tests did not fail... they didn't even flake. They seem to pass consistently now with these changes. | As I mentioned here, I think the deprecation changes should be removed from this PR and included in a new PR that also includes the forRemoval annotations on the Deprecated annotations in the Property class. Having this a separate PR would allow us to discuss the merits of forRemoval and hopefully put this issue to rest. | I created #4032 for the forRemoval stuff.","Fix MemoryStarved ITs

Rewrite MemoryConsumingIterator's method to compute the amount of memory
to consume, so that:

* The implementation is more comprehensible
* Replace exception with bounds checking
* Avoid allocating more than necessary (a single byte is sufficient)
* The log message includes the amount of used memory detected
* The waiting message appears, even when memory was detected, because
  that's the behavior
* Give the GC more time to detect the changed GC condition before trying
  to detect the low memory condition

Also, remove hard-coded comments for size of heap and incorrect interval
frequency, and increase the configured free memory threshold, so that
the memory percentage isn't so low, it doesn't get lower than the
minimum that G1GC needs to do its job by default on a 256K VM.

This fixes #3868

Also include trivial fixes:

* Fix deprecation warning issues for getSplitCreationTime by making impl
  class deprecated instead of suppressing the interface deprecation, and
  use regular deprecations, not forRemoval=true, which complicates the
  way deprecations get inherited (see comments on #3977)
* Remove unused Logger | Apply suggestions from code review"
apache/accumulo,2347,https://github.com/apache/accumulo/pull/2347,New IteratorMincClassCastBugIT and fixes to #1411,"* Create new IT for testing runtime bug that was introduced when moving
the iterators to declare them public API in #1411. The other changes in
this commit are for fixing the bug.
* Drop new class InterruptibleMapIterator that was created in #1411
* Revert SortedMapIterator back to previous implementation where it
was an InterruptibleIterator but keep it in core.iterators pkg
* Move InterruptibleIterator interface to API and add javadoc
* Move ColumnFamilySkippingIterator to API
* Add javadoc to IterationInterruptedException
* Fixes #2341",2021-11-10T20:59:54Z,milleruntime,milleruntime,comprehensible,"@Manno15 @ctubbsii thanks for taking a look at this. I guess I should have been more explicit or made this a draft PR because a lot of the code in your feedback was just yanked from the examples repo, specifically the ChunkCombiner to try and simulate what Datawave is doing with iterators. I am not sure if I should clean it up or just stick it in a test jar so we don't have to look at it.
Do either of you know how to get MiniAccumuloCluster to load classes from a jar during runtime? I thought it might be a better test to do that but couldn't get it working. | @ctubbsii @Manno15 This PR changed significantly since your initial reviews. I moved the iterators back to the iteratorsImpl package, along with moving SortedMapIterator there. The more I thought about it, the less I liked including InterruptibleIterator in the public API. We created the YieldingKeyValueIterator interface with the intention of public use so we don't need InterruptibleIterator in the API. I also rewrote the IT to just use a simple iterator that will still cause the class cast exception to happen without the fix. | Changes look good, I don't fully know the impact of the interuptible changes but I didn't run into any issues while testing. Confirmed that these changes to fix the reported issue.

Thanks for testing these changes. Just curious, how did you reproduce the issue? | Thanks for testing these changes. Just curious, how did you reproduce the issue?

Using the IT you created. So maybe that isn't a definitive test but it did fail against main while working with the rest of these changes.","New IteratorMincClassCastBugIT and Partial revert of #1411

* Create new IT for testing runtime bug that was introduced when moving
the iterators to declare them public API in #1411. The other changes in
this commit are for fixing the bug.
* Drop new class created InterruptibleMapIterator
* Revert SortedMapIterator back to previous implementation where it
was an InterruptibleIterator but keep it in core.iterators pkg
* Move InterruptibleIterator interface to API and add javadoc
* Move ColumnFamilySkippingIterator to API
* Add javadoc to IterationInterruptedException | Fix for secbugs | Update test/src/main/java/org/apache/accumulo/test/functional/IteratorMincClassCastBugIT.java

Co-authored-by: Jeffrey Manno <jeffreymanno15@gmail.com> | Move iterators back to system

* Move SortedMapIterator to iteratorsImpl | Rewrite IteratorMincClassCastBugIT | Clean up | Move IterationInterruptedException to iteratorsImpl"
gradle/gradle,32151,https://github.com/gradle/gradle/pull/32151,Deprecate internal load-after-store flag,Fixes https://github.com/gradle/gradle/issues/31839,2025-01-24T15:25:43Z,alllex,alllex,"readability, readable","@bot-gradle test this | I've triggered the following builds:

Pull Request Feedback (Trigger) (Check)

Build Scan | The following builds have passed:

Pull Request Feedback (Trigger) (Check)

Build Scan","Deprecate internal load-after-store flag

nagging whenever the flag is set, even if the value matches the default.
The flag is planned for removal in 9.0

The computation of the loadAfterStore parameter is made lazy, so that the DeprecationLogger has enough time to configure the problem reporting. Otherwise, the deprecation nagging is ignored silently.
The order of the computation is also changed, so that the nagging is unconditional, rather than being dependent on the short-circuiting of boolean expressions. | Test deprecation of load-after-store | Fix formatting | Polish deprecation message in test

for readability"
gradle/gradle,28218,https://github.com/gradle/gradle/pull/28218,Improve capability conflict tracking,,2024-03-07T15:18:39Z,bot-gradle,ljacomet,"readability, readable","This is missing a test but has been validated as fixing

ljacomet/logging-capabilities#20 | Test has been added | @bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

PullRequestFeedback build | I ran this against some of the tests here: #28266
This change seems to fix a couple of them | @bot-gradle merge | The merge queue build has started. Click here to see all failures if any. | The merge queue build has started. Click here to see all failures if any. | This ended up being reverted, see #29208","Refactor capability conflict tracking

* Track based on capability ID instead of conflict identity
* Replace previous conflict for the same capability by the new one as it
 must be a superset. | Remove dead code | Add test for ordering issue in resolution

Includes existing code cleanup"
gradle/gradle,27344,https://github.com/gradle/gradle/pull/27344,Allow disabling `gradle init` dialog,"Fixes https://github.com/gradle/gradle/issues/24624 by introducing `--use-defaults` flag, which assumes the default answer is accepted to all interactive questions asked in the `gradle init` dialog",2023-12-13T19:49:54Z,bot-gradle,alllex,"readability, readable","🥷 Code experts: tresat, adammurdoch
tresat, lkasso have most 👩‍💻 activity in the files.
adammurdoch has most 🧠 knowledge in the files.
 
 See details
platforms/documentation/docs/src/docs/dsl/org.gradle.buildinit.tasks.InitBuild.xml
Activity based on git-commit:




tresat
lkasso




DEC




NOV

71 additions & 0 deletions


OCT




SEP




AUG




JUL





Knowledge based on git-blame:
platforms/documentation/docs/src/docs/release/notes.md
Activity based on git-commit:




tresat
lkasso




DEC




NOV

379 additions & 0 deletions


OCT




SEP




AUG




JUL





Knowledge based on git-blame:
platforms/software/build-init/src/integTest/groovy/org/gradle/buildinit/plugins/BuildInitInteractiveIntegrationTest.groovy
Activity based on git-commit:




tresat
lkasso




DEC




NOV




OCT




SEP
279 additions & 0 deletions



AUG




JUL





Knowledge based on git-blame:
adammurdoch: 49%
platforms/software/build-init/src/integTest/groovy/org/gradle/buildinit/plugins/BuildInitPluginIntegrationTest.groovy
Activity based on git-commit:




tresat
lkasso




DEC




NOV




OCT




SEP
463 additions & 0 deletions



AUG




JUL





Knowledge based on git-blame:
adammurdoch: 29%
platforms/software/build-init/src/main/java/org/gradle/buildinit/plugins/BuildInitPlugin.java
Activity based on git-commit:




tresat
lkasso




DEC




NOV




OCT




SEP
143 additions & 0 deletions



AUG




JUL





Knowledge based on git-blame:
adammurdoch: 22%
platforms/software/build-init/src/main/java/org/gradle/buildinit/plugins/internal/BuildInitializer.java
Activity based on git-commit:




tresat
lkasso




DEC




NOV




OCT




SEP
84 additions & 0 deletions



AUG




JUL





Knowledge based on git-blame:
adammurdoch: 53%
platforms/software/build-init/src/main/java/org/gradle/buildinit/tasks/InitBuild.java
Activity based on git-commit:




tresat
lkasso




DEC




NOV




OCT
1 additions & 1 deletions



SEP
450 additions & 0 deletions



AUG




JUL





Knowledge based on git-blame:
adammurdoch: 21%
platforms/software/build-init/src/test/groovy/org/gradle/buildinit/tasks/InitBuildSpec.groovy
Activity based on git-commit:




tresat
lkasso




DEC




NOV




OCT




SEP
338 additions & 0 deletions



AUG




JUL





Knowledge based on git-blame:
adammurdoch: 15%

To learn more about /:\ gitStream - Visit our Docs | @bot-gradle merge | WARN: This is a feature pull request but no changes to release note found. | Your PR is queued. See the queue page for details. | I've triggered a build for you. Click here to see all failures if there's any.",Allow disabling InitBuild dialog | Add release notes | Improve javadoc | Use property injection | Improve existing javadocs | Rename `--no-dialog` to `--use-defaults` | Fix unit tests
gradle/gradle,27284,https://github.com/gradle/gradle/pull/27284,Improve readability of inferred version catalog values in javadocs,"Current javadocs generated for version catalog accessors are rendered as a monotone single paragraph text, making it hard to get a value out of looking at them. At the same time, they already contain useful information inferred from the values in the actual version catalog.

This PR improves the generated javadocs by
- consistently marking all inferred values as bold
- rewording the scaffolding sentences to make them more concise and clear
- splitting each javadoc into multiple paragraphs, keeping different information visually separated
- provides a sensible javadoc for empty bundles
- fixes the indentation of the generated accessor sources

## Before

Versions
<img width=""700"" alt=""image"" src=""https://github.com/gradle/gradle/assets/2759152/912966bd-a51c-41cd-9b1f-8af3c825cc35"">

Libraries
<img width=""700"" alt=""image"" src=""https://github.com/gradle/gradle/assets/2759152/ed482c55-9150-4417-ab10-bf8a0bec3d87"">

Plugins
<img width=""700"" alt=""image"" src=""https://github.com/gradle/gradle/assets/2759152/61070cea-7600-4827-938d-1282f4bf373d"">


## After

Versions
<img width=""699"" alt=""image"" src=""https://github.com/gradle/gradle/assets/2759152/de0db4d9-e414-41a2-a0ac-33160476d57c"">


Libraries
<img width=""700"" alt=""image"" src=""https://github.com/gradle/gradle/assets/2759152/5f7c34b0-4eac-47bd-93a4-0fb5e8eea015"">

Plugins
<img width=""700"" alt=""image"" src=""https://github.com/gradle/gradle/assets/2759152/b8eda2b6-97ac-4ef0-aa0e-4547245c792b"">

",2023-12-06T17:06:17Z,bot-gradle,alllex,"readability, readable","@bot-gradle test without pts | I've triggered the following builds with parameters: -DenablePredictiveTestSelection=false for you. Click here to see all build failures.

PullRequestFeedback build | 🥷 Code experts: tresat
tresat has most 👩‍💻 activity in the files.
 
 See details
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/catalog/DefaultVersionCatalogBuilder.java
Activity based on git-commit:




tresat




DEC



NOV



OCT
655 additions & 0 deletions


SEP



AUG



JUL




Knowledge based on git-blame:
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/catalog/LibrariesSourceGenerator.java
Activity based on git-commit:




tresat




DEC



NOV



OCT
817 additions & 0 deletions


SEP



AUG



JUL




Knowledge based on git-blame:
platforms/software/dependency-management/src/test/groovy/org/gradle/api/internal/catalog/LibrariesSourceGeneratorTest.groovy
Activity based on git-commit:




tresat




DEC



NOV



OCT
501 additions & 0 deletions


SEP



AUG



JUL




Knowledge based on git-blame:

To learn more about /:\ gitStream - Visit our Docs | @bot-gradle merge | Your PR is queued. See the queue page for details. | I've triggered a build for you. Click here to see all failures if there's any.",Polish generated version catalog javadocs
gradle/gradle,22399,https://github.com/gradle/gradle/pull/22399,Make SoftwareComponentInternal finalizable,"Fixes: #20581
",2022-11-29T10:52:10Z,bot-gradle,xaviarias,"readability, readable","@bot-gradle test this | OK, I've already triggered the following builds for you:

PullRequestFeedback build | A couple things on the approach proposed:

You can ignore the native plugins for this work.


Ok I will do that 👍🏼


Any solution will have to cope with a deprecation period. We cannot make such a change without warning for a while first, and then break on the next major. But a first attempt could focus on the breaking behaviour to understand the problem space better.


Understood, I will add the deprecation after finishing the code changes 👍🏼


Why not add methods to SoftwareComponentInternal?


Indeed the method can be specified at SoftwareComponentInternal, but if we ignore the native plugins we'll have to add a default implementation. That could be just an empty method.

Also for me, the complexity is not in the API to indicate a value has been finalized, it is in checking when this needs to be made and if there are no unforseen consequences when trying to do that.

Ok I will explore and develop more on this direction. | @bot-gradle test and merge | OK, I've already triggered a build for you. | Removed milestone as this PR was reverted in #22900","WIP Make SoftwareComponentInternal finalizable

Fixes: #20581 | Fix sanity check codestyle | Make DefaultSwiftStaticLibrary finalizable | Ignore native plugins for this issue | WIP Check for component finalization | WIP Invoke deprecation logger instead of throwing | Add user guide entry for deprecations | Finalize component and delete abstract class | Improve issue docs and fix formatting | Improve warning message and documentation | Add integration tests and fix Ivy publication | Fix CodeNarc reported errors | Update suggestions

Co-authored-by: nate contino <ncontino@gradle.com> | Replace realization by population

Co-authored-by: nate contino <ncontino@gradle.com> | Update suggestion: reword to reduce wordiness

Co-authored-by: nate contino <ncontino@gradle.com> | Co-authored-by: nate contino <ncontino@gradle.com>"
gradle/gradle,22954,https://github.com/gradle/gradle/pull/22954,"Refactor dependency management for readability, clarity on theoretical example","<!--- The issue this PR addresses -->
- https://github.com/gradle/gradle/issues/20640
- Readability improvements across the board

### Context
- Rephrases ""example project"" explanation to avoid confusion (there is no project, just a graphic)
- General best-practice documentation sentence and section rewrites for clarity and readability

### Current Page
https://docs.gradle.org/current/userguide/core_dependency_management.html

### Rendered Docs Preview of Changes
https://builds.gradle.org/repository/download/Gradle_Master_Check_BuildDistributions/59052397:id/distributions/gradle-8.0-docs.zip!/gradle-8.0-20221202201707%2B0000/docs/userguide/core_dependency_management.html

### Contributor Checklist
- [X] [Review Contribution Guidelines](https://github.com/gradle/gradle/blob/master/CONTRIBUTING.md)
- [X] Make sure that all commits are [signed off](https://git-scm.com/docs/git-commit#Documentation/git-commit.txt---signoff) to indicate that you agree to the terms of [Developer Certificate of Origin](https://developercertificate.org/).
- [X] Make sure all contributed code can be distributed under the terms of the [Apache License 2.0](https://github.com/gradle/gradle/blob/master/LICENSE), e.g. the code was written by yourself or the original code is licensed under [a license compatible to Apache License 2.0](https://apache.org/legal/resolved.html).
- [X] Check [""Allow edit from maintainers"" option](https://help.github.com/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork/) in pull request so that additional changes can be pushed by Gradle team
- [X] Provide integration tests (under `<subproject>/src/integTest`) to verify changes from a user perspective
- [X] Provide unit tests (under `<subproject>/src/test`) to verify logic
- [X] Update User Guide, DSL Reference, and Javadoc for public-facing changes
- [X] Ensure that tests pass sanity check: `./gradlew sanityCheck`
- [X] Ensure that tests pass locally: `./gradlew <changed-subproject>:quickTest`

### Gradle Core Team Checklist
- [ ] Verify design and implementation 
- [ ] Verify test coverage and CI build status
- [ ] Verify documentation
",2022-12-08T15:43:49Z,bot-gradle,nathan-contino,"readability, readable, clarity","Copy edit - LGTM
Much improved readability | I find it confusing that there is theoretical project illustrated below: and below that sentence, there is an image Dependency management big picture that has no relation to that theoretical project. And the whole explanation with a theoretical project is confusing as well, very abstract. But it's much better after the update, the previous version was really hard to follow and included whole dependencies terminology. | @asodja good point, the image here really doesn't describe much. Do you think it adds value, or should we remove it from the page? | I think it adds some value, just the position is incorrect so it makes the example confusing.
But there could be a short paragraph, where elements of the picture are explained (I guess it shows how Gradle fetches dependencies from repositories and where it stores them locally). | @bot-gradle test BD | OK, I've already triggered the following builds for you:

BuildDistributions build | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | OK, I've already triggered a build for you.","Refactor dependency management for readability, clarity on theoretical example | Remove example entirely

Co-authored-by: Amanda L Martin <hythloda@gmail.com> | moving image, add words"
gradle/gradle,25284,https://github.com/gradle/gradle/pull/25284,Remove as much internal usage of ResolvedConfiguation as possible,"This removes internal usage and test usage of the `ResolvedConfiguration` type as much as is possible.  

In parameterized tests that explore multiple resolution patterns, when alternatives to `ResolvedConfiguration` are **NOT** present, the `ResolvedConfiguration` is replaced with `ArtifactView`s.  When use of `ArtifactView` is already present, these tests were left as is - when `RC` is removed, they should become simple deletions of these cases.

We cannot easily deprecate this API yet, but this helps migrate away from internal usage.",2023-06-21T13:10:01Z,bot-gradle,tresat,"readability, readable, easier to read","I think I've addressed all the comments here. | @bot-gradle test this | I've triggered the following builds for you:

PullRequestFeedback build | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | I've triggered a build for you. Click here to see all failures if there's any.","Remove as much internal usage of ResolvedConfiguation as possible

This removes internal usage and test usage as much as is possible.  When alternative to ResolvedConfiguration are NOT present when testing multiple ways of resolving, the ResolvedConfiguration is replaced with ArtifactViews.  When use of ArtifactView was already present, these tests were left as is - when RC is removed, they should become simple case deletions. | This test needs to remain, the ordering is only correct for legacy resolution via ResolvedConfiguration | Remove unnecessary comment | Make use of root resolution result access | Update docs | Remove unnecessary usage of lenient | Use idiomatic-er Spock parameters | Replace unnecessary AV with direct usage of incoming artifacts | Simplify test by removing no longer necessary block | Simplify files getting | Remove logging injection and checking | Test each parameter again as was probably intended here | Remove unused imports | Leave test as CC incompatible to avoid dependency resolution during configuration phase | Use custom task to avoid resolution at configuration time | Use custom task to avoid resolution at configuration time | Avoid resolution during configuration phase, making test CC-incompatible | Restore original intent of test class to verify lenientConf = lenient artifactView | Use resolved deps instead of declared ones | Make tests CC compatible

ResolutionResult is not CC compatible, but its rootComponent property is | Revert changes to integration test

We need to continue to check that we produce a good error message for ResolvedConfiguration. | Remove unnecessary logging dependency | Clarify parameter role in javadoc | Restore original indentation | Restore original file contents | Merge branch 'master' into tt/83/resolved-dependency-minimize-internal-usage | Relax test expectation"
gradle/gradle,23179,https://github.com/gradle/gradle/pull/23179,allow labels for gradle wrapper task versions,"permitted labels:
- ""latest""
- ""release-candidate""
- ""nightly""
- ""release-nightly""

- Implements: #13504

Signed-off-by: Reinhold Degenfellner <rdegenfellner@gradle.com>",2022-12-19T07:50:03Z,bot-gradle,reinsch82,"readability, readable","@bot-gradle test and merge | No milestone for this PR. Please set a milestone and retry. | @bot-gradle test and merge | OK, I've already triggered a build for you. | A follow up issue should be created about supporting milestone. | @reinsch82 did you get to create the follow up issue about supporting milestone?
We should also review how our custom wrapper tasks in the gradle/gradle build could be rewritten using this new feature. See https://github.com/gradle/gradle/blob/master/build-logic/build-update-utils/src/main/kotlin/gradlebuild.wrapper.gradle.kts | @eskatos just did

#23369

should I create an issue for the custom wrapper adaption? | Thanks!
A direct PR changing our build logic would be fine for our custom wrapper tasks.","allow labels for gradle wrapper task versions

permitted labels:
- ""latest""
- ""release-candidate""
- ""nightly""
- ""release-nightly""

Implements: #13504
Signed-off-by: Reinhold Degenfellner <rdegenfellner@gradle.com>

allow labels for gradle wrapper task versions

permitted labels:
- ""latest""
- ""release-candidate""
- ""nightly""
- ""release-nightly""

Implements: #13504
Signed-off-by: Reinhold Degenfellner <rdegenfellner@gradle.com> | fix mapping ""latest"" to /versions/current

Implements: #13504
Signed-off-by: Reinhold Degenfellner <rdegenfellner@gradle.com>

allow labels for gradle wrapper task versions

permitted labels:
- ""latest""
- ""release-candidate""
- ""nightly""
- ""release-nightly""

Implements: #13504
Signed-off-by: Reinhold Degenfellner <rdegenfellner@gradle.com> | clean up visibility and quotes

Implements: #13504

Signed-off-by: Reinhold Degenfellner <rdegenfellner@gradle.com> | fix GradleVersionResolverTest

Implements: #13504

Signed-off-by: Reinhold Degenfellner <rdegenfellner@gradle.com> | update doc

Implements: #13504

Signed-off-by: Reinhold Degenfellner <rdegenfellner@gradle.com> | some minor doc housekeeping

Implements: #13504

Signed-off-by: Reinhold Degenfellner <rdegenfellner@gradle.com> | fix missing and unclear doc

Implements: #13504

Signed-off-by: Reinhold Degenfellner <rdegenfellner@gradle.com> | add release notes for wrapper version labels

Implements: #13504

Signed-off-by: Reinhold Degenfellner <rdegenfellner@gradle.com> | doc fixes

Implements: #13504

Signed-off-by: Reinhold Degenfellner <rdegenfellner@gradle.com>"
gradle/gradle,21350,https://github.com/gradle/gradle/pull/21350,Add configuration cache info to performance docs page,"<!--- The issue this PR addresses -->
The ""Improving the Performance of Gradle Builds"" page doesn't currently mention the configuration cache at all. Adding a small section on it. Additionally, I tweaked the wording of a bulleted list of inputs that impact the configuration cache that now appears on both the performance and the configuration cache docs pages (readability improvement).

### Context
We'd like to surface the configuration cache in trainings and developer advocacy programs soon, so I'm adding it to this page to make it more discoverable.

### Contributor Checklist
- [X] [Review Contribution Guidelines](https://github.com/gradle/gradle/blob/master/CONTRIBUTING.md)
- [X] Make sure that all commits are [signed off](https://git-scm.com/docs/git-commit#Documentation/git-commit.txt---signoff) to indicate that you agree to the terms of [Developer Certificate of Origin](https://developercertificate.org/).
- [X] Make sure all contributed code can be distributed under the terms of the [Apache License 2.0](https://github.com/gradle/gradle/blob/master/LICENSE), e.g. the code was written by yourself or the original code is licensed under [a license compatible to Apache License 2.0](https://apache.org/legal/resolved.html).
- [X] Check [""Allow edit from maintainers"" option](https://help.github.com/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork/) in pull request so that additional changes can be pushed by Gradle team
- [N/A] Provide integration tests (under `<subproject>/src/integTest`) to verify changes from a user perspective
- [N/A] Provide unit tests (under `<subproject>/src/test`) to verify logic
- [X] Update User Guide, DSL Reference, and Javadoc for public-facing changes
- [X] Ensure that tests pass sanity check: `./gradlew sanityCheck`
- [X] Ensure that tests pass locally: `./gradlew <changed-subproject>:quickTest`

### Gradle Core Team Checklist
- [ ] Verify design and implementation 
- [ ] Verify test coverage and CI build status
- [ ] Verify documentation
- [ ] Recognize contributor in release notes
",2022-07-27T14:20:45Z,bot-gradle,nathan-contino,"readability, readable","Staged preview of the docs content: https://builds.gradle.org/repository/download/Gradle_Master_Check_BuildDistributions/54315299:id/distributions/gradle-7.6-docs.zip!/gradle-7.6-20220727132851%2B0000/docs/userguide/performance.html | @bot-gradle test BD | OK, I've already triggered the following builds for you:

BuildDistributions build | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | OK, I've already triggered a build for you.",Add configuration cache info to performance docs page | Update subprojects/docs/src/docs/userguide/running-builds/performance.adoc | small wording revisions | adam suggestions
gradle/gradle,20353,https://github.com/gradle/gradle/pull/20353,"Rewrites comment to correct typo: word ""loose""","Rewrites comment to read better
Rewrites comment to correct typo: word ""loose""

Signed-off-by: T-A-B <78736596+T-A-B@users.noreply.github.com>

<!--- The issue this PR addresses -->
Fixes #?  Comment language is ineffective.

### Context
<!--- Why do you believe many users will benefit from this change? -->
Readability is increased which will allow for faster comprehension of the edited file.

<!--- Link to relevant issues or forum discussions here -->
N/A

### Contributor Checklist
- [ x] [Review Contribution Guidelines](https://github.com/gradle/gradle/blob/master/CONTRIBUTING.md)
- [ x] Make sure that all commits are [signed off](https://git-scm.com/docs/git-commit#Documentation/git-commit.txt---signoff) to indicate that you agree to the terms of [Developer Certificate of Origin](https://developercertificate.org/).
- [ x] Make sure all contributed code can be distributed under the terms of the [Apache License 2.0](https://github.com/gradle/gradle/blob/master/LICENSE), e.g. the code was written by yourself or the original code is licensed under [a license compatible to Apache License 2.0](https://apache.org/legal/resolved.html).
- [ x] Check [""Allow edit from maintainers"" option](https://help.github.com/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork/) in pull request so that additional changes can be pushed by Gradle team
- [ x] Provide integration tests (under `<subproject>/src/integTest`) to verify changes from a user perspective
- [x ] Provide unit tests (under `<subproject>/src/test`) to verify logic
- [x ] Update User Guide, DSL Reference, and Javadoc for public-facing changes
- [ x] Ensure that tests pass sanity check: `./gradlew sanityCheck`
- [x ] Ensure that tests pass locally: `./gradlew <changed-subproject>:quickTest`

### Gradle Core Team Checklist
- [ ] Verify design and implementation 
- [ ] Verify test coverage and CI build status
- [ ] Verify documentation
- [ ] Recognize contributor in release notes
",2022-04-06T15:05:21Z,bot-gradle,T-A-B,"readability, readable","@bot-gradle test and merge | Your PR is queued. See the queue page for details. | OK, I've already triggered a build for you.","Rewrites comment to correct typo: word ""loose""

Rewrites comment to read better

Signed-off-by: T-A-B <78736596+T-A-B@users.noreply.github.com> | Rewrites comment to correct typo: word ""loose""

Rewrites comment to read better

Signed-off-by: T-A-B <78736596+T-A-B@users.noreply.github.com> | Merge remote-tracking branch 'origin/master'"
gradle/gradle,20031,https://github.com/gradle/gradle/pull/20031,Make the symmetric nullable check clearer,"This check was extremely opaque, relying on the implicit unique nature of collecting to a set to determine if something was both null and non-null. @big-guy and I had trouble decoding it while debugging another PR. This should be a marked readability improvement, as well as a little bit faster since there's no boxing or set creation.",2022-02-24T22:19:24Z,bot-gradle,octylFractal,"readability, readable","@bot-gradle test and merge | OK, I've already triggered a build for you.",Make the symmetric nullable check clearer
gradle/gradle,18990,https://github.com/gradle/gradle/pull/18990,Refactor DefaultRootComponentMetadataBuilder to use Factory Pattern,Application of the Dependency Inversion Principle against the `DefaultRootComponentMetadataBuilder`,2021-11-16T00:34:27Z,bot-gradle,JLLeitschuh,"readability, readable","@bot-gradle test AST | OK, I've already triggered AllSmokeTestsReadyForMerge build for you. | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | OK, I've already triggered a build for you.","Refactor DefaultRootComponentMetadataBuilder to use Factory Pattern

Signed-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gradle.com>"
gradle/gradle,18834,https://github.com/gradle/gradle/pull/18834,Outputs from Test tasks are exposed as variants to dependency management,"<!--- The issue this PR addresses -->
Fixes 18791

First draft of work for the Associated Spec for review.",2021-11-16T01:26:24Z,bot-gradle,tresat,"readability, readable, easier to read","OK, I've already triggered a build for you. | Pre-tested commit build failed. | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | OK, I've already triggered a build for you.","Add new Attributes and Attribute Values for use with test task outputs. | Fix cut and paste errors. | TestSuiteType Attribute definition. | Make new Attribute interfaces Named. | Create coverageDataElements variant for test tasks from JacocoPlugin - demonstrate issue with configuration order. | fix: Temporary fix for broken test. | Workaround until attributes are lazily configurable. | Add tests for resolving jacoco data via configs. | Add type definition for binary artifacts. | feat: Set binary data artifact type on jacoco binary data in coverageData config. | feat: Binary test data variants added when adding test suites. | test: Improve indentation style and correct tests now that artifact type is reported as binary. | style: Change how jacoco test results file is referenced. | test: Finish tests for the binary test data variant. | style: Clean spacing. | chore: Update comments. | feat: Java Plugin producing mainSourceElements variant. | style: Clean unused imports. | test: Fix outgoing variants test to expect new variants. | test: Restore lazy configuration of test task, which outgoing test variant was forcing to realize. | test: Fix broken tests due to additional variants. | feat: Make variant properly use lazy API to link to test task results. | feat: Fix Jacoco Plugin to use lazy API better, mark built by on output. | test: Fix test which was running code at config time instead of execution time - issue revealed by new configurations. | test: Fix tests for new variants. | test: Workaround for failing CC tests. | test: Workaround for failing Project Isolation tests. | feat: First attempt at adding Providers to AttributeContainer - finalizing all attributes upon any access to stored values.  This results in many failing tests, outside of the few corrected here by calling asImmutable() to create a temp container to check values against. | feat: Have the DefaultMutableAttributeContainer realize attributes upon calling get() for that attribute. | test: Fix broken core tests. | feat: Add test type and sourceSetDirectories artifacts to variants lazily; restore commented tests which now pass.

Includes a workaround for supplying ObjectFactory to DefaultConfigurationPublications which will need to be addressed. | style: Rename variable to reflect actual type. | test: Add a unit test to ensure a provider of artifacts works. | test: Fix unit test confirming lazy behavior of adding artifact providers. | style: Fix whitespace for checkstyle. | style: Fix imports for checkstyle. | refactor: Remove hacky workaround. | fix: Prevent StackOverflow when realizing lazy attributes. | test: Ingore strange test failure. | feat: Add fail-fast type checking for attribute types matching their values or value providers. | refactor: Rename cache to factory, to better describe its intent. | fix: Workaround for test issue. | Merge branch 'master' into tt/74/outputs-from-test-tasks-exposed-as-variants-18791 | fix: Fix issues with test source being grabbed before tasks which generate it complete. | fix: Add additional type to provider-accepting attribute method to disambiguate it in the Kotlin DSL, also check against that additional type as well to fail-faster. | test: Update test broken due to error message change. | Account for Windows paths in output when running tests on CI. | Revert ""refactor: Remove hacky workaround.""

This reverts commit dd4aa621d01647ce6d2cc5acc550ca3c93c71d19. | fix: Remove unnecessary workaround for building sources output for buildSrc. | fix: Use alternate way to get name for PublishArtifact if name is null, this fixes tests for ArtifactTransforms. | fix: Window path issues. | Refactor: reverse 3rd param fix for broken test by supplying type hint instead of using extension method. | test: Fix Windows path checks. | fix: Attributes in Kotlin. | fix: All the remaining attributes named 1-arg versions in Kotlin code. | test: Fix test broken due to change in attribute() method. | test: Fix some path issues on Windows causing tests to fail. | test: Fix missed path issue on Windows causing tests to fail. | test: Mark tests as not configuration-cacheable. | test: Fix and ignore some tests to pass Configuration Cache checks. | fix: Clean some missed Kotlin objects.named() issues with ambiguous calls, removed namedAttributes() which causes the same problems. | style: Simplify sample property setup in tests. | test: Use explicit project name. | fix: Updated error message for clarity. | fix: Update sample to use new attributes provided by plugins. | fix: Make jvm-multi-project-with-code-coverage sample work with new variants. | test: Fix Samples, add transitive dependency tests. | doc: Fix spelling.

Co-authored-by: Sterling Greene <big-guy@users.noreply.github.com> | test: Set explicit project names. | test: Set explicit project names. | test: Make comparisons to partial file paths clearer by building relative paths to base test directory with new utility method. | fix: Remove unnecessary setter. | fix: Set TestType convention in plugin, not during Suite initialization. | test: Hard code paths to enable configuration caching. | Merge branch 'master' into tt/74/outputs-from-test-tasks-exposed-as-variants-18791 | fix: Remove the need to pass ObjectFactory before adding multiple artifacts lazily. | fix: Update name of test results variant. | docs: Improve javadoc.

Co-authored-by: Sterling Greene <big-guy@users.noreply.github.com> | fix: Remove unnecessary indirect access to plugin. | doc: Correct javadoc. | style: Be groovier in assertion. | style: Compress artifact definition and configuration.

Co-authored-by: Sterling Greene <big-guy@users.noreply.github.com> | Merge branch 'tt/74/outputs-from-test-tasks-exposed-as-variants-18791' of https://github.com/gradle/gradle into tt/74/outputs-from-test-tasks-exposed-as-variants-18791 | style: Compress attribute mapping.

Co-authored-by: Sterling Greene <big-guy@users.noreply.github.com> | style: Clean imports. | feat: Merge branch 'master' into tt/74/outputs-from-test-tasks-exposed-as-variants-18791, and resolve merge conflicts.

# Conflicts:
#	build-logic-commons/code-quality/src/main/kotlin/gradlebuild.code-quality.gradle.kts
#	build-logic/buildquality/src/main/kotlin/gradlebuild.configuration-cache-report.gradle.kts
#	build-logic/buildquality/src/main/kotlin/gradlebuild.incubation-report-aggregation.gradle.kts
#	build-logic/buildquality/src/main/kotlin/gradlebuild.incubation-report.gradle.kts
#	build-logic/integration-testing/src/main/kotlin/gradlebuild.test-fixtures.gradle.kts
#	build-logic/integration-testing/src/main/kotlin/gradlebuild/integrationtests/shared-configuration.kt
#	build-logic/jvm/src/main/kotlin/gradlebuild.launchable-jar.gradle.kts
#	build-logic/jvm/src/main/kotlin/gradlebuild.unittest-and-compile.gradle.kts
#	build-logic/packaging/src/main/kotlin/gradlebuild.distributions.gradle.kts
#	build-logic/packaging/src/main/kotlin/gradlebuild.shaded-jar.gradle.kts
#	build-logic/profiling/src/main/kotlin/gradlebuild.jmh.gradle.kts
#	build-logic/publishing/src/main/kotlin/gradlebuild.kotlin-dsl-plugin-bundle.gradle.kts
#	build-logic/publishing/src/main/kotlin/gradlebuild.publish-public-libraries.gradle.kts
#	subprojects/configuration-cache/build.gradle.kts
#	subprojects/configuration-cache/src/integTest/groovy/org/gradle/configurationcache/ConfigurationCacheJavaIntegrationTest.groovy
#	subprojects/core/src/main/java/org/gradle/api/internal/attributes/DefaultMutableAttributeContainer.java
#	subprojects/core/src/test/groovy/org/gradle/api/internal/attributes/DefaultMutableAttributeContainerTest.groovy
#	subprojects/dependency-management/src/integTest/groovy/org/gradle/integtests/resolve/attributes/LazyAttributesIntegrationTest.groovy
#	subprojects/dependency-management/src/integTest/groovy/org/gradle/integtests/resolve/constraints/DependencyConstraintsIntegrationTest.groovy
#	subprojects/docs/src/samples/build-organization/structuring-software-projects/kotlin/build-logic/commons/src/main/kotlin/com.example.jacoco.gradle.kts
#	subprojects/docs/src/samples/build-organization/structuring-software-projects/kotlin/build-logic/report-aggregation/src/main/kotlin/com.example.report-aggregation.gradle.kts
#	subprojects/docs/src/samples/java/jvm-multi-project-with-code-coverage/kotlin/buildSrc/src/main/kotlin/myproject.jacoco-aggregation.gradle.kts
#	subprojects/docs/src/samples/java/jvm-multi-project-with-code-coverage/kotlin/buildSrc/src/main/kotlin/myproject.java-conventions.gradle.kts
#	subprojects/docs/src/snippets/dependencyManagement/customizingResolution-attributeSubstitutionRule/kotlin/consumer/build.gradle.kts
#	subprojects/docs/src/snippets/dependencyManagement/customizingResolution-ivyMetadataRule/kotlin/build.gradle.kts
#	subprojects/docs/src/snippets/dependencyManagement/customizingResolution-metadataRule/kotlin/build.gradle.kts
#	subprojects/docs/src/snippets/dependencyManagement/modelingFeatures-crossProjectPublications-advanced-published/kotlin/buildSrc/src/main/kotlin/com/acme/InstrumentedJarsPlugin.kt
#	subprojects/docs/src/snippets/dependencyManagement/modelingFeatures-crossProjectPublications-advanced/kotlin/producer/build.gradle.kts
#	subprojects/docs/src/snippets/developingPlugins/pluginWithVariants/kotlin/build.gradle.kts
#	subprojects/docs/src/snippets/swift/testReport/kotlin/build.gradle.kts
#	subprojects/docs/src/snippets/swift/testReport/kotlin/buildSrc/src/main/kotlin/myproject.xctest-conventions.gradle.kts
#	subprojects/docs/src/snippets/testing/testReport/kotlin/build.gradle.kts
#	subprojects/docs/src/snippets/testing/testReport/kotlin/buildSrc/src/main/kotlin/myproject.java-conventions.gradle.kts
#	subprojects/docs/src/snippets/workerApi/md5ClassloaderIsolation/kotlin/build.gradle.kts
#	subprojects/docs/src/snippets/workerApi/md5ProcessIsolation/kotlin/build.gradle.kts
#	subprojects/kotlin-dsl-integ-tests/src/integTest/kotlin/org/gradle/kotlin/dsl/integration/PrecompiledScriptPluginIntegrationTest.kt
#	subprojects/wrapper/build.gradle.kts | test: Fix test with new variant names and cleanup file separators in output conditions. | test: Fix some broken tests after variant rename, error message format change. | fix: Simplify jacoco output artifact mapping. | fix: Fix coverage sample to work with new outgoing variants. | fix: Update consumer configurations in structuring large software projects sample to make tests pass. | style: Restore original whitespace. | fix: Delete unnecessary getter and property definitions which will be automatically generated. | fix: Remove java plugin. | style: Restore old position of method. | style: Use groovier syntax. | style: Simplify setup in tests by using file method. | style: Switch to Junit assert from Guava. | fix: Unused import removal. | fix: Unused import removal."
gradle/gradle,28483,https://github.com/gradle/gradle/pull/28483,Fix doc info,"<!--- The issue this PR addresses -->
<!-- Fixes #? -->
#28482

### Context
<!--- Why do you believe many users will benefit from this change? -->
<!--- Link to relevant issues or forum discussions here -->

### Contributor Checklist
- [x] [Review Contribution Guidelines](https://github.com/gradle/gradle/blob/master/CONTRIBUTING.md).
- [x] Make sure that all commits are [signed off](https://git-scm.com/docs/git-commit#Documentation/git-commit.txt---signoff) to indicate that you agree to the terms of [Developer Certificate of Origin](https://developercertificate.org/).
- [x] Make sure all contributed code can be distributed under the terms of the [Apache License 2.0](https://github.com/gradle/gradle/blob/master/LICENSE), e.g. the code was written by yourself or the original code is licensed under [a license compatible to Apache License 2.0](https://apache.org/legal/resolved.html).
- [x] Check [""Allow edit from maintainers"" option](https://help.github.com/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork/) in pull request so that additional changes can be pushed by Gradle team.
- [ ] Provide integration tests (under `<subproject>/src/integTest`) to verify changes from a user perspective.
- [ ] Provide unit tests (under `<subproject>/src/test`) to verify logic.
- [ ] Update User Guide, DSL Reference, and Javadoc for public-facing changes.
- [ ] Ensure that tests pass sanity check: `./gradlew sanityCheck`.
- [ ] Ensure that tests pass locally: `./gradlew <changed-subproject>:quickTest`.

### Reviewing cheatsheet

Before merging the PR, comments starting with 
- ❌ ❓**must** be fixed
- 🤔 💅 **should** be fixed
- 💭 **may** be fixed
- 🎉 celebrate happy things
",2024-03-18T16:46:14Z,lkasso,hejoefs,readable,"Change Summary This PR is 50% new code.   Platform Added Lines % of Total Line Changes Deleted Lines % of Total Line Changes Files Changed % of Total Files Changed 
 bt_ge_build_cache 0 0% 0 0% 0 0% 
 build_infrastructure 0 0% 0 0% 0 0% 
 core_configuration 0 0% 0 0% 0 0% 
 core_execution 0 0% 0 0% 0 0% 
 core_runtime 0 0% 0 0% 0 0% 
 documentation 5 50% 5 50% 1 100% 
 extensibility 0 0% 0 0% 0 0% 
 gradle_enterprise 0 0% 0 0% 0 0% 
 ide 0 0% 0 0% 0 0% 
 jvm 0 0% 0 0% 0 0% 
 kotlin_dsl 0 0% 0 0% 0 0% 
 release_coordination 0 0% 0 0% 0 0% 
 software 0 0% 0 0% 0 0% | The merge queue build has started. Click here to see all failures if any.","Fix doc info

Signed-off-by: Hélio Fernandes Sebastião <155336173+helfese@users.noreply.github.com> | Update platforms/documentation/docs/src/docs/design/gradle-module-metadata-latest-specification.md

Co-authored-by: Vlad Chesnokov <ov7a@yandex.ru>
Signed-off-by: Laura Kassovic <lkassovic@gradle.com>"
gradle/gradle,28356,https://github.com/gradle/gradle/pull/28356,Decouple dependency locking from configurations,"Dependency locking was written assuming configurations were the only thing that could be resolved. Shortly, this will no longer be true, so we update dependency locking to avoid this assumption.

In practice, this mostly means we introduce the concept of a 'LockId', which is the name attached to the dependencies for a given resolution within a lockfile. For a configuration, the lockId is equal to the configuration name.

### Reviewing cheatsheet

Before merging the PR, comments starting with 
- ❌ ❓**must** be fixed
- 🤔 💅 **should** be fixed
- 💭 **may** be fixed
- 🎉 celebrate happy things
",2024-03-07T03:32:46Z,jvandort,jvandort,readable,"@bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

PullRequestFeedback build | @bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

PullRequestFeedback build | The merge queue build has started. Click here to see all failures if any. | 🥷 Code experts: tresat, ljacomet
tresat, jvandort have most 👩‍💻 activity in the files.
ljacomet, jvandort have most 🧠 knowledge in the files.
 
 See details
platforms/software/dependency-management/src/integTest/groovy/org/gradle/integtests/resolve/locking/AbstractLockingIntegrationTest.groovy
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
832 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 95%
platforms/software/dependency-management/src/integTest/groovy/org/gradle/integtests/resolve/locking/DependencyLockingStrictModeIntegrationTest.groovy
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
123 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 100%
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/DefaultDependencyManagementServices.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB
12 additions & 11 deletions



JAN
13 additions & 11 deletions
39 additions & 44 deletions


DEC

1 additions & 31 deletions


NOV
5 additions & 4 deletions



OCT
677 additions & 5 deletions




Knowledge based on git-blame:
jvandort: 13%
ljacomet: 8%
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/ResolveContext.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN

2 additions & 9 deletions


DEC




NOV




OCT
75 additions & 0 deletions




Knowledge based on git-blame:
jvandort: 18%
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/configurations/DefaultConfiguration.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN

199 additions & 369 deletions


DEC
5 additions & 5 deletions
155 additions & 228 deletions


NOV




OCT
2367 additions & 0 deletions




Knowledge based on git-blame:
jvandort: 19%
ljacomet: 1%
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/configurations/ResolutionHost.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN

9 additions & 0 deletions


DEC




NOV




OCT
36 additions & 0 deletions




Knowledge based on git-blame:
jvandort: 20%
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/configurations/ResolutionStrategyFactory.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
114 additions & 0 deletions




Knowledge based on git-blame:
jvandort: 100%
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/configurations/ResolutionStrategyInternal.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
117 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 18%
jvandort: 3%
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/dsl/dependencies/DependencyLockingProvider.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
83 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 100%
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/ivyservice/DefaultConfigurationResolver.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB

4 additions & 17 deletions


JAN

279 additions & 82 deletions


DEC

2 additions & 12 deletions


NOV




OCT
307 additions & 0 deletions




Knowledge based on git-blame:
jvandort: 73%
ljacomet: 1%
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/ivyservice/ShortCircuitEmptyConfigurationResolver.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN

62 additions & 71 deletions


DEC

19 additions & 21 deletions


NOV




OCT
210 additions & 0 deletions




Knowledge based on git-blame:
jvandort: 40%
ljacomet: 4%
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/ivyservice/resolutionstrategy/DefaultResolutionStrategy.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
413 additions & 0 deletions




Knowledge based on git-blame:
jvandort: 8%
ljacomet: 8%
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/ivyservice/resolveengine/artifact/ArtifactSetToFileCollectionFactory.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV

2 additions & 2 deletions


OCT
307 additions & 0 deletions




Knowledge based on git-blame:
jvandort: 1%
platforms/software/dependency-management/src/main/java/org/gradle/internal/locking/DefaultDependencyLockingProvider.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
343 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 78%
platforms/software/dependency-management/src/main/java/org/gradle/internal/locking/DependencyLockingGraphVisitor.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC
4 additions & 3 deletions



NOV




OCT
177 additions & 0 deletions
0 additions & 16 deletions



Knowledge based on git-blame:
ljacomet: 81%
jvandort: 4%
platforms/software/dependency-management/src/main/java/org/gradle/internal/locking/InvalidLockFileException.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
23 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 100%
platforms/software/dependency-management/src/main/java/org/gradle/internal/locking/LockFileReaderWriter.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
266 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 95%
platforms/software/dependency-management/src/main/java/org/gradle/internal/locking/MissingLockStateException.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
24 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 100%
platforms/software/dependency-management/src/main/java/org/gradle/internal/locking/NoOpDependencyLockingProvider.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
75 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 100%
platforms/software/dependency-management/src/test/groovy/org/gradle/api/internal/artifacts/configurations/DefaultConfigurationSpec.groovy
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN

163 additions & 202 deletions


DEC

27 additions & 44 deletions


NOV




OCT
1948 additions & 0 deletions




Knowledge based on git-blame:
jvandort: 13%
platforms/software/dependency-management/src/test/groovy/org/gradle/api/internal/artifacts/ivyservice/ShortCircuitEmptyConfigurationResolverSpec.groovy
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN

3 additions & 10 deletions


DEC




NOV




OCT
205 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 26%
jvandort: 20%
platforms/software/dependency-management/src/test/groovy/org/gradle/api/internal/artifacts/ivyservice/resolutionstrategy/DefaultResolutionStrategySpec.groovy
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
338 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 6%
platforms/software/dependency-management/src/test/groovy/org/gradle/internal/locking/DefaultDependencyLockingProviderTest.groovy
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
289 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 96%
platforms/software/dependency-management/src/test/groovy/org/gradle/internal/locking/DependencyLockingGraphVisitorTest.groovy
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
289 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 85%
jvandort: 5%
platforms/software/dependency-management/src/test/groovy/org/gradle/internal/locking/LockFileReaderWriterTest.groovy
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
298 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 81%
platforms/software/dependency-management/src/test/groovy/org/gradle/internal/locking/NoOpDependencyLockingProviderTest.groovy
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT
48 additions & 0 deletions




Knowledge based on git-blame:
ljacomet: 100%

To learn more about /:\ gitStream - Visit our Docs | The merge queue build has started. Click here to see all failures if any.","Decouple dependency locking from configurations

Dependency locking was written assuming configurations were the only thing that could be resolved.
Shortly, this will no longer be true, so we update dependency locking to avoid this assumption.

In practice, this mostly means we introduce the concept of a 'LockId', which is the name attached
to the dependenices for a given resolution within a lockfile. For a configuration, the lockId is equal
to the configuration name. | Add String returning display name shorthand back to ResolutionHost

So we can avoid .displayName().getDisplayName() | Revert scala and groovy source set deprecations

Signed-off-by: Justin Van Dort <jvandort@gradle.com>"
gradle/gradle,26761,https://github.com/gradle/gradle/pull/26761,Add Configuration Cache codec for Unit type,"Fixes #25560

### Context
Added codec for Unit type. It ain’t much, but it’s honest work 😉 

No integration tests for this change, because existing tests are using GroovyDSL, so referencing Kotlin classes would not be trivial nor readable.

### Contributor Checklist
- [x] [Review Contribution Guidelines](https://github.com/gradle/gradle/blob/master/CONTRIBUTING.md)
- [x] Make sure that all commits are [signed off](https://git-scm.com/docs/git-commit#Documentation/git-commit.txt---signoff) to indicate that you agree to the terms of [Developer Certificate of Origin](https://developercertificate.org/).
- [x] Make sure all contributed code can be distributed under the terms of the [Apache License 2.0](https://github.com/gradle/gradle/blob/master/LICENSE), e.g. the code was written by yourself or the original code is licensed under [a license compatible to Apache License 2.0](https://apache.org/legal/resolved.html).
- [x] Check [""Allow edit from maintainers"" option](https://help.github.com/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork/) in pull request so that additional changes can be pushed by Gradle team
- [ ] Provide integration tests (under `<subproject>/src/integTest`) to verify changes from a user perspective
- [x] Provide unit tests (under `<subproject>/src/test`) to verify logic
- [ ] Update User Guide, DSL Reference, and Javadoc for public-facing changes
- [x] Ensure that tests pass sanity check: `./gradlew sanityCheck`
- [x] Ensure that tests pass locally: `./gradlew <changed-subproject>:quickTest`

### Reviewing cheatsheet

Before merging the PR, comments starting with 
- ❌ ❓**must** be fixed
- 🤔 💅 **should** be fixed
- 💭 **may** be fixed
- 🎉 celebrate happy things
",2023-11-06T17:45:52Z,bot-gradle,ILikeYourHat,readable,"Change Summary This PR is 100% new code.   Platform Added Lines % of Total Line Changes Deleted Lines % of Total Line Changes Files Changed % of Total Files Changed 
 bt_ge_build_cache 0 0% 0 0% 0 0% 
 build_infrastructure 0 0% 0 0% 0 0% 
 core_configuration 69 100% 0 0% 3 100% 
 core_execution 0 0% 0 0% 0 0% 
 core_runtime 0 0% 0 0% 0 0% 
 documentation 0 0% 0 0% 0 0% 
 extensibility 0 0% 0 0% 0 0% 
 gradle_enterprise 0 0% 0 0% 0 0% 
 ide 0 0% 0 0% 0 0% 
 jvm 0 0% 0 0% 0 0% 
 kotlin_dsl 0 0% 0 0% 0 0% 
 release_coordination 0 0% 0 0% 0 0% 
 software 0 0% 0 0% 0 0% | Any update on this PR? Do you need anything more from me? | @bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

PullRequestFeedback build | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | I've triggered a build for you. Click here to see all failures if there's any. | @ILikeYourHat thank you for fixing this!","Add codec for Unit type

Signed-off-by: Marcin Laskowski <marcin.laskowski@allegro.com> | Better test for Unit codec

Signed-off-by: Marcin Laskowski <marcin.laskowski@allegro.com>"
gradle/gradle,24847,https://github.com/gradle/gradle/pull/24847,docs: Make the configuration avoidance API table more readable,"### Context

The current table at https://docs.gradle.org/current/userguide/task_configuration_avoidance.html#sec:old_vs_new_configuration_api_overview is hard to read as old and new value are below each other instead of next to each other, and prefixed with distracting ""Instead of:"" and ""Use:"" terms.

### Contributor Checklist

- [x] [Review Contribution Guidelines](https://github.com/gradle/gradle/blob/master/CONTRIBUTING.md)
- [x] Make sure that all commits are [signed off](https://git-scm.com/docs/git-commit#Documentation/git-commit.txt---signoff) to indicate that you agree to the terms of [Developer Certificate of Origin](https://developercertificate.org/).
- [x] Make sure all contributed code can be distributed under the terms of the [Apache License 2.0](https://github.com/gradle/gradle/blob/master/LICENSE), e.g. the code was written by yourself or the original code is licensed under [a license compatible to Apache License 2.0](https://apache.org/legal/resolved.html).
- [x] Check [""Allow edit from maintainers"" option](https://help.github.com/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork/) in pull request so that additional changes can be pushed by Gradle team
- [ ] Provide integration tests (under `<subproject>/src/integTest`) to verify changes from a user perspective
- [ ] Provide unit tests (under `<subproject>/src/test`) to verify logic
- [x] Update User Guide, DSL Reference, and Javadoc for public-facing changes
- [x] Ensure that tests pass sanity check: `./gradlew sanityCheck`
- [x] Ensure that tests pass locally: `./gradlew <changed-subproject>:quickTest`

### Gradle Core Team Checklist

- [ ] Verify design and implementation 
- [ ] Verify test coverage and CI build status
- [ ] Verify documentation
",2023-07-20T18:51:35Z,bot-gradle,sschuberth,readable,"Screenshot of the new layout: | Let's make the new table 3 columns and not just 2 to make it clearer to the user.

I've actually considered doing that, but deliberately decided against that for the following reasons:

The table uses alternating background colors for rows. In my proposed layout, notes are always displayed in rows with a dimmed background, making them distinct from the ""real deal rows"" of comparing old vs new API.
With 3 columns the table gets wider, resulting in wrapping of cell contents, making the table actually less readable IMO.

So I'd clearly prefer to stick to the proposed layout. | So I'd clearly prefer to stick to the proposed layout.

@lkasso does that make sense to you? | So I'd clearly prefer to stick to the proposed layout.

@lkasso does that make sense to you?

Hey @sschuberth, this makes sense, but I have two problems:

The table layout doesn't make sense for all items in the table. For example, when an entry is ""This returns a TaskProvider instead of a Task."" it doesn't immediately let the user know which API is which.
Tables have to be the same format across the entire documentation, so I would have to change all the other tables as part of this PR as well.

I agree the current doc is also confusing.
Here is an option:
[cols=""a,a,a"", options=""header""]
|===
| Old API
| New API
| Notes

| [.small]#`task myTask(type: MyTask){}`#
| [.small]#`tasks.register(""myTask"", MyTask){}`#
| [.small]#There is no shorthand Groovy DSL for using the new API.#

| [.small]#link:{javadocPath}/org/gradle/api/tasks/TaskContainer.html#create-java.util.Map-groovy.lang.Closure-[TaskContainer.create()]#
| [.small]#No direct equivalent.#
| [.small]#Use one of the alternatives below.#

Proposal to make it more readable:

It's 3 columns
It uses the [.small] to make the text smaller.
It removes the methods input type.

Thoughts @sschuberth? | Sorry for the late reply.


The table layout doesn't make sense for all items in the table. For example, when an entry is ""This returns a TaskProvider instead of a Task."" it doesn't immediately let the user know which API is which.


IMO this could be easily addressed by slightly rewording those sentences.

Tables have to be the same format across the entire documentation

Frankly, that sounds like a ridiculous requirement... what's the rationale for that? Also, how does your proposal to use 3 columns fit to this requirement?

Proposal to make it more readable:

What I still like better in my proposal is that all rows with notes get a different background color due to the alternating row colors assigned to all tables. I.e. everything with a dimmed background is less important as it's just a note.
On the other hand, when putting the same information into all rows, the background row color has no meaning anymore. | @sschuberth, let's go ahead and merge for now.
I can look into it when I go over this section later this year during my general doc review/update cycle. | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | I've triggered a build for you. Click here to see all failures if there's any.","docs: Make the configuration avoidance API table more readable

Show old vs. new API next to each other instead of one above the other.
The description now spans row cells instead of column cells and is shown
below the API translation.

Signed-off-by: Sebastian Schuberth <sschuberth@gmail.com>"
gradle/gradle,24520,https://github.com/gradle/gradle/pull/24520,Don't fail TestLauncher execution when only tasks are configured,,2023-08-31T14:37:34Z,bot-gradle,donat,"readable, understandable","This pull request has been automatically marked as stale because it has not had recent activity. Given the limited bandwidth of the team, it will be closed if no further activity occurs. If you intend to work on this pull request, please ask the team to reopen the PR or push a new PR. Thank you for your contributions. | 🥷 Code experts: ldaley
ldaley has most 🧠 knowledge in the files.
 
 See details
subprojects/tooling-api-builders/src/main/java/org/gradle/tooling/internal/provider/runner/TestExecutionResultEvaluator.java
Knowledge based on git-blame:
ldaley: 21%
subprojects/tooling-api-builders/src/test/groovy/org/gradle/tooling/internal/provider/runner/TestExecutionResultEvaluatorTest.groovy
Knowledge based on git-blame:
ldaley: 27%

To learn more about /:\ gitStream - Visit our Docs | This PR is 97.7% new code.
There are 85 added lines and 2 deleted lines. | I've triggered a build for you. Click here to see all failures if there's any. | Pre-tested commit build failed. Click here to see all build failures. | @bot-gradle test and merge
I'm looking at the IOExceptions. | I've triggered a build for you. Click here to see all failures if there's any.",Don't fail TestLauncher execution when only tasks are configured
gradle/gradle,25140,https://github.com/gradle/gradle/pull/25140,Add build operation around root build tree finished,"This should allow to determine better when the execution phase finishes in scans, in particular when the root build doesn't execute tasks since its configuration failed.",2023-05-24T12:57:58Z,bot-gradle,wolfs,readable,I've triggered a build for you. | Pre-tested commit build failed. | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | I've triggered a build for you.,Add build operation around root build finish | Add package-info to new enterprise operations package | Improve naming for FinishRootBuildTreeBuildOperationType | Add test for FinishRootBuildTreeBuildOperationType | Add javadoc to FinishRootBuildTreeBuildOperationType | Add test for failing configuration phase | Add TestFile.prepend and use it in CompositeBuildOperationsIntegrationTest | Replace IgnoreIf by Requires | Add test for project without included build | Fix test on Windows
gradle/gradle,24961,https://github.com/gradle/gradle/pull/24961,Add non-tracking reasons to up-to-date and cacheability reasons,"<!--- The issue this PR addresses -->
Fixes https://github.com/gradle/ge/issues/20936

### Context
The change allows easier understanding of why a task is not considered up-to-date and why it was not cached
The issue linked above has references to internal discussions about this topic

### Contributor Checklist
- [x] [Review Contribution Guidelines](https://github.com/gradle/gradle/blob/master/CONTRIBUTING.md)
- [x] Make sure that all commits are [signed off](https://git-scm.com/docs/git-commit#Documentation/git-commit.txt---signoff) to indicate that you agree to the terms of [Developer Certificate of Origin](https://developercertificate.org/).
- [x] Make sure all contributed code can be distributed under the terms of the [Apache License 2.0](https://github.com/gradle/gradle/blob/master/LICENSE), e.g. the code was written by yourself or the original code is licensed under [a license compatible to Apache License 2.0](https://apache.org/legal/resolved.html).
- [ ] Check [""Allow edit from maintainers"" option](https://help.github.com/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork/) in pull request so that additional changes can be pushed by Gradle team
- [x] Provide integration tests (under `<subproject>/src/integTest`) to verify changes from a user perspective
- [x] Provide unit tests (under `<subproject>/src/test`) to verify logic
- [x] Update User Guide, DSL Reference, and Javadoc for public-facing changes
- [x] Ensure that tests pass sanity check: `./gradlew sanityCheck`
- [x] Ensure that tests pass locally: `./gradlew <changed-subproject>:quickTest`

### Gradle Core Team Checklist
- [ ] Verify design and implementation 
- [ ] Verify test coverage and CI build status
- [ ] Verify documentation
",2023-05-15T10:34:48Z,bot-gradle,maczikasz,readable,"This PR contains the changes proposed to cacheability message, this can be reverted if we agree differently on the spec | I've triggered a build for you. | Pre-tested commit build failed. | @bot-gradle test and merge | I've triggered a build for you.","Refactor TaskExecutionMode to hold an arbitrary message

TaskExecutionMode was turned into an Interface and DefaultTaskExecutionMode was created to implement it.
DefaultTaskExecutionMode provides facotyr methods for each of the types of execution modes that was previously the enum constants.
DefaultTaskExecutionMode also can accept a reason for `untracked` execution modes that can be used to enrich the execution context with the reason of the track not being tracked

https://github.com/gradle/ge/issues/20936
Signed-off-by: Szava Maczika <smaczika@gradle.com> | Add cacheability disabled reason for when tasks are untracked

When a track is marked with @UntrackedTask or Task.doNotTrackState(reason) but it is not marked as cacheable, the reason for not tracking the task is used as the cacheability disabled reason.

https://github.com/gradle/ge/issues/20936
Signed-off-by: Szava Maczika <smaczika@gradle.com> | Align the messages used for untracked tasks

https://github.com/gradle/ge/issues/20936
Signed-off-by: Szava Maczika <smaczika@gradle.com> | Update the missing test case with the correct message

Signed-off-by: Szava Maczika <smaczika@gradle.com> | Apply the review suggestions from the open PR

https://github.com/gradle/gradle/pull/24961
https://github.com/gradle/ge/issues/20936

Signed-off-by: Szava Maczika <smaczika@gradle.com> | Apply the remaining comment on the open PR

https://github.com/gradle/gradle/pull/24961
https://github.com/gradle/ge/issues/20936

Signed-off-by: Szava Maczika <smaczika@gradle.com> | Update the test code to assert the correct message | Fix minor PR comment

https://github.com/gradle/gradle/pull/24961
https://github.com/gradle/ge/issues/20936"
gradle/gradle,25643,https://github.com/gradle/gradle/pull/25643,Separate DefaultMavenPom and MavenPublicationInternal,"DefaultMavenPom no longer depends on a MavenPublicationInternal. This means we can now model POMs outside of a publication context, opening the door publications with _multiple_ POMs.

Move several related publishing classes from various projects to into :publish project.

Improve MavenPomFileGenerator by moving related logic from GenerateMavenPom and updating tests to avoid test-only methods and use MavenPomInternal API. This will allow us to more easily generate multiple POMs in a single task invocation
",2023-07-12T18:20:35Z,bot-gradle,jvandort,readable,"🥷 Code experts: marcphilipp, bigdaz
jvandort has most 👩‍💻 activity in the files.
bigdaz, marcphilipp have most 🧠 knowledge in the files.
 
 See details
subprojects/architecture-test/src/changes/archunit_store/internal-api-nullability.txt
Activity based on git-commit:




jvandort




JUL
0 additions & 13 deletions


JUN
0 additions & 22 deletions


MAY
1 additions & 38 deletions


APR
4 additions & 4 deletions


MAR



FEB




Knowledge based on git-blame:
subprojects/ivy/src/main/java/org/gradle/api/publish/ivy/internal/publication/DefaultIvyPublication.java
Activity based on git-commit:




jvandort




JUL
275 additions & 173 deletions


JUN



MAY
5 additions & 4 deletions


APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 11%
marcphilipp: 5%
subprojects/language-native/src/main/java/org/gradle/language/cpp/internal/DefaultCppExecutable.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
subprojects/language-native/src/main/java/org/gradle/language/cpp/internal/DefaultCppSharedLibrary.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
subprojects/language-native/src/main/java/org/gradle/language/cpp/internal/DefaultCppStaticLibrary.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
subprojects/language-native/src/main/java/org/gradle/language/cpp/internal/MainLibraryVariant.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
subprojects/language-native/src/main/java/org/gradle/language/plugins/NativeBasePlugin.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
subprojects/language-native/src/main/java/org/gradle/language/swift/internal/DefaultSwiftExecutable.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
subprojects/language-native/src/main/java/org/gradle/language/swift/internal/DefaultSwiftSharedLibrary.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
subprojects/language-native/src/main/java/org/gradle/language/swift/internal/DefaultSwiftStaticLibrary.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
subprojects/language-native/src/test/groovy/org/gradle/language/plugins/NativeBasePluginTest.groovy
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
subprojects/maven/src/integTest/groovy/org/gradle/api/publish/maven/MavenPublishIssuesIntegTest.groovy
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 38%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/MavenDependency.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 81%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/artifact/DefaultMavenArtifactSet.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 69%
marcphilipp: 2%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/dependencies/MavenDependencyInternal.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 89%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/dependencies/MavenVersionRangeMapper.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 3%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/DefaultMavenPom.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY
8 additions & 39 deletions


APR



MAR



FEB




Knowledge based on git-blame:
marcphilipp: 66%
bigdaz: 9%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/DefaultMavenPomDeveloper.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
marcphilipp: 95%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/DefaultMavenPomDistributionManagement.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
marcphilipp: 100%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/DefaultMavenPomLicense.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
marcphilipp: 100%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/DefaultMavenPomMailingList.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
marcphilipp: 99%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/DefaultMavenPomOrganization.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
marcphilipp: 100%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/DefaultMavenPomProjectManagement.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
marcphilipp: 100%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/DefaultMavenPomRelocation.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
marcphilipp: 100%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/DefaultMavenPomScm.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
marcphilipp: 100%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/DefaultMavenPublication.java
Activity based on git-commit:




jvandort




JUL



JUN
12 additions & 2 deletions


MAY
277 additions & 231 deletions


APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 14%
marcphilipp: 4%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/MavenPomDependencies.java
Activity based on git-commit:




jvandort




JUL



JUN
8 additions & 8 deletions


MAY
50 additions & 0 deletions


APR



MAR



FEB




Knowledge based on git-blame:
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/MavenPomDistributionManagementInternal.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
marcphilipp: 100%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/MavenPomInternal.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY
3 additions & 16 deletions


APR



MAR



FEB




Knowledge based on git-blame:
marcphilipp: 41%
bigdaz: 8%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publication/MavenPublicationInternal.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY
0 additions & 18 deletions


APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 31%
marcphilipp: 4%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/publisher/MavenNormalizedPublication.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 49%
marcphilipp: 10%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/internal/tasks/MavenPomFileGenerator.java
Activity based on git-commit:




jvandort




JUL



JUN
28 additions & 0 deletions


MAY
2 additions & 35 deletions


APR



MAR



FEB




Knowledge based on git-blame:
marcphilipp: 39%
bigdaz: 20%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/plugins/MavenPublishPlugin.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 25%
marcphilipp: 1%
subprojects/maven/src/main/java/org/gradle/api/publish/maven/tasks/GenerateMavenPom.java
Activity based on git-commit:




jvandort




JUL



JUN
8 additions & 7 deletions


MAY
18 additions & 15 deletions


APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 46%
marcphilipp: 1%
subprojects/maven/src/test/groovy/org/gradle/api/publish/maven/internal/publication/DefaultMavenPublicationTest.groovy
Activity based on git-commit:




jvandort




JUL



JUN
3 additions & 2 deletions


MAY
18 additions & 19 deletions


APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 43%
marcphilipp: 1%
subprojects/maven/src/test/groovy/org/gradle/api/publish/maven/internal/publisher/ValidatingMavenPublisherTest.groovy
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 86%
marcphilipp: 3%
subprojects/maven/src/test/groovy/org/gradle/api/publish/maven/internal/tasks/MavenPomFileGeneratorTest.groovy
Activity based on git-commit:




jvandort




JUL



JUN



MAY
4 additions & 4 deletions


APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 48%
marcphilipp: 27%
subprojects/maven/src/test/groovy/org/gradle/api/publish/maven/plugins/MavenPublishPluginTest.groovy
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
bigdaz: 40%
subprojects/plugins/src/main/java/org/gradle/api/plugins/internal/PluginAuthorServices.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR



FEB
3 additions & 11 deletions



Knowledge based on git-blame:
subprojects/plugins/src/main/java/org/gradle/jvm/component/internal/DefaultJvmSoftwareComponent.java
Activity based on git-commit:




jvandort




JUL



JUN



MAY



APR



MAR
47 additions & 214 deletions


FEB
61 additions & 58 deletions



Knowledge based on git-blame:

To learn more about /:\ gitStream - Visit our Docs | @bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

PullRequestFeedback build | This PR is 42.05% new code.
There are 561 added lines and 773 deleted lines. | @bot-gradle test and merge | I've triggered a build for you. Click here to see all failures if there's any.","Separate DefaultMavenPom and MavenPublicationInternal

DefaultMavenPom no longer depends on a MavenPublicationInternal. This means we can now model POMs
outside of a publication context, opening the door publications with _multiple_ POMs.

Move several related publishing classes from various projects to into :publish project.

Improve MavenPomFileGenerator by moving related logic from GenerateMavenPom and updating
tests to avoid test-only methods and use MavenPomInternal API. This will allow us to more easily
generate multiple POMs in a single task invocation | Link internal KMM usage

Also rename some test methods | Rename MavenProjectIdentity to MavenPublicationCoordinates"
gradle/gradle,24000,https://github.com/gradle/gradle/pull/24000,Respect build log level in tooling api builds,"if non was set via the command line or the `verboseLogging` flag, it would fall back to the default log level. But we don't want any influence if it not explicitly overridden with the `verboseLogging`

- #19340",2023-03-01T13:33:57Z,bot-gradle,reinsch82,readable,"@bot-gradle test ACT | I've triggered the following builds for you:

AllCrossVersionTestsReadyForNightly build | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | I'm a bit late with this request, but could you please add a section to the release notes? A separate PR is fine, given that test and merge goes through. | I've triggered a build for you.","Respect build log level in tooling api builds

if non was set via the command line or the `verboseLogging` flag, it would fall back to the default log level. But we don't want any influence if it not explicitly overridden with the `verboseLogging`

- #19340 | Respect build log level in tooling api builds

if non was set via the command line or the `verboseLogging` flag, it would fall back to the default log level. But we don't want any influence if it not explicitly overridden with the `verboseLogging` or command line options

- #19340 | Create cross version test  for LogLevel settings in Tooling API

- #19340 | Don't test smaller than 7.0

- #19340 | Clean up

- Don't derive from utility class ToolingApiTestCommon
- formatting

- #19340 | Clean up tests

- reduce duplication
- remove comments

- #19340"
gradle/gradle,24327,https://github.com/gradle/gradle/pull/24327,Unify execution plan creation and sub-plan extraction,"Summary:
- Extract scheduled nodes traversal logic from `BuildOperationFiringBuildWorkPreparer` into `PlannedNodeGraph`.
- Introduce explicit levels of detail for the execution plans (tasks only, tasks+transform steps)
- Generalize the traversal logic to be the same for the `Node -> PlannedNode` graph conversion and lower-resolution detail `PlannedNode` graph conversion
  - The latter is needed for the `List<PlannedNode> getExecutionPlan(Set<NodeIdentity.NodeType> types)` build operation API

TODO:
- [x] Javadocs
- [x] More tests",2023-03-22T16:59:45Z,bot-gradle,alllex,readable,@bot-gradle test and merge | Your PR is queued. See the queue page for details. | I've triggered a build for you.,"Remove duplicate transform registration | Fix execution plan resolution in build operation | Improve javadoc | Make NodeType sets static | Merge branch 'master' into alllex/scans/more-transform-bops-tests | Merge branch 'master' into alllex/scans/more-transform-bops-tests | Introduce PlannedNodeGraph | Add multiple transformer consumers test | Mark ToPlannedNodeConverter(Registry) as thread-safe | Move ToPlannedNodeConverterRegistry to `execution.plan` package | Polish test setup | Fix PlannedTransformStepIdentity.consumerProjectPath

The consumer project path has been including the build path, and non only the project path.

(cherry picked from commit e9becd43559c999b76dc6e1bb207459755550cde) | Use identity task path in TaskIdentity string | Add composite build test | Deprecate PlannedTask.getDependencies | Address PR feedback | Fix problems with using NodeIdentity as keys | Merge branch 'master' into alllex/scans/more-transform-bops-tests

# Conflicts:
#	subprojects/core/src/main/java/org/gradle/execution/plan/ToPlannedNodeConverter.java
#	subprojects/core/src/main/java/org/gradle/execution/plan/ToPlannedNodeConverterRegistry.java
#	subprojects/core/src/main/java/org/gradle/internal/build/BuildOperationFiringBuildWorkPreparer.java | Add another composite build test for transform operations | Polish checking for detail level in PlannedNodeGraph | Enrich an error message for a deprecated task dependencies getter | Remove outdated comment"
gradle/gradle,23437,https://github.com/gradle/gradle/pull/23437,Add the Kotlin DSL public API to `:architecture-tests`,"This makes the Kotlin DSL public API tested with the same tests as the Gradle public API.

This PR also freezes the archunit store to capture the current state of affairs.
This is looking good except that the public API references delegates that are currently considered internal.
",2023-01-10T14:19:19Z,bot-gradle,eskatos,readable,"@bot-gradle test this | I've triggered the following builds for you:

PullRequestFeedback build | @bot-gradle test and merge | I've triggered a build for you.","Add the Kotlin DSL API to :architecture-tests

Signed-off-by: Paul Merlin <paul@gradle.com> | Freeze archunit store after including the Kotlin DSL public API

Signed-off-by: Paul Merlin <paul@gradle.com> | Skip kotlin internal members of the public api in architecture tests

Signed-off-by: Paul Merlin <paul@gradle.com> | Refreeze architecture tests

Signed-off-by: Paul Merlin <paul@gradle.com> | Rename archunit freeze files

Signed-off-by: Paul Merlin <paul@gradle.com> | Add explaining comment

Signed-off-by: Paul Merlin <paul@gradle.com>"
gradle/gradle,23635,https://github.com/gradle/gradle/pull/23635,Track systemProps changes in DefaultGradlePropertiesLoader,Fixes #19184 ,2023-02-06T12:54:47Z,bot-gradle,6hundreds,readable,"@bot-gradle test this | I've triggered the following builds for you:

PullRequestFeedback build | @bot-gradle test and merge | I've triggered a build for you.",Track systemProps changes in DefaultGradlePropertiesLoader
gradle/gradle,22919,https://github.com/gradle/gradle/pull/22919,Follow-up for attribute transform work,"Short follow-up for: https://github.com/gradle/gradle/pull/22802

Add upgrade guide entry
Add some extra documentation
Apply recommendations from previous PR
",2022-12-02T00:52:51Z,bot-gradle,jvandort,readable,"@bot-gradle test this | OK, I've already triggered the following builds for you:

PullRequestFeedback build | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | OK, I've already triggered a build for you.","Follow-up for attribute transform work

Add upgrade guide entry
Add some extra documentation
Apply recommendations from previous PR"
gradle/gradle,22892,https://github.com/gradle/gradle/pull/22892,Replace PropertyWalker internals with TypeMetadataWalker,,2022-12-08T11:51:29Z,bot-gradle,lptr,readable,"@bot-gradle test this | OK, I've already triggered the following builds for you:

PullRequestFeedback build | @bot-gradle test and merge | OK, I've already triggered a build for you.","Start replacing PropertyWalker internals | Extract handling of values for properties from PropertyAnnotationHandlers to PropertyValueHandlers

This just extracts the logic, but wireing of new handlers still needs to be done | Revert ""Extract handling of values for properties from PropertyAnnotationHandlers to PropertyValueHandlers""

This reverts commit 513071b01f25856d02b5d9ef5b621d98c83bc836. | Remove RuntimeBeanNodeFactory and RuntimeBeanNode and its implementations | Set property getter accessible before invoked in AbstractTypeMetadataWalker | Initial handling of provider unpacking errors in AbstractTypeMetadataWalker

We might need to handle every getter call in the Walker, but handling just providers might be a good initial step | Handle nested iterables with named elements correctly, also start counter for iterable keys from 0 | Improve cycles detection in AbstractTypeMetadataWalker so it detects cycles between root and child nodes

Cycles between root and child nodes were not detected before.
Also we throw IllegalStateException instead of GradleException for cycles now. | Improve handling of invoking and unpacking nested properties in AbstractTypeMetadataWalker | Qualify nested handler method names | Introduce separated StaticMetadataVisitor and InstanceMetadataVisitor in TypeMetadataWalker

Co-authored-by: Lóránt Pintér <lorant@gradle.com> | Handle Optional for Nested properties in DefaultPropertyWalker | Small code cleanup of DefaultPropertyWalker and AbstractTypeMetadataWalker | Add some more tests for AbstractTypeMetadataWalker | Handle absent Providers in collections | Inc. review comments for DefaultPropertyWalker changes | Merge branch 'master' into asodja-lptr/execution/use-type-metadata-walker-in-property-walker | Improve code readability for unpacking Providers/Nested properties in AbstractTypeMetadataWalker | Simplify error handling | Improve error message"
gradle/gradle,21764,https://github.com/gradle/gradle/pull/21764,Simplify testing validation problems,,2022-09-26T16:52:12Z,bot-gradle,lptr,readable,"OK, I've already triggered a build for you. | Pre-tested commit build failed. | @bot-gradle test and merge please. | Your PR is queued. See the queue page for details. | OK, I've already triggered a build for you.",Simplify testing validation problems | Make the tested message look nicer | Only strip indents from manual warnings | Fix test on Java 15+
gradle/gradle,22258,https://github.com/gradle/gradle/pull/22258,Document corner case issue for incrementality,,2022-09-30T15:22:10Z,bot-gradle,ljacomet,readable,"@bot-gradle test and merge | OK, I've already triggered a build for you.",Document corner case issue for incrementality
gradle/gradle,22261,https://github.com/gradle/gradle/pull/22261,"Improve output of integration tests failing check for ""output contains string"" tests","When integration test fails checking for presence of line in output, show only difference with most similar line.

The format is picked explicitly to match IntelliJ's pattern for detecting comparison failures here:

https://github.com/JetBrains/intellij-community/blob/3f97400071e1a0b6a21a6bbd2af1fa27c47467c0/plugins/gradle/java/src/execution/GradleRunnerUtil.java#L97-L116

Output after the changes (17 lines, only non-matching line is shown):

```text
Expected: ""result = [b-dir.green, c-dir.red]""
 but: was ""result = [b-dir.green, c-dir.green]""
Expected :result = [b-dir.green, c-dir.red]
Actual   :result = [b-dir.green, c-dir.green]
<Click to see difference>


Expected: ""result = [b-dir.green, c-dir.red]""
 but: was ""result = [b-dir.green, c-dir.green]""
	at app//org.gradle.integtests.fixtures.executer.OutputScrapingExecutionResult.failOnDifferentLine(OutputScrapingExecutionResult.java:431)
	at app//org.gradle.integtests.fixtures.executer.OutputScrapingExecutionResult.lambda$assertContentContains$2(OutputScrapingExecutionResult.java:258)
	at java.base@11.0.14/java.util.Optional.ifPresent(Optional.java:183)
	at app//org.gradle.integtests.fixtures.executer.OutputScrapingExecutionResult.assertContentContains(OutputScrapingExecutionResult.java:258)
	at app//org.gradle.integtests.fixtures.executer.OutputScrapingExecutionResult.assertOutputContains(OutputScrapingExecutionResult.java:267)
	at app//org.gradle.integtests.fixtures.executer.InProcessGradleExecuter$InProcessExecutionResult.assertOutputContains(InProcessGradleExecuter.java:533)
	at app//org.gradle.integtests.fixtures.AbstractIntegrationSpec.outputContains(AbstractIntegrationSpec.groovy:633)
	at org.gradle.integtests.resolve.transform.ArtifactTransformInputArtifactIntegrationTest.honors @PathSensitive(#sensitivity) to input artifact property for project artifact directory when caching(ArtifactTransformInputArtifactIntegrationTest.groovy:534)
```

Output before the changes (65 lines, full build output repeated three times):

```text
Condition failed with Exception:

outputContains(""result = [b-dir.green, c-dir.red]"")
|
java.lang.AssertionError: Did not find expected text in build output.
Expected: result = [b-dir.green, c-dir.red]
 
Build output:
=======
> Task :b:producer
> Task :c:producer UP-TO-DATE
 
> Task :a:resolve
result = [b-dir.green, c-dir.green]
 
	at org.gradle.integtests.fixtures.executer.OutputScrapingExecutionResult.failOnMissingOutput(OutputScrapingExecutionResult.java:394)
	at org.gradle.integtests.fixtures.executer.OutputScrapingExecutionResult.assertContentContains(OutputScrapingExecutionResult.java:227)
	at org.gradle.integtests.fixtures.executer.OutputScrapingExecutionResult.assertOutputContains(OutputScrapingExecutionResult.java:234)
	at org.gradle.integtests.fixtures.executer.InProcessGradleExecuter$InProcessExecutionResult.assertOutputContains(InProcessGradleExecuter.java:533)
	at org.gradle.integtests.fixtures.AbstractIntegrationSpec.outputContains(AbstractIntegrationSpec.groovy:633)
	at org.gradle.integtests.resolve.transform.ArtifactTransformInputArtifactIntegrationTest.honors @PathSensitive(#sensitivity) to input artifact property for project artifact directory when caching(ArtifactTransformInputArtifactIntegrationTest.groovy:534)

Condition failed with Exception:

outputContains(""result = [b-dir.green, c-dir.red]"")
|
java.lang.AssertionError: Did not find expected text in build output.
Expected: result = [b-dir.green, c-dir.red]
 
Build output:
=======
> Task :b:producer
> Task :c:producer UP-TO-DATE
 
> Task :a:resolve
result = [b-dir.green, c-dir.green]
 
	at org.gradle.integtests.fixtures.executer.OutputScrapingExecutionResult.failOnMissingOutput(OutputScrapingExecutionResult.java:394)
	at org.gradle.integtests.fixtures.executer.OutputScrapingExecutionResult.assertContentContains(OutputScrapingExecutionResult.java:227)
	at org.gradle.integtests.fixtures.executer.OutputScrapingExecutionResult.assertOutputContains(OutputScrapingExecutionResult.java:234)
	at org.gradle.integtests.fixtures.executer.InProcessGradleExecuter$InProcessExecutionResult.assertOutputContains(InProcessGradleExecuter.java:533)
	at org.gradle.integtests.fixtures.AbstractIntegrationSpec.outputContains(AbstractIntegrationSpec.groovy:633)
	at org.gradle.integtests.resolve.transform.ArtifactTransformInputArtifactIntegrationTest.honors @PathSensitive(#sensitivity) to input artifact property for project artifact directory when caching(ArtifactTransformInputArtifactIntegrationTest.groovy:534)

	at org.gradle.integtests.resolve.transform.ArtifactTransformInputArtifactIntegrationTest.honors @PathSensitive(#sensitivity) to input artifact property for project artifact directory when caching(ArtifactTransformInputArtifactIntegrationTest.groovy:534)
Caused by: java.lang.AssertionError: Did not find expected text in build output.
Expected: result = [b-dir.green, c-dir.red]

Build output:
=======
> Task :b:producer
> Task :c:producer UP-TO-DATE

> Task :a:resolve
result = [b-dir.green, c-dir.green]

	at org.gradle.integtests.fixtures.executer.OutputScrapingExecutionResult.failOnMissingOutput(OutputScrapingExecutionResult.java:394)
	at org.gradle.integtests.fixtures.executer.OutputScrapingExecutionResult.assertContentContains(OutputScrapingExecutionResult.java:227)
	at org.gradle.integtests.fixtures.executer.OutputScrapingExecutionResult.assertOutputContains(OutputScrapingExecutionResult.java:234)
	at org.gradle.integtests.fixtures.executer.InProcessGradleExecuter$InProcessExecutionResult.assertOutputContains(InProcessGradleExecuter.java:533)
	at org.gradle.integtests.fixtures.AbstractIntegrationSpec.outputContains(AbstractIntegrationSpec.groovy:633)
	... 1 more
```

This is far from perfect, as the new output still shows the expected/actual values three times for some reason in IntelliJ (only twice in TeamCity), but I think it's still a significant improvement in how readable the output is.",2022-10-06T17:49:07Z,bot-gradle,lptr,readable,"I'd consider adding 2 things to the new message:

A line number, so that we can quickly find where in long actual output, the mismatch occurred.  Something like:

Expected: ""result = [b-dir.green, c-dir.red]"" but: line 4/26 was ""result = [b-dir.green, c-dir.green]""

Some sort of disclaimer that this is a best-effort.  I think it's inevitable that at some point this will be used to compare against an actual output where multiple lines are close to the expected line.  It's possible it could identify an actual line which is not the indented matching target.  I can see myself getting confused trying to alter line 4 of the actual output, when really it's line 17 that should match my expectation.

Expected: ""result = [b-dir.green, c-dir.red]"" but: potential match on line 4/26 was ""result = [b-dir.green, c-dir.green]""

Ah, I should have added something about why the format is how it is. Added now.
So we use the current format so IntelliJ can catch the comparison when parsing the output with this code:
https://github.com/JetBrains/intellij-community/blob/3f97400071e1a0b6a21a6bbd2af1fa27c47467c0/plugins/gradle/java/src/execution/GradleRunnerUtil.java#L97-L116
This results in the <Click to see difference> label that pops up a diff view of the expected and actual texts.
The problem is that if we want to do this, we need to match the exception's message format exactly, which allows no additional text before or after. | OK, I've already triggered a build for you. | Pre-tested commit build failed. | @bot-gradle test and merge please. | Your PR is queued. See the queue page for details. | OK, I've already triggered a build for you.","Add intentional error to test TeamCity | Show difference to most similar line when comparing outputs | Add test | Revert ""Add intentional error to test TeamCity""

This reverts commit c943fba333fdb52eebf6a8e61d04d953c1797c73."
gradle/gradle,21234,https://github.com/gradle/gradle/pull/21234,Improve dependency verification update process,"Generated origins have to be replaced with the right justification for
their existence.",2022-07-14T15:20:53Z,bot-gradle,ljacomet,readable,"LGTM | @bot-gradle test this | OK, I've already triggered the following builds for you:

PullRequestFeedback build | This PR ends up adding one improvement to the verification report: indicate when a signing key that is also a trusted key is ignored. | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | Maybe we could rewrite that signature check code to be similar to the one where verification metadata entry exists, so there in case where verification metadata entry exists we have:
DefaultSignatureVerificationResultBuilder result = new DefaultSignatureVerificationResultBuilder(file, signature);
verifySignature(signatureVerificationService, file, signature, allTrustedKeys(foundArtifact, verification.getTrustedPgpKeys()), allIgnoredKeys(verification.getIgnoredPgpKeys()), result);
if (result.hasOnlyIgnoredKeys()) {
    builder.failWith(new OnlyIgnoredKeys(file));
    if (verification.getChecksums().isEmpty()) {
        builder.failWith(new MissingChecksums(file));
        return;
    } else {
        verifyChecksums(checksumService, file, verification, builder);
        return;
    }
}
if (result.hasError()) {
    builder.failWith(result.asError(publicKeyService));
    return;
}

So here we would have:
DefaultSignatureVerificationResultBuilder result = new DefaultSignatureVerificationResultBuilder(file, signature);
verifySignature(signatureVerificationService, file, signature, allTrustedKeys(foundArtifact, Collections.emptySet()), allIgnoredKeys(Collections.emptySet()), result);
if (result.hasOnlyIgnoredKeys()) {
    builder.failWith(new OnlyIgnoredKeys(file));
    builder.failWith(new MissingChecksums(file));
    return;
}
if (result.hasError()) {
    builder.failWith(result.asError(publicKeyService));
    return;
}
return;

Not sure if that makes it more readable for everyone or just for me. In the end it would be best to to move that duplicated logic to some method, but maybe that is more work. | OK, I've already triggered a build for you.","Clarify verification metadata update instructions

Generated origins have to be replaced with the right justification for
their existence. | Clean up generated origins

* Playwright libraries are trusted by group for their signing key
* Entries with a pgp element should not have the checksum one
* Updated origin otherwise | Improve dependency verification report

When a dependency that only had a trusted key entry with that same key
ignored, the only error displayed was that the checksum was missing.
Now the report will indicate that keys are ignored."
gradle/gradle,17743,https://github.com/gradle/gradle/pull/17743,Add line break before using multiline parameters,"I find this much more readable:
- The parameters are further left, so you need less line length
- The body is nicely separated from the list of arguments

Before:
```java
public DefaultAfterPreviousExecutionState(OriginMetadata originMetadata,
                                          ImplementationSnapshot implementation,
                                          ImmutableList<ImplementationSnapshot> additionalImplementations,
                                          ImmutableSortedMap<String, ValueSnapshot> inputProperties,
                                          ImmutableSortedMap<String, FileCollectionFingerprint> inputFileProperties,
                                          ImmutableSortedMap<String, FileSystemSnapshot> outputFilesProducedByWork,
                                          boolean successful) {
    super(implementation, additionalImplementations, inputProperties, inputFileProperties);
```

After:

```java
public DefaultAfterPreviousExecutionState(
    OriginMetadata originMetadata,
    ImplementationSnapshot implementation,
    ImmutableList<ImplementationSnapshot> additionalImplementations,
    ImmutableSortedMap<String, ValueSnapshot> inputProperties,
    ImmutableSortedMap<String, FileCollectionFingerprint> inputFileProperties,
    ImmutableSortedMap<String, FileSystemSnapshot> outputFilesProducedByWork,
    boolean successful
) {
    super(implementation, additionalImplementations, inputProperties, inputFileProperties);
```

",2021-07-19T07:11:30Z,bot-gradle,wolfs,readable,"The 2nd style also makes rename and change signature refactorings produce smaller and more meaningful diffs. | @bot-gradle test and merge | OK, I've already triggered a build for you. | Thank you, Stefan, I wanted to do this ages ago!","Add line break before using multiline parameters

Before:
```
public DefaultAfterPreviousExecutionState(OriginMetadata originMetadata,
                                          ImplementationSnapshot implementation,
                                          ImmutableList<ImplementationSnapshot> additionalImplementations,
                                          ImmutableSortedMap<String, ValueSnapshot> inputProperties,
                                          ImmutableSortedMap<String, FileCollectionFingerprint> inputFileProperties,
                                          ImmutableSortedMap<String, FileSystemSnapshot> outputFilesProducedByWork,
                                          boolean successful) {
    super(implementation, additionalImplementations, inputProperties, inputFileProperties);
```

After:

```
public DefaultAfterPreviousExecutionState(
    OriginMetadata originMetadata,
    ImplementationSnapshot implementation,
    ImmutableList<ImplementationSnapshot> additionalImplementations,
    ImmutableSortedMap<String, ValueSnapshot> inputProperties,
    ImmutableSortedMap<String, FileCollectionFingerprint> inputFileProperties,
    ImmutableSortedMap<String, FileSystemSnapshot> outputFilesProducedByWork,
    boolean successful
) {
    super(implementation, additionalImplementations, inputProperties, inputFileProperties);
```"
gradle/gradle,17184,https://github.com/gradle/gradle/pull/17184,Provide java.version in java toolchains' metadata,"<!--- The issue this PR addresses -->
Fixes #17085 
",2021-05-19T00:32:40Z,bot-gradle,mlopatkin,readable,"OK, I've already triggered a build for you. | Pre-tested commit build failed. | @bot-gradle test and merge | OK, I've already triggered a build for you.",Provide java.version in java toolchains' metadata
gradle/gradle,15851,https://github.com/gradle/gradle/pull/15851,Take filtering into account for missing dependency detection,"This allows for the use-case to zip up the root project
directory, as long as no outputs of other tasks are
included in the zip.

Note that filters for outputs are not taken into account,
only filters for inputs. This is because for outputs,
even when the output is a filtered file tree, the task
still may produce files anywhere in the output directory.",2021-01-21T07:16:51Z,wolfs,wolfs,readable,"@lptr PTAL! | @bot-gradle test this | OK, I've already triggered ReadyForMerge build for you.","Take filtering into account for missing dependency detection

This allows for the use-case to zip up the root project
directory, as long as no outputs of other tasks are not
included in the zip.

Note that filters for outputs are not taken into account,
only filters for inputs. This is because for outputs,
even when the output is a filtered file tree, the task
still may produce files anywhere in the output directory. | Rename RelatedLocations -> ExecutionNodeAccessHierarchies | Move caseSensitivity closer to children

CaseSensitivity should be a part of childMap, though that is a bigger change. | Generify RelatedLocation | Move ValuedPathHierarchy to separate class

and rename the methods. | Simplify ExecutionNodeAccessHierarchy | Simplify ReadOnlyFileTreeElement | Use right method for missing file tree dependencies | Improve test | Rename ValuedPathHierarchy.visitValues methods | Polish tests | Another small improvement for the deprecation message | Use input/outputHierarchy | Add Javadoc to ExecutionNodeAccessHierarchy | Rename Valued{Path -> Vfs}Hierarchy"
gradle/gradle,31485,https://github.com/gradle/gradle/pull/31485,Apply artifact exclusions after build dependencies execute,"Some exclusions declared in ivy descriptors may exclude artifacts by name. We can only apply these exclusions after build dependencies are executed, as a task may create those files and we are not sure what those files may be until after the task has executed.

We delay applying these ivy artifact exclusions until after task dependencies have been visited, so we can ensure the file is present and therefore can apply the filter to the file name.

Baby steps toward fixing #24131

### Reviewing cheatsheet

Before merging the PR, comments starting with 
- ❌ ❓**must** be fixed
- 🤔 💅 **should** be fixed
- 💭 **may** be fixed
- 🎉 celebrate happy things
",2024-12-10T16:08:43Z,jvandort,jvandort,"understandable, easier to read, comprehensible","@bot-gradle test this | I've triggered the following builds:

Pull Request Feedback (Trigger) (Check)

Build Scan | The following builds have failed:

Pull Request Feedback (Trigger) (Check)

Build Scan | @bot-gradle test apt rfm | I've triggered the following builds:

All Performance Tests (Trigger)

Build Scan


Pull Request Feedback (Trigger) (Check)

Build Scan | The following builds have passed:

Pull Request Feedback (Trigger) (Check)

Build Scan | The following builds have passed:

All Performance Tests (Trigger)

Build Scan | @bot-gradle test this | I've triggered the following builds:

Pull Request Feedback (Trigger) (Check)

Build Scan | The following builds have failed:

Pull Request Feedback (Trigger) (Check)

Build Scan","Apply artifact exclusions after build dependencies execute

Some exclusions declared in ivy descriptors may exclude artifacts by name. We can only apply these
exclusions after build dependencies are executed, as a task may create those files and we are not sure
what those files may be until after the task has executed.

We delay applying these ivy artifact exclusions until after task dependencies have been visited, so we
can ensure the file is present and therefore can apply the filter to the file name.

Baby steps toward fixing #24131 | Resolve some review comments

Renamings and javadoc improvements
Replace nested anonymous classes with static named inner classes
Propose new names for some classes
Use a separate identifier implementation instead of implementing the identifier ourselves | Add link to issue describing deprecation and removal of artifact type registry

Also add test to verify behavior of building tasks producing artifacts excluded by name in an ivy descriptor | Do not call getArtifacts within computeIfAbsent

getArtifacts accesses project state. If we want to access project state we while also performing locking,
we must do so within a CalculatedValue, or a cache that is backed by a CalculatedValue, like that returned by
InMemoryCacheFactory. Since ConcurrentHashMap does locking without a CalculatedValue, we ran into deadlocks here.

Moving getArtifacts outside of the locking done by the ConcurrentHashMap avoids the deadlock.

See https://github.com/gradle/gradle-private/issues/4573"
gradle/gradle,30168,https://github.com/gradle/gradle/pull/30168,Make dependency management sanity check more obvious,"This makes the assertion error understandable when encountering it.

### Reviewing cheatsheet

Before merging the PR, comments starting with 
- ❌ ❓**must** be fixed
- 🤔 💅 **should** be fixed
- 💭 **may** be fixed
- 🎉 celebrate happy things
",2024-08-13T13:41:30Z,lptr,lptr,understandable,,"Make dependency management sanity check more obvious

This makes the assertion error understandable when encountering it."
gradle/gradle,25268,https://github.com/gradle/gradle/pull/25268,Update kts snippet: use `coerceAtLeast(1)` util function,"<!--- The issue this PR addresses -->

Minor docs tweak to use a more succinct helper function, [`coerceAtLeast()`](https://kotlinlang.org/api/latest/jvm/stdlib/kotlin.ranges/coerce-at-least.html)

### Context

This helps make the snippet more comprehensible, as it uses a util function which has a more understandable name, and doesn't require knowledge of `takeIf {}` and the elvis operator.

### Contributor Checklist
- [ ] [Review Contribution Guidelines](https://github.com/gradle/gradle/blob/master/CONTRIBUTING.md)
- [ ] Make sure that all commits are [signed off](https://git-scm.com/docs/git-commit#Documentation/git-commit.txt---signoff) to indicate that you agree to the terms of [Developer Certificate of Origin](https://developercertificate.org/).
- [ ] Make sure all contributed code can be distributed under the terms of the [Apache License 2.0](https://github.com/gradle/gradle/blob/master/LICENSE), e.g. the code was written by yourself or the original code is licensed under [a license compatible to Apache License 2.0](https://apache.org/legal/resolved.html).
- [ ] Check [""Allow edit from maintainers"" option](https://help.github.com/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork/) in pull request so that additional changes can be pushed by Gradle team
- [ ] Provide integration tests (under `<subproject>/src/integTest`) to verify changes from a user perspective
- [ ] Provide unit tests (under `<subproject>/src/test`) to verify logic
- [ ] Update User Guide, DSL Reference, and Javadoc for public-facing changes
- [ ] Ensure that tests pass sanity check: `./gradlew sanityCheck`
- [ ] Ensure that tests pass locally: `./gradlew <changed-subproject>:quickTest`

### Gradle Core Team Checklist
- [ ] Verify design and implementation 
- [ ] Verify test coverage and CI build status
- [ ] Verify documentation
",2023-06-19T12:16:57Z,bot-gradle,aSemy,"understandable, comprehensible","Thanks Adam.
The DCO check is failing, could you please follow the instructions to fix this? | @aSemy, we can't accept a contribution with a failing DCO, could you please follow the instructions to fix this? | @eskatos this should be done now. It's a bit inconvenient to add sign off because I made this PR through the GitHub Web UI which doesn't give an option, and the git commands didn't seem to work for me - I had to play around with them a lot. | Yeah, sorry for the hurdle for this small contribution. It looks good now, thank you! | @bot-gradle test and merge | I've triggered a build for you. Click here to see all failures if there's any.","use `coerceAtLeast(1)` util function

Signed-off-by: anon <anon@anon.anon>"
gradle/gradle,21702,https://github.com/gradle/gradle/pull/21702,Polish DefaultAttributesSchema and ComponentAttributeMatcher,"A collection of changes I've wanted to make since getting comfortable with this code. 

- Make `DefaultAttributesSchema.MergedSchema` -- the sole implementation of `AttributeSelectionSchema` -- static. Rename to `DefaultAttributeSelectionSchema`.
  - Move associated `extraAttributesCache` into static class. previous implementation could have improper cache behavior if consumer and producer schemas differed.
  - Updated `equals` and `hashCode` implementation to take into account consumer schema. Previous implementation was incorrect and did not.
  - This class should eventually be moved into its own file so it can be unit tested separately. We're leaving it where it is for now to limit unnecessary changes.
- Remove `DefaultAttributesSchema.DefaultAttributeMatcher` in favor of having `ComponentAttributeMatcher` implement `AttributeMatcher` itself.
  - The existing default implementation was a very light wrapper over `ComponentAttributeMatcher` which captured a `AttributeSelectionSchema` instance. Now, `ComponentAttributeMatcher` saves this instance itself.
- Update caching implementation of `ComponentAttributeMatcher`
  - Remove `schema` as a key to the cache, since each `ComponentAttributeMatcher` has its own schema and thus it will always be the same in each cache key.
  - Better document caching logic by renaming methods/variables and adding comment block explaining the reasoning for caching the way we do
  - Utilize `ConcurrentHashMap.computeIfAbsent` to simplify caching logic and thus make it more understandable.
  - Make candidateList to indices logic list-aware now that we always matching on a list of candidates. This simplifies the logic greatly.
- Optimize `MultipleCandidateMatcher` and `ComponentAttributeMatcher` by removing intermediary List, for performance. ",2023-02-04T03:56:55Z,bot-gradle,jvandort,understandable,I've triggered a build for you. | Pre-tested commit build failed. | @bot-gradle test and merge | I've triggered a build for you.,"Polish DefaultAttributesSchema and ComponentAttributeMatcher

- Make `DefaultAttributesSchema.MergedSchema`, the sole implementation of `AttributeSelectionSchema` static. Rename to `DefaultAttributeSelectionSchema`.
  - Move associated `extraAttributesCache` into static class -- previous implementation could have improper cache behavior if consumer and producer schemas differed.
  - Updated `equals` and `hashCode` implementation to take into account consumer schema. Previous implementation was incorrect and did not.
  - This class should eventually be moved into its own file so it can be unit tested separately. We're leaving it where it is for now to limit unnecessary changes.
- Remove `DefaultAttributesSchema.DefaultAttributeMatcher` in favor of having `ComponentAttributeMatcher` implement `AttributeMatcher` itself.
  - The existing default implementation was a very light wrapper over `ComponentAttributeMatcher` which captured a `AttributeSelectionSchema` instance. Now, `ComponentAttributeMatcher` saves this instance itself.
- Update caching implementation of `ComponentAttributeMatcher`
  - Remove `schema` as a key to the cache, since each `ComponentAttributeMatcher` has its own schema and thus it will always be the same in each cache key.
  - Better document caching logic by renaming methods/variables and adding comment block explaining the reasoning for caching the way we do
  - Utilize `ConcurrentHashMap.computeIfAbsent` to simplify caching logic and thus make it more understandable.
  - Make candidateList to indices logic list-aware now that we always matching on a list of candidates. This simplifies the logic greatly. | Optimize MultipleCandidateMatcher and ComponentAttributeMatcher by removing intermediary List

Previously, MulitpleCandidateMatcher would create a list of matches from its indices, just for ComponentAttributeMatcher to turn that back into indices.
We remove the intermediary list and now have MulitpleCandidateMatcher return an index list, as a performance optimization"
gradle/gradle,23438,https://github.com/gradle/gradle/pull/23438,Simplify building of synthetic dependencies,"In order to resolve a configuration, `DependencyGraphBuilder` first builds the root component's metadata and then fetches the configuration's metadata from the root metadata. This causes the resolution engine to interpret a resolvable configuration as a variant. We want to reduce the reliance of the resolution engine on the root component's metadata in order to eventually decouple the resolution of a resolvable configuration from the metadata which is traditionally used to describe consumable components.

This PR is a first step in reducing the reliance on the root component metadata by adding a `getSyntheticDependencies` method directly to `ResolveContext`. Then, we removed the same-named method from all Component metadata types. This method is only useful for the configuration being resolved, so it should not be present on the metadata of any other component or variant other than the root configuration. Further, this change allows us to remove the `RootConfigurationMetadata` and `RootLocalComponentMetadata` specialization types, thus making the root component metadata more similar to a traditional component's metadata. 
",2023-01-13T18:41:29Z,bot-gradle,jvandort,understandable,"@tresat In this case Local refers to the fact that the component for which the metadata describes is a ""local"" component, or resides on the current machine. This is in opposition to a non-local component which are conventionally fetched from a repository (maven/ivy).
If you look at the superclass, ComponentGraphResolveMetadata, you can see it also defines the getConfiguration method as well, so really all of these metadatas can be configuration-based.
In the medium term, I see this type moving away from its reliance on configurations towards variant based resolution. | If you look at the superclass, ComponentGraphResolveMetadata, you can see it also defines the getConfiguration method as well, so really all of these metadatas can be configuration-based.

Ah, ok.  It's just there to narrow the return type.
I still don't like Local, but maybe it just needs better docs. | @bot-gradle test this | I've triggered the following builds for you:

PullRequestFeedback build | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | I've triggered a build for you.","Simplify building of synthetic dependencies

In order to resolve a configuration, DependencyGraphBuilder first builds the root component's metadata and then
fetches the configuration's metadata from the root metadata. This causes the resolution engine to interpret a
resolvable configuration as a variant. We want to reduce the reliance of the resolution engine on the root component's
metadata in order to eventually decouple the resolution of a resolvable configuration from the metadata which is
traditionally used to describe consumable components.

This PR is a first step in reducing the reliance on the root component metadata by adding a getSyntheticDependencies method
directly to ResolveContext. Then, we can remove the same-named method from all Component metadata types. This method is only
useful for the configuration being resolved, so it should not be present on the root component. Further, this change allows
us to remove the RootConfigurationMetadata and RootLocalComponentMetadata specialization types, thus making the root component
metadata more similar to a traditional component's metadata. | Rename getSyntheticDependencies to generateLockDependencyConstraints | Use the synthetic dependencies terminology"
gradle/gradle,22098,https://github.com/gradle/gradle/pull/22098,Fix attribute bug for component metadata rules,"Previously, any action which modifies the result of getAttributes from the action passed into 'withVariant' or 'allVariants', or any of the other methods in 'ComponentMetadataDetailsAdapter' which delegate to those methods, would always modify the attributes of all variants, regardless of whether a specific variant was specified. This is fixed by ensuring the result of 'getAttributes()' returns a unique mutable attribute container for any individual variant whose parent is the component's attributes.

Along the way, implements a 'ImmutableAttributesFactory#join' method which joins two attribute containers in a parent-child relationship where any updates in the parent or child are reflected in the joined container. This implementation was based on the parent functionality already implemented for 'DefaultMutableAttributeContainer'.",2022-09-27T05:12:05Z,bot-gradle,jvandort,understandable,"@bot-graddle test this | @bot-gradle test this | OK, I've already triggered the following builds for you:

PullRequestFeedback build | @bot-gradle test and merge | OK, I've already triggered a build for you.","Fix attribute bug for component metadata rules

Previously, any action which modifies the result of getAttributes from the action passed into
'withVariant' or 'allVariants', or any of the other methods in 'ComponentMetadataDetailsAdapter' which
delegate to those methods, would always modify the attributes of all variants, regardless of whether
a specific variant was specified. This is fixed by ensuring the result of 'getAttributes()' returns a
unique mutable attribute container for any individual variant whose parent is the component's attributes.

Along the way, implements a 'ImmutableAttributesFactory#join' method which joins two attribute containers
in a parent-child relationship where any updates in the parent or child are reflected in the joined container.
This implementation was based on the parent functionality already implemented for 'DefaultMutableAttributeContainer'.

Also added '@Deprecated' annotations to 'ImmutableAttributes' to give immediate feedback to developers using
these classes that these instances cannot be mutated. This is similar to how Guava immutable types handle
an immutable implementation of a mutable interface. | Write test for joined attribute container

Write test to verify we can modify getAttributes() | Review updates

* Remove deprecated annotations on mutable methods for immutable containers
* Rename JoinedAttributeContainer to HierarchicalAttributeContainer
* Improve documentation
* Rename test fixture from id to gav
* Add tests which test Providers | Rename parent/child to fallback/primary"
gradle/gradle,20421,https://github.com/gradle/gradle/pull/20421,Fix incorrect output detection under the presence of filtered inputs,Fixes part of #20391 by invalidating the output locations just before we snapshot the outputs.,2022-04-13T11:02:31Z,bot-gradle,wolfs,understandable,"@bot-gradle test and merge | OK, I've already triggered a build for you.","Add test for #20391

This recreates the issue we had in our build with respect to filtered file trees
and jar tasks. See #20366 for our concrete problem. | Simple fix for #20391

By calling `beforeOutputChange` just before snapshotting the
outputs we ensure that we get an up-to-date version of the
outputs. | Fix test with configuration caching | Add a factory method for preparing and capturing outputs

so it isn't possible anymore to do this in the wrong order, by
e.g. moving the broadcast ""braket"" out of the capture outputs
step. | Use try-finally for BroadcastChangingOutputsStep | Revert ""Add a factory method for preparing and capturing outputs""

This reverts commit 626158e223a01a04936a88dace728d0535df1a51. | Use ChangesOutputContext for right order of steps

Now all the steps which change the outputs need to be within
the BroadcastChangingOutputsStep bracket. | Use the type system to ensure the right order

between `BroadcastChangingOutputsStep` and `CaptureStateAfterExecutionStep`. | Improve Javadoc

Co-authored-by: Lóránt Pintér <lorant@gradle.com> | Merge BroadcastChangingOutputsStep into CaptureStateAfterExecutionStep | Rename {Changes -> Changing}OutputContext | Polish integration test"
gradle/gradle,17547,https://github.com/gradle/gradle/pull/17547,Treat non-existing files as regular files when detecting missing task dependencies,"This allows excluding/including non-existing files via `**/...` patterns.

Fixes #17561",2021-06-29T09:05:04Z,bot-gradle,wolfs,understandable,"@bot-gradle test this | OK, I've already triggered ReadyForMerge build for you. | @lptr PTAL! | @bot-gradle test and merge | OK, I've already triggered a build for you.","Treat non-existing files as regular files when detecting missing task dependencies

This allows excluding non-existing files via `**/...` includes.

See #16980 | Address review feedback | Fix unit test"
gradle/gradle,15915,https://github.com/gradle/gradle/pull/15915,External plugin validation,"This commit introduces a new plugin which allows validating
external plugins. Basically, it performs the same checks as
the Gradle Plugin Plugin validation but instead of doing it
on the plugin under development, this version applies the
validation on the plugins which are applied to the project
itself.
",2021-01-27T15:53:35Z,melix,melix,understandable,"@bot-gradle test RFM | OK, I've already triggered ReadyForMerge build for you. | I'd be keen on merging this once CI is green and pursuing with improvements/more smoke tests in separate PRs. | @bot-gradle test RFM | OK, I've already triggered ReadyForMerge build for you.","Spike external plugin validation

This commit introduces a new plugin which allows validating
external plugins. Basically, it performs the same checks as
the Gradle Plugin Plugin validation but instead of doing it
on the plugin under development, this version applies the
validation on the plugins which are applied to the project
itself. | Improved handling of suspicious cases

Log whenever we cannot find the jar associated to a plugin, and make it
so that we can still perform analysis of a plugin if we cannot identify
its plugin id from the pluginclass itself. | Rework how smoke tests validate external plugins

We will now make sure that we validate all external plugins, and make
sure that if there's a validation problem, the actual list of problems
is tested. If there are more than one plugin applied, then we also
make sure that we verify all of them. | Merge branch 'master' into cc/exec/validate-external-plugins | Rework how validation is performed in smoke tests

There are now 2 different base classes for validating plugins in smoke
tests:

   - the `AbstractPluginValidatingSmokeTest` class is for the general
case where a smoke test may verify different versions of multiple
plugins (for example look at the Nebula plugins smoke test)
   - the `AbstractSinglePluginValidatingSmokeTest` is a specialization
of this test in case a single plugin is smoke tested (it can still use
multiple versions) | Merge remote-tracking branch 'upstream/master' into cc/exec/validate-external-plugins | Add smoke tests for popular plugins

This adds a number of smoke tests for popular plugins. Currently
only limited to validation of plugins. | Add more plugin validation smoke tests

and remove the ""single"" version of the plugin validation test class
as it wasn't so helpful and quite verbose. | Fix smoke tests"
gradle/gradle,29532,https://github.com/gradle/gradle/pull/29532,Update Isolated Projects page in User Manual,"### Details
This is a Documentation change ONLY.

### Context
This is an update for the User Manual page for Isolated Projects.",2024-06-13T19:13:37Z,lkasso,lkasso,clarity,"@bamboo, any additional changes for 8.9RC1?",initial commit | feedback from @bamboo
gradle/gradle,30506,https://github.com/gradle/gradle/pull/30506,Setup only reporting resolution failures as problems when enabled,"Potential fix for #30442.  Replaces #30474.

This solves the problem of many, many unuseful Problems being reported whenever an `ArtifactView` with `lenient = true` is used.  It's important to avoid reporting these prior to releasing 8.11, to avoid spamming anyone using common plugins that do this with false positive errors.  

This adds a `ReportableAsProblem` interface for dependency management exceptions to implement.  These exceptions will **not** be immediately reported as problems when they are created, but only when the exception would have been thrown and reported.

We check to report failures: 
- When creating a `DefaultArtifactCollection`, where previously we would have thrown the failure on non-lenient access
- In `ResolutionBackedFileCollection`, when visiting dependencies, where previously we would have thrown the failure on non-lenient access
- In `DefaultLenientConfguration`, and `DefaultResolvedConfiguration`, when getting files or artifacts, where previously we would have thrown the failure all the time",2024-09-19T17:14:30Z,bot-gradle,tresat,clarity,"@bot-gradle test QFL | I've triggered the following builds for you. Click here to see all build failures.

Quick Feedback - Linux Only (Trigger) (Check) | @bot-gradle test QFL | I've triggered the following builds for you. Click here to see all build failures.

Quick Feedback - Linux Only (Trigger) (Check) | @bot-gradle test QFL | I've triggered the following builds for you. Click here to see all build failures.

Quick Feedback - Linux Only (Trigger) (Check) | @bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

Pull Request Feedback (Trigger) (Check) | @bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

Pull Request Feedback (Trigger) (Check) | @bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

Pull Request Feedback (Trigger) (Check) | @bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

Pull Request Feedback (Trigger) (Check) | @bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

Pull Request Feedback (Trigger) (Check) | @bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

Pull Request Feedback (Trigger) (Check) | @bot-gradle test and merge","Defer reporting dependency resolution failures as problems until we know the failure wouldn't be suppressed

This adds a ReportableAsProblem interface for dependency management exceptions to implement. These exceptions will not be immediately reported as problems when they are created, but only when the exception would have been thrown and reported.

# Conflicts:
#	platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/ivyservice/resolveengine/artifact/ArtifactSetToFileCollectionFactory.java

# Conflicts:
#	platforms/software/dependency-management/src/integTest/groovy/org/gradle/integtests/internal/component/resolution/failure/ResolutionFailureHandlerIntegrationTest.groovy
#	platforms/software/dependency-management/src/main/java/org/gradle/internal/component/resolution/failure/ResolutionFailureHandler.java | Update test setup | Update platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/ConfigurationResolver.java

Co-authored-by: Justin <jvandort@gradle.com>
Signed-off-by: Tom Tresansky <tom.tresansky@gmail.com> | Update platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/ConfigurationResolver.java

Co-authored-by: Justin <jvandort@gradle.com>
Signed-off-by: Tom Tresansky <tom.tresansky@gmail.com> | Simplify DefaultResolutionHost problems | Remove generic type argument from ReportableAsProblem, as it was unused

# Conflicts:
#	platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/ivyservice/resolveengine/artifact/ArtifactSetToFileCollectionFactory.java | Add note about potential refactoring | Narrow return type for increased precision | Simplify traversal code | Assume non-lenient when selecting artifacts using default visited artifact set

Per Justin:
This is the legacy way of specifying leniency. LenientConfiguration uses this to be lenient -- though with modern constructs (ResolutionBackedFileCollection) we specify leniency by either ignoring or rethrowing exceptions at the time of visiting these selected results.

Stated more concisely, legacy mechanism request leniently during selection. Modern mechanism ask selection to be strict, then ignore the failures later if its a lenient resolution. | Revert some meaningless changes | Fix rebase problems with method renames | Keep track of seen failures in case of reference cycles | Remove unnecessary import | Inject problems service into constructor of DefaultConfiguration

This avoids referencing the service via the configuration factory instance, like the many other services a configuration requires. | Update injection after rebase and report additional data with problem"
gradle/gradle,29008,https://github.com/gradle/gradle/pull/29008,Support polymorphism in Tooling API model interfaces,,2024-05-10T17:34:49Z,jbartok,donat,clarity,"What are the use-cases for TAPI model polymorphism? Shouldn't TAPI models just represent flat data? | What are the use-cases for TAPI model polymorphism? Shouldn't TAPI models just represent flat data?

One type of use-case is pre-existing models, please see

#29038

Another use case popped up in Declarative DSL related changes, where we at first had to flatten the model interfaces but would like to revert that, because it's very unnatural. See

#28657

Example interfaces which are flat at the moment of writing, but should be reverted to polymorphic by the time the above PR is ready:

org.gradle.declarative.dsl.schema.DataType
org.gradle.declarative.dsl.schema.FunctionSemantics | LGTM; approved. (I cannot approve my own changes, I can only click 'comment' in the 'finish your review' dialog).

Thank you! | @bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

PullRequestFeedback build | @bot-gradle test and merge | The merge queue build has failed. Click here to see all failures.",Add test exhibiting desired behavior | Introduce annotation to declare tooling API contract for subtypes | Make interface detection non-shallow | Add test for nested model case | Convert test to cross-version test | Write javadoc for marker annotation | Refactor the useful code a bit | Address review concerns | Limit target gradle version for the test
gradle/gradle,25993,https://github.com/gradle/gradle/pull/25993,providing actionable guidance in MissingLockStateException,"<!--- The issue this PR addresses -->
<!-- Fixes #25756 -->

### Context
This change offers more clarity for Users: The updated exception message offers actionable guidance to users encountering the specific scenario where a locked configuration lacks a lock state. This will help users understand the issue better and know exactly what action to take to resolve it.

Fixes #25756

### Contributor Checklist
- [x] [Review Contribution Guidelines](https://github.com/gradle/gradle/blob/master/CONTRIBUTING.md)
- [x] Make sure that all commits are [signed off](https://git-scm.com/docs/git-commit#Documentation/git-commit.txt---signoff) to indicate that you agree to the terms of [Developer Certificate of Origin](https://developercertificate.org/).
- [x] Make sure all contributed code can be distributed under the terms of the [Apache License 2.0](https://github.com/gradle/gradle/blob/master/LICENSE), e.g. the code was written by yourself or the original code is licensed under [a license compatible to Apache License 2.0](https://apache.org/legal/resolved.html).
- [x] Check [""Allow edit from maintainers"" option](https://help.github.com/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork/) in pull request so that additional changes can be pushed by Gradle team
- [ ] Provide integration tests (under `<subproject>/src/integTest`) to verify changes from a user perspective
- [x] Provide unit tests (under `<subproject>/src/test`) to verify logic
- [x] Update User Guide, DSL Reference, and Javadoc for public-facing changes
- [x] Ensure that tests pass sanity check: `./gradlew sanityCheck`
- [x] Ensure that tests pass locally: `./gradlew <changed-subproject>:quickTest`

### Reviewing cheatsheet

Before merging the PR, comments starting with 
- ❌ ❓**must** be fixed
- 🤔 💅 **should** be fixed
- 💭 **may** be fixed
- 🎉 celebrate happy things
",2023-08-02T22:18:38Z,bot-gradle,ahmedehabb,clarity,@bot-gradle test and merge | Your PR is queued. See the queue page for details. | I've triggered a build for you. Click here to see all failures if there's any.,"providing actionable guidance in MissingLockStateException

Signed-off-by: ahmedehabb <Ahmed.bahloul00@eng-st.cu.edu.eg>"
gradle/gradle,25484,https://github.com/gradle/gradle/pull/25484,Enable load after store CC tests in logging subproject,I ignored the tests that are not yet supported by the configuration cache.,2023-06-26T13:07:58Z,bot-gradle,wolfs,clarity,I've triggered a build for you. Click here to see all failures if there's any. | Pre-tested commit build failed. Click here to see all build failures. | @bot-gradle test and merge | I've triggered a build for you. Click here to see all failures if there's any.,"Enable load after store for tests in logging | Mark ""can group output from custom test listener with task"" as CC incompatible

beforeTest/afterTest doesn't work yet with CC. | Mark `sample*Logging` tests as CC incompatible

The log level needs to be captured per task by CC. | Convert test to Spock | Make LoggingIntegrationTest more idiomatic Spock | Separate test for custom logger"
gradle/gradle,25648,https://github.com/gradle/gradle/pull/25648,Skip too new versioned directories of multi-release JARs when instrumenting,"ASM library that we use for bytecode instrumentation has an upper limit to the bytecode version it can understand.
When processing multi-release JARs, the restriction caused errors even when running on the supported JVM, because instrumentation was processing all classes inside the JAR regardless of whether it was multi-release or not: e.g. JAR with versioned directories for Java 21 and Java 11 was still problematic even when running on Java 11 JVM, where new version of classes compiled for Java 21 aren't actually used.

This PR rewrites the instrumentation code to skip all versioned directories which aren't supported. The resulting instrumented JARs only contain versioned directories up to latest ASM-supported Java version. This way the cached transformed JAR is independent of the version of the JVM running Gradle.

However, there's a hole left when the build runs on the not-yet-supported-by-asm (and, therefore, Gradle) JVM, as it can then load non-instrumented classes from the versioned directories of the original JARs. This is going to be addressed in some follow-up PR.

Part of #25434.
",2023-07-12T17:03:30Z,bot-gradle,mlopatkin,"clarity, easier to read","I think this should have a test that the MRJAR entries are ignored, though I'm not sure how difficult that would be to add. | I think this should have a test that the MRJAR entries are ignored, though I'm not sure how difficult that would be to add.

Thanks, I've added unit test coverage. Setting up a proper integration test is harder, I don't think there is enough ROI. | @bot-gradle test RfR | I've triggered the following builds for you. Click here to see all build failures.

ReadyForRelease build | @bot-gradle test and merge | I've triggered a build for you. Click here to see all failures if there's any.","Refactor InstrumentingClasspathFileTransformer

This commit encapsulates all transformation logic into a separate class
instead of pulling out pieces of it into the policy instance. Policy now
serves as a factory for transformations.

The refactoring paves way to extracting more info from JAR when
preparing the instrumentation strategy. | Filter unsupported versioned directories when instrumenting MRJARs

ASM can only process class files up to a certain version. This can be
problematic for multi-release JARs containing newer classes in the
appropriate versioned directory. Such JARs can be used on a
Gradle-supported JVM, but prior to this change, instrumentation failed
even if the unsupported classes were never loaded.

This commit makes instrumentation skip unsupported versioned
directories. For legacy instrumentation, these directories are filtered
out completely. For agent-based instrumentation, classes of the
unsupported versions are not instrumented and aren't placed into the
transformed archive.

Unfortunately, ASM doesn't provide any kind of constant to specify the
latest supported class file version, so we have to keep track of this
manually. | Clarify some corner case in TransformReplacer | Do not use JavaVersion in AsmConstants

In some cases JavaVersion class may come from a TAPI client which can be
outdated and lack necessary constants. | Add test coverage for multi-release jar instrumentation"
gradle/gradle,23612,https://github.com/gradle/gradle/pull/23612,Preparatory work for making version catalogs work in kotlin-dsl `plugins {}` block,"This PR is preparatory work for
* https://github.com/gradle/gradle/issues/22797

It does the following:
* unify the use of `PluginDependenciesScope` for kts `plugins {}` block in script templates
* unify the use of `ScriptHandlerScope` for kts `buildscript {}` block in script templates
* deprecate `ScriptHandlerScopeDelegate`
* add deprecation nagging for deprecated delegates
* add more test coverage for `PartialEvaluator`
* fix some version catalog tests for the configuration cache
* fix and refine kdoc and comments around kotlin-dsl compilation and evaluation
* rename members in `ResidualProgramCompiler` for clarity
* fix various formatting to limit future changesets

See individual commits for more details.",2023-01-27T16:28:51Z,bot-gradle,eskatos,clarity,"@bot-gradle test RFN | I've triggered the following builds for you:

ReadyForNightly build | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | I've triggered a build for you.","Let compiled standalone kts script templates use plugins {} scope

in order to match the compilation and ide script templates

Signed-off-by: Paul Merlin <paul@gradle.com> | Let precompiled kts script templates use plugins {} scope

in order to match the standalone script templates

Signed-off-by: Paul Merlin <paul@gradle.com> | Fix formatting in PluginAccessorsClassPath.kt

Signed-off-by: Paul Merlin <paul@gradle.com> | Fix version catalogs tests for the configuration cache

Signed-off-by: Paul Merlin <paul@gradle.com> | Polish KotlinDslVersionCatalogExtensionIntegrationTest

by simplifying scripts under test

Signed-off-by: Paul Merlin <paul@gradle.com> | Add and refine kdoc and comments in kotlin-dsl

Signed-off-by: Paul Merlin <paul@gradle.com> | Remove usage of ScriptHandlerDelegate and deprecate it

Signed-off-by: Paul Merlin <paul@gradle.com> | Simplify usage of ScriptHandlerScope

for uniformity with PluginDependenciesSpecScope

Signed-off-by: Paul Merlin <paul@gradle.com> | Deprecate delegates for Gradle, Settings, Project and ScriptHandler

Signed-off-by: Paul Merlin <paul@gradle.com> | Add more coverage to PartialEvaluatorTest

Signed-off-by: Paul Merlin <paul@gradle.com> | Rename `precompiled` to `compiled` in kts `execution` package for clarity

This refers to compiled classes for standalone script fragments only.
It doesn't refer to ""precompiled script plugins"".

This served as a basis to come up with the ""precompiled script plugins"" name in the past.
The usage of the name ""precompiled"" in this context is confusing.
Using ""compiled"" in this context is valid.

Signed-off-by: Paul Merlin <paul@gradle.com> | Fix conditional deprecation nagging in delegates

Note that there isn't any direct tests for these deprecations as using them requires implementing
an Kotlin internal function and lots of Closure taking methods.

Signed-off-by: Paul Merlin <paul@gradle.com> | Add more coverage in PartialEvaluatorTest

Now with complete coverage"
gradle/gradle,22802,https://github.com/gradle/gradle/pull/22802,Consider all variants when finding artifact transform chains,"The previous implementation would attempt to construct artifact transform chains for each variant. This would result in situations where we would construct an inefficient chain where an intermediary transformed variant could be provided by an existing producer variant.

To resolve this, we update the algorithm to consider all producer variants instead of being run against a single producer variant. We update the algorithm to search breadth-first instead of depth-first in order to ensure we prioritize shorter artifact transform chains over longer ones.

Some of the previous disambiguation behavior was maintained.
If two transformed variants have attributes such that for any given key they do not have a different value, we consider those variants to be matching. We perform disambiguation by always choosing the last candidate, and then include any other candidate which is not matching with the last candidate. This in some way hides ambiguity, though we keep this behavior to not affect existing transform chains.

Finally, added functionality to remove transforms from consideration if they have been used previously in a chain. This fixes a bug where two transforms which reference each other's attributes circularly would cause a ""Recursive Update"" error related to using concurrent hash maps. We would previously otherwise have likely run into a stack overflow error if there was no alternative solution in the transformation graph.

Fixes #22735
",2022-11-30T12:11:55Z,bot-gradle,jvandort,clarity,"@wolfs Not looking for a review, but heads up about some changes going on in the dependency management side of finding transform chains. | @bot-gradle test RfR | OK, I've already triggered the following builds for you:

ReadyForRelease build | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | OK, I've already triggered a build for you.","Consider all variants when finding artifact transform chains

The previous implementation would attempt to construct artifact transform chains for each variant. This
would result in situations where we would construct an inefficient chain where an intermediary transformed
variant could be provided by an existing producer variant.

To resolve this, we update the algorithm to consider all producer variants instead of being run against a single
producer variant. We update the algorithm to search breadth-first instead of depth-first in order to ensure we
prioritize shorter artifact transform chains over longer ones.

Some of the previous disambiguation behavior was maintained.
If two transformed variants have attributes such that for any given key they do not have a different value, we consider those
variants to be matching. We perform disambiguation by always choosing the last candidate, and then include any other
candidate which is not matching with the last candidate.
This in some way hides ambiguity, though we keep this behavior to not affect existing transform chains.

Finally, added functionality to remove transforms from consideration if they have been used previously in a chain. This
fixes a bug where two transforms which reference each other's attributes circularly would cause a ""Recursive Update"" error
related to using concurrent hash maps. We would previously otherwise have likely run into a stack overflow error if there
was no alternative solution in the tranformation graph. | Minor changes to tests | Introduce an assertion method to check transform chains

Also rename local variables to make their purpose clearer | Rename more attribute sets in test to clarify the purpose of them | Change attribute values in ConsumerProvidedVariantFinderTest to match their purpose | Add explanations to expected matches for transform selection"
gradle/gradle,21697,https://github.com/gradle/gradle/pull/21697,Create compile elements,"Create a new `compileElements` consumable configuration, intended initially to be used by test suites to create unit-test-like test suites. Eventually, we have plans to migrate the default test suite to use this configuration. 

`compileElements` acts much like `apiElements` except it provides the ""complete view"" of a project during compile time instead of the ""api view"" which projects normally consume during compile-time. This is particularly useful for unit tests, but can also prove useful for writing test fixtures or implementing analytics plugins. By complete view, we mean the entirety of the compile classpath, including `api`, `implementation`, `compileOnly`, and `compileOnlyApi`. 

To implement this, we add a new attribute, `CompileView`, which is only applicable when resolving a compile classpath -- or in other words, is only applicable when combined with `Usage#JAVA_API`. We additionally add attribute schema compatibility rules in the `jvm-ecosystem` plugin to ensure that the current behavior remains consistent -- that is, if the `CompileView` attribute is not requested, the default vale of the attribute equates to `CompileView#JAVA_API`, which will resolve `apiElements`. 

This new configuration will not be published. The intention is not to resolve this configuration from outside of the project, though this pattern is not explicitly forbidden. We do not restrict users from doing so, however we do not provide any means, via the DSL, to depend on another project's complete compile view. Power users can accomplish this, if they wish, by manually constructing a dependency with the proper attributes, however this is not encouraged for any known use case. 

- [x] Fix `ThirdPartyGradleModuleMetadataSmokeTest`
- [x] Write more tests in `JavaEcosystemAttributeMatcherTest`
- [x] Ensure we can resolve the runtime classpath when we have a dependency with a `CompileVIew.JAVA_COMPLETE` attribute",2022-09-10T02:13:52Z,bot-gradle,jvandort,clarity,"Please ignore the failing test, is unrelated is happens when running the Master CI builds against the release branch.
Passing build: https://builds.gradle.org/buildConfiguration/Gradle_Release_Check_Stage_PullRequestFeedback_Trigger/55608529?focusLine=0&hideProblemsFromDependencies=false&hideTestsFromDependencies=false | @bot-gradle test RfR | OK, I've already triggered the following builds for you:

ReadyForRelease build | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | OK, I've already triggered a build for you.","Create the CompileView attribute

Update JavaEcosystemAttributesDescriber to support new attribute
Add compatibility and disambiguation rules in JvmEcosystemSupport so that CompileView.JAVA_API is the default | Create compileElements

Create a new consumable configuration representing the compile classpath of the main source set
Use the CompileView attribute to differentiate it from apiElements.
Update Maven derived variants to use CompileView.JAVA_COMPILE for its compile scope variant | Action items

Updated documentation
Used proper consumable/resolvable flags on configuration
Update access modifiers to compatibility and disambiguration rules"
gradle/gradle,21830,https://github.com/gradle/gradle/pull/21830,Create projectInternalView() dependency method in JvmComponentDepenencies,"Adds a method which creates a dependency on the current project, except with an additional CompileView#JAVA_COMPLETE attribute.
",2022-09-16T13:41:53Z,bot-gradle,jvandort,clarity,"@bot-gradle test this | OK, I've already triggered the following builds for you:

PullRequestFeedback build | @bot-gradle test and merge | OK, I've already triggered a build for you.","Create projectComplete() dependency method in JvmComponentDepenencies

Adds a method which creates a dependency on the current project, except with an additional CompileView#JAVA_COMPLETE attribute | Rename projectComplete to projectInternalView | Apply recommended doc updates | Add projectInternalView() documentation | Improve projectInternalView() documentation

Add new sample which shows difference between project() and projectInternalView()
Update tests to be more clear"
gradle/gradle,22206,https://github.com/gradle/gradle/pull/22206,Implement injectable shared build services,"Fixes: #16168

Expands on #21754 to use conventions to resolve services by name
Adds buildService snippet variant that uses ServiceReference

### New `@ServiceReference` annotation
Properties can now be marked as service references.

* without a string value, dispenses with the need to call `Task.usesService()`, but a value needs to be set before the property can be used to access the service
* with a string value (the name of the service being consumed), is similar to setting the corresponding build service provider reference as a convention, dispensing with the need to set a value, and to call `Task.usesService()`
* service references are ignored for any input/output up-to-date checking, just like `@Internal` properties
* Suggestions of a better name are welcome.

### New ""smart collection"" for handling a task's required services

* Handles service references declared using the new annotation or explicitly set using `Task.usesService()` uniformly (see `DetaultTaskRequiredServices`)
* Expanded `PropertyVisitor` and new `PropertyAnnotationHandler` to support visiting properties marked as service references (in addition to inputs, outputs etc) via annotation or using the runtime API

### Changes to `BuildServiceRegistry`  and `BuildServiceProvider`

* Refactored `BuildServiceProvider` into two subclasses: `RegisteredBuildServiceProvider` and `ConsumedBuildServiceProvider `- the former plays the role of the original BSP, namely instantiate the underlying service lazily from a reference obtained directly from a service registration, and the latter allows a consumer to resolve a build service reference based the service name and type alone. 
* Moved obtaining a `SharedResource` from a build service provider from `AbstractTask` to `BuildServiceRegistry`. 

### Configuration cache implications

* When storing build service provider information, we can store either a `ConsumedBuildServiceProvider` (unresolved) or `RegisteredBuildServiceProvider`(resolved) - when restoring, we always read as resolved providers
* When storing build service providers required by a task, we store both those set explicitly (using Task.usesService()` or declared via annotation as if they were originally set explicitly. 

### Additional changes

* Lazily setting a convention to the build service property via instrumentation to double lazily resolve to the registered service provider. 
* validation of properties marked as service references and corresponding diagnostic message tests
* one new snippet showing off the new feature
* new integration tests validating the new service declaration syntax 
* updates to unit tests that needed upkeeping to handle the new smart collection for managing required services for a task

",2022-11-14T21:07:21Z,bot-gradle,abstratt,clarity,"I left a few comments. My main questions is why we are introducing our own annotation instead of reusing @javax.inject.Inject and @javax.inject.Named for this?

Thanks, @lptr! I answered your design question in the spec document itself. Feel free to continue the conversation on that topic there.
Really appreciate your points about implementation issues you identified. | One more thing: should we add build dependencies of @ServiceReference inputs as task dependencies? If so, we need to visit it here:

  
    
      gradle/subprojects/core/src/main/java/org/gradle/api/internal/tasks/DefaultTaskInputs.java
    
    
        Lines 192 to 215
      in
      46be65e
    
  
  
    

        
          
           @Override 
        

        
          
           public void visitDependencies(TaskDependencyResolveContext context) { 
        

        
          
               TaskPropertyUtils.visitProperties(propertyWalker, task, new PropertyVisitor.Adapter() { 
        

        
          
                   @Override 
        

        
          
                   public void visitInputProperty(String propertyName, PropertyValue value, boolean optional) { 
        

        
          
                       context.add(value.getTaskDependencies()); 
        

        
          
                   } 
        

        
          
            
        

        
          
                   @Override 
        

        
          
                   public void visitInputFileProperty( 
        

        
          
                       final String propertyName, 
        

        
          
                       boolean optional, 
        

        
          
                       InputBehavior behavior, 
        

        
          
                       DirectorySensitivity directorySensitivity, 
        

        
          
                       LineEndingSensitivity lineEndingSensitivity, 
        

        
          
                       @Nullable Class<? extends FileNormalizer> fileNormalizer, 
        

        
          
                       PropertyValue value, 
        

        
          
                       InputFilePropertyType filePropertyType 
        

        
          
                   ) { 
        

        
          
                       FileCollection actualValue = FileParameterUtils.resolveInputFileValue(fileCollectionFactory, filePropertyType, value); 
        

        
          
                       context.add(actualValue); 
        

        
          
                   } 
        

        
          
               }); 
        

        
          
           } | One more thing: should we add build dependencies of @ServiceReference inputs as task dependencies? If so, we need to visit it here:

  
    
      gradle/subprojects/core/src/main/java/org/gradle/api/internal/tasks/DefaultTaskInputs.java
    
    
        Lines 192 to 215
      in
      46be65e
    
  
  
    

        
          
           @Override 
        

        
          
           public void visitDependencies(TaskDependencyResolveContext context) { 
        

        
          
               TaskPropertyUtils.visitProperties(propertyWalker, task, new PropertyVisitor.Adapter() { 
        

        
          
                   @Override 
        

        
          
                   public void visitInputProperty(String propertyName, PropertyValue value, boolean optional) { 
        

        
          
                       context.add(value.getTaskDependencies()); 
        

        
          
                   } 
        

        
          
            
        

        
          
                   @Override 
        

        
          
                   public void visitInputFileProperty( 
        

        
          
                       final String propertyName, 
        

        
          
                       boolean optional, 
        

        
          
                       InputBehavior behavior, 
        

        
          
                       DirectorySensitivity directorySensitivity, 
        

        
          
                       LineEndingSensitivity lineEndingSensitivity, 
        

        
          
                       @Nullable Class<? extends FileNormalizer> fileNormalizer, 
        

        
          
                       PropertyValue value, 
        

        
          
                       InputFilePropertyType filePropertyType 
        

        
          
                   ) { 
        

        
          
                       FileCollection actualValue = FileParameterUtils.resolveInputFileValue(fileCollectionFactory, filePropertyType, value); 
        

        
          
                       context.add(actualValue); 
        

        
          
                   } 
        

        
          
               }); 
        

        
          
           } 
        
    
  



@lptr Do you mean, dependencies between tasks consuming services and tasks publishing such services?
I ask, because services have no relationships to any task. They are just singletons that may or not be out there. | @lptr Do you mean, dependencies between tasks consuming services and tasks publishing such services?
I ask, because services have no relationships to any task. They are just singletons that may or not be out there.

Whatever build dependency is there. Maybe your service has a parameter that is a Provider backed by a task or something. We shouldn't speculate here, but instead make sure that if there are dependencies, they are discovered. | ❓ Another thing I noticed is that currently only tasks can declare services used like this. Aren't transforms allowed to consume services? | @lptr Do you mean, dependencies between tasks consuming services and tasks publishing such services?
I ask, because services have no relationships to any task. They are just singletons that may or not be out there.

Whatever build dependency is there. Maybe your service has a parameter that is a Provider backed by a task or something. We shouldn't speculate here, but instead make sure that if there are dependencies, they are discovered.

I see. We have no test for that AFAIK, and my gut tells me such a dependency would not be visible. I will look into setting a test up for that. | ❓ Another thing I noticed is that currently only tasks can declare services used like this. Aren't transforms allowed to consume services?

Sort of - for transform parameters. There is no enforcement of parallel usage though (ATM).
See:
gradle/configuration-cache#156
I also added this test case. | I left a few comments. My main questions is why we are introducing our own annotation instead of reusing @javax.inject.Inject and @javax.inject.Named for this?

Thanks, @lptr, both options have their merit, I hope you agree the approach we took is not patently inferior to using Inject/Named (even more so considering we have use cases that do not involve resolving a name or injecting a value, so we do need the annotation anyways), and we can change that any time before we ship 8.0, it is more of a cosmetic choice (wihci of course has long term implications once released). Thanks for your design feedback on the Spec itself, happy to continue the discussion there.
I applied your suggestions regarding the implementation itself. | OK, I've already triggered a build for you. | Pre-tested commit build failed. | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | OK, I've already triggered a build for you.","Implement injectable shared build services

* Validate ServiceReference properties
* Adopt @ServiceReference in snippets
* Add test for @ServiceReference under CC
* Adjust test fixtures in unrelated test cases
* Use ASM-based instrumentation to set convention
Introduce the idea of consumer/registered build services providers
(the latter resolving lazily to the former).

Fixes: #16168 | Honor @Optional vs @ServiceReference

* Clean up DefaultTaskRequiredServices

Remove support for visiting explicity registered services as there is
no use case for that yet.

* Add test case for service refs in @Nested beans

Allow services of type BuildServiceRegistry to be available for
injection into instances of type BuildService.

Also add failing test cases for service refs in artifact transform
parameters and other service parameters.

Co-authored-by: Rodrigo Oliveira <rodrigo@gradle.com>

Issue: #16168 | Avoid unwrapping a property value eagerly

If a service reference visitor needs the provider, it can unwrap the
provider reference itself.

Make `ConsumedBuildServiceProvider` thread-safe

Issue: #16168

Co-authored-by: Rodrigo Oliveira <rodrigo@gradle.com> | Include build id when comparing service providers

Issue: #16168"
gradle/gradle,19015,https://github.com/gradle/gradle/pull/19015,Allow applying plugins with matching versions,"This resolves a long-standing issue with applying a plugin in a
subproject when that same plugin is already applied in the parent project.
Prior to this commit, you would need to avoid specifying a version,
which only became a true hassle with the introduction of plugins in the
version catalog, as shown in #18236.

This commit does not attempt to resolve conflicting versions, or inspect
the classpath for a plugin version. It only records the data that Gradle
knows for sure, and uses that to slightly relax this check. It is
sufficient to allow the issue mentioned prior to be resolved.

Replaces #18728
Fixes #18236

If accepted, I'll do the following:
- [x] Update documention
- [x] Document the feature in release notes",2021-12-15T22:44:07Z,bot-gradle,octylFractal,clarity,"Awesome that you found a way to collect applied plugin versions! I just left a comment regarding having a direct dependency on plugins in ClassLoaderScope.
One question: Could we add also functionality to define ""withoutVersion"" plugins in the Version catalog?
I think it might come in handy in some cases, e.g. so you can generate type-safe accessors for precompiled scripts, or when the plugin is added to the classpath through methods where we cannot collect versions. | Currently failing due to classcycle, see internal Slack. Resolved. | @ljacomet @big-guy this is almost ready to go, but do you know if we have anywhere in the documentation that references the restriction on versions that this PR weakens? I didn't see any, so I didn't change the documentation, but maybe it's in an unusual place. | @asodja re:

One question: Could we add also functionality to define ""withoutVersion"" plugins in the Version catalog? I think it might come in handy in some cases, e.g. so you can generate type-safe accessors for precompiled scripts, or when the plugin is added to the classpath through methods where we cannot collect versions.

I did not tackle this in this PR, we have decided to save it for later. I filed #19302 to track this. | OK, I've already triggered a build for you. | Pre-tested commit build failed. | @bot-gradle test and merge | OK, I've already triggered a build for you.","Allow applying plugins with matching versions

This resolves a long-standing issue with applying a plugin in a
subproject when that same plugin is already applied in the root project.
Prior to this commit, you would need to avoid specifying a version,
which only became a true hassle with the introduction of plugins in the
version catalog, as shown in #18236.

This commit does not attempt to resolve conflicting versions, or inspect
the classpath for a plugin version. It only records the data that Gradle
knows for sure, and uses that to slightly relax this check. It is
sufficient to allow the issue mentioned prior to be resolved. | Add test for settings block configuration | Merge branch 'master' into ot/feature/plugin-block-conflict-resolution | Move plugin-specific knowledge into data object | Remove unused import | Move tracker to satisfy class cycle check | Merge branch 'master' into ot/feature/plugin-block-conflict-resolution | Document plugin block improvements | Link to catalog plugin docs | Externalize association between ClassLoaderScopes and plugin versions

So it doesn't introduce overhead when loading from the configuration cache. | Remove outdated class loader changes | Clean up diff | Fix test kit plugin test"
gradle/gradle,31048,https://github.com/gradle/gradle/pull/31048,Let value sources be shared across projects,"Fixes #31039 

### Reviewing cheatsheet

Before merging the PR, comments starting with 
- ❌ ❓**must** be fixed
- 🤔 💅 **should** be fixed
- 💭 **may** be fixed
- 🎉 celebrate happy things
",2024-10-29T20:42:55Z,bamboo,bamboo,easier to read,"@bot-gradle test ACC | I've triggered the following builds:

All ConfigCache Tests for Pull Request Feedback (Trigger) (Pull Request Feedback)

Build Scan


ConfigCache Java21 Adoptium Macos Aarch64 (Trigger) (Ready for Release)

Build Scan


ConfigCache Java21 Adoptium Windows Amd64 (Trigger) (Ready for Release)

Build Scan | The following builds have failed:

All ConfigCache Tests for Pull Request Feedback (Trigger) (Pull Request Feedback)

Build Scan | The following builds have failed:

ConfigCache Java21 Adoptium Macos Aarch64 (Trigger) (Ready for Release)

Build Scan | The following builds have failed:

ConfigCache Java21 Adoptium Windows Amd64 (Trigger) (Ready for Release)

Build Scan | Thanks for the initial review, @mlopatkin. Please take another look. | The merge queue build has failed. Click here to see all failures. | The merge queue build has failed. Click here to see all failures. | the build failure seems legit ;( | the build failure seems legit ;(

Indeed. The easier fix was to lookup the projects directly from the BuildState service instead of relying on the ProjectProvider to be set.
This works because build services and value sources are only ever read after the projects have already been registered. | This works because build services and value sources are only ever read after the projects have already been registered.

Looks like this doesn't hold. For example, build scan callbacks are deserialized before projects, so if a build scan callback holds a value source/build service reference, it can kick off shared object reader. Cached environment state may also hold arbitrary stuff.","Let value sources be shared across projects

By encoding them as shared objects.

This change restores the 8.10 behavior for value source instances encoded via
`encodePreservingSharedIdentityOf`.

Fixes #31039 | Format `ConfigurationCacheValueSourceIntegrationTest` | Polish `DefaultGlobalValueCodec`

- Format file
- Replace `try {} finally { resource.close() }` by `resource.use {}` | Polish `ConfigurationCacheValueSourceIntegrationTest`

- Replace random integers by new String instances

Using a String instance is better than using a custom class instance because
build service parameters are isolated and the `Isolatable` implementation for
String simply returns the original value whereas the Isolatable for a custom
class instance would serialize it then deserialize it for isolation (thus
invalidating the identity check). | Replace CC singleton property by direct service lookup | Fix `projectHealth` after `ValueSource` encoding fix | Test for build service with transformer-dependent parameter

Issue: #31039"
gradle/gradle,28843,https://github.com/gradle/gradle/pull/28843,Add more extensive Build Scan project isolation smoke test,The new test also tests the test acceleration features.,2024-04-19T11:42:34Z,wolfs,wolfs,easier to read,"🥷 Code experts: ljacomet
ljacomet has most 👩‍💻 activity in the files.
alllex, donat have most 🧠 knowledge in the files.
 
 See details
testing/internal-integ-testing/src/main/groovy/org/gradle/integtests/fixtures/RepoScriptBlockUtil.groovy
Activity based on git-commit:




ljacomet




APR



MAR
234 additions & 0 deletions


FEB



JAN



DEC



NOV




Knowledge based on git-blame:
donat: 24%
testing/smoke-test/src/smokeTest/groovy/org/gradle/smoketests/BuildScanPluginSmokeTest.groovy
Activity based on git-commit:




ljacomet




APR



MAR
399 additions & 0 deletions


FEB



JAN



DEC



NOV




Knowledge based on git-blame:
alllex: 32%

To learn more about /:\ gitStream - Visit our Docs | The merge queue build has failed. Click here to see all failures.","Add more extensive Build Scan project isolation smoke test

The new test also tests the test acceleration features. | Fix RepoScriptBlockUtil for isolated projects | Improve readability

Co-authored-by: Rodrigo Bamboo <rodrigo@gradle.com>
Signed-off-by: Stefan Wolf <wolf@gradle.com> | Fix RepoScriptBlockUtil for older Gradle versions"
gradle/gradle,27532,https://github.com/gradle/gradle/pull/27532,Combine ResolveState and ResovlerResults,"Both of these types were used to represent the resolution state of a Configuration This commit combines them into a single type to avoid duplication

### Reviewing cheatsheet

Before merging the PR, comments starting with 
- ❌ ❓**must** be fixed
- 🤔 💅 **should** be fixed
- 💭 **may** be fixed
- 🎉 celebrate happy things
",2024-01-02T21:04:49Z,bot-gradle,jvandort,easier to read,"@bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

PullRequestFeedback build | @bot-gradle test and merge","Combine ResolveState and ResovlerResults

Both of these types were used to represent the resolution state of a Configuration
This commit combines them into a single type to avoid duplication | Add isFullyResolved to avoid verbose if statements"
gradle/gradle,21434,https://github.com/gradle/gradle/pull/21434,"Make it an error (not a deprecation warning) to reference tasks from an included build using finalizedBy, mustRunAfter, or shouldRunAfter","<!--- The issue this PR addresses -->
Fixes #15875 ",2022-08-11T22:24:44Z,bot-gradle,tresat,easier to read,"@bot-gradle test this | OK, I've already triggered the following builds for you:

PullRequestFeedback build | @bot-gradle test and merge | OK, I've already triggered a build for you.","Stop ProjectBuilder instances in more integration tests | Close Tooling API resolver in CrossVersionTestEngine | Defer generating the diagnostic information for an execution plan until a deadlock is detected.

This allows the generation to be somewhat expensive, for example can do some additional analysis. | Include the entry nodes in the diagnostic information logged when an execution deadlock is detected. | Add some more detail to the diagnostics for a node. | Fix assignment of ordinal group for a task when that task appears in multiple entry points. | Merge pull request #21406 Fix the assignment of task to ordinal group

<!--- The issue this PR addresses -->
Fixes #?

### Context

Fix the assignment of a task to an ordinal group, when that task is a requested task and also reachable from an earlier requested task.

Also add some more node and execution plan details to the diagnostics logged when an execution deadlock is detected.

### Contributor Checklist
- [ ] [Review Contribution Guidelines](https://github.com/gradle/gradle/blob/master/CONTRIBUTING.md)
- [ ] Make sure that all commits are [signed off](https://git-scm.com/docs/git-commit#Documentation/git-commit.txt---signoff) to indicate that you agree to the terms of [Developer Certificate of Origin](https://developercertificate.org/).
- [ ] Make sure all contributed code can be distributed under the terms of the [Apache License 2.0](https://github.com/gradle/gradle/blob/master/LICENSE), e.g. the code was written by yourself or the original code is licensed under [a license compatible to Apache License 2.0](https://apache.org/legal/resolved.html).
- [ ] Check [""Allow edit from maintainers"" option](https://help.github.com/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork/) in pull request so that additional changes can be pushed by Gradle team
- [ ] Provide integration tests (under `<subproject>/src/integTest`) to verify changes from a user perspective
- [ ] Provide unit tests (under `<subproject>/src/test`) to verify logic
- [ ] Update User Guide, DSL Reference, and Javadoc for public-facing changes
- [ ] Ensure that tests pass sanity check: `./gradlew sanityCheck`
- [ ] Ensure that tests pass locally: `./gradlew <changed-subproject>:quickTest`

### Gradle Core Team Checklist
- [ ] Verify design and implementation
- [ ] Verify test coverage and CI build status
- [ ] Verify documentation
- [ ] Recognize contributor in release notes

Co-authored-by: Adam Murdoch <adam@gradle.com> | Update test bucket json | Merge branch 'release' | Publish 7.5.1-20220730234330+0000 | Merge branch 'master' into alllex/stop-project-builders-in-integ-tests | testing tags | Make `OrdinalGroupFactory` accept group requests in any order

To restore compatibility with `OrdinalNodeCodec`. | adds verbs to titles and reorganize titles | outline change with TODO and verbs | Apply suggestions from code review

Co-authored-by: Nathan Contino <ncontino@protonmail.com> | Apply suggestions from code review

Co-authored-by: Nathan Contino <ncontino@protonmail.com> | Merge branch 'master' into alllex/stop-project-builders-in-integ-tests | Implement Zinc bug workaround

Workaround as implemented in Sbt and maven-scala-plugin

Fixes #21400 | Upgradle to latest nightly

To have all the new Build Scan events. | Apply suggestions from code review

Co-authored-by: Nathan Contino <ncontino@protonmail.com>
Co-authored-by: Sterling Greene <big-guy@users.noreply.github.com> | Make it an error (not a deprecation warning) to reference tasks from an included build using finalizedBy, mustRunAfter, or shouldRunAfter | Merge pull request #21432 Upgradle to latest nightly

To have all the new Build Scan events.

Co-authored-by: Stefan Wolf <wolf@gradle.com> | Move integration tests to the location of existing tests for deprecation warning, update tests for new behavior | Fix test assertion

The two finalizers can run in parallel. | Merge pull request #21408 Merge release into master

There were quite a few conflicts:
```
CONFLICT (content): Merge conflict in subprojects/composite-builds/src/test/groovy/org/gradle/composite/internal/DefaultIncludedBuildTaskGraphParallelTest.groovy
CONFLICT (content): Merge conflict in subprojects/core/src/main/java/org/gradle/execution/plan/DefaultExecutionPlan.java
CONFLICT (content): Merge conflict in subprojects/core/src/main/java/org/gradle/execution/plan/OrdinalGroup.java
CONFLICT (content): Merge conflict in subprojects/core/src/main/java/org/gradle/execution/plan/OrdinalNodeAccess.java
CONFLICT (content): Merge conflict in subprojects/core/src/main/java/org/gradle/execution/plan/TaskInAnotherBuild.java
```

Mostly between `OrdinalNodeFactory` and the changes in `OrdinalNodeAccess`.

Co-authored-by: Stefan Wolf <wolf@gradle.com> | Fix test assertion

The two finalizers can run in parallel. | Merge pull request #21438 Fix flaky FinalizerTaskIntegrationTest

Cherry-picked from `master` (https://github.com/gradle/gradle/pull/21408).

The two finalizers can run in parallel.

Co-authored-by: Stefan Wolf <wolf@gradle.com> | Rebaseline performance test

Referenced snapshot is gone. | Merge pull request #21407 Update test bucket json

Update test bucket json

Co-authored-by: bot-teamcity <bot-teamcity@gradle.com> | Merge pull request #21431 Implement Zinc bug workaround

Workaround as implemented in Sbt and maven-scala-plugin

Fixes #21400

Unfortunately, I am not able to reproduce the issue with Gradle so far. Even when using [the reproducer for `maven-scala-plugin`](https://github.com/LuciferYang/scala-maven-plugin-test), the build does not fail with Gradle 7.5.
However the fix is based on the ones made for sbt and the Maven plugin and has been OKed by the Zinc folks, so we should indeed add this protection.

Co-authored-by: Louis Jacomet <louis@gradle.com> | Limit ToolingApiDistributionResolver usage to a one-off action | Merge branch 'master' into alllex/stop-project-builders-in-integ-tests | Merge branch 'master' into release-notes-7.6-outline | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Update subprojects/docs/src/docs/release/notes.md | Merge pull request #21423 Outlined and Rephrased 7.6 Release Notes (in a standardized docs-y way)

<!--- The issue this PR addresses -->
- Reworded section titles to make it easier for users to understand what changed.
- Reworded (minimally) section descriptions in a more docs-y voice.
- Added CTA (Call To Action) links at the bottom of sections, when possible.
- Added ""TODO"" sections for features that have not yet landed (but should).
- Sorted changes into sections that reflect engineering's structure (developer productivity, configuration, execution, plugin, JVM, IDE, etc.)
- This diff is horrible to look at. For a better view, check out the [rendered distrubtion release notes](https://builds.gradle.org/repository/download/Gradle_Master_Check_BuildDistributions/54651612:id/distributions/gradle-7.6-docs.zip!/gradle-7.6-20220804163622%2B0000/docs/release-notes.html)

### Context
Education is taking over ownership of the release notes. This is us getting the release notes into a state we feel comfortable iterating from.

### Contributor Checklist
- [x] [Review Contribution Guidelines](https://github.com/gradle/gradle/blob/master/CONTRIBUTING.md)
- [x] Make sure that all commits are [signed off](https://git-scm.com/docs/git-commit#Documentation/git-commit.txt---signoff) to indicate that you agree to the terms of [Developer Certificate of Origin](https://developercertificate.org/).
- [x] Make sure all contributed code can be distributed under the terms of the [Apache License 2.0](https://github.com/gradle/gradle/blob/master/LICENSE), e.g. the code was written by yourself or the original code is licensed under [a license compatible to Apache License 2.0](https://apache.org/legal/resolved.html).
- [x] Check [""Allow edit from maintainers"" option](https://help.github.com/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork/) in pull request so that additional changes can be pushed by Gradle team
- [x] Provide integration tests (under `<subproject>/src/integTest`) to verify changes from a user perspective
- [x] Provide unit tests (under `<subproject>/src/test`) to verify logic
- [x] Update User Guide, DSL Reference, and Javadoc for public-facing changes
- [x] Ensure that tests pass sanity check: `./gradlew sanityCheck`
- [x] Ensure that tests pass locally: `./gradlew <changed-subproject>:quickTest`

### Gradle Core Team Checklist
- [ ] Verify design and implementation
- [ ] Verify test coverage and CI build status
- [ ] Verify documentation
- [ ] Recognize contributor in release notes

Co-authored-by: Nathan Contino <ncontino@protonmail.com> | Remove getSourcePath from javadoc options | Add test that excludes packages from Javadoc | Demonstrate workaround for #19726 | accept archunit test changes | Fix using default repository when resolving dependencies in cross-version tests | Merge pull request #21389 Stop ProjectBuilder instances in more integration tests

Continuation of #21387

Co-authored-by: Alex Semin <asemin@gradle.com> | Upgrade tested AGP versions

from 7.2.1 to 7.2.2
from 7.4.0-alpha08 to 7.4.0-alpha09

Signed-off-by: Paul Merlin <paul@gradle.com> | Remove false claim from plugins user manual chapter

Signed-off-by: Paul Merlin <paul@gradle.com> | Merge pull request #21447 Upgrade tested AGP versions

from 7.2.1 to 7.2.2
from 7.4.0-alpha08 to 7.4.0-alpha09

Co-authored-by: Paul Merlin <paul@gradle.com> | Merge remote-tracking branch 'origin/release' into bhegyi/merge-release-into-master

# Conflicts:
#	subprojects/performance/src/performanceTest/groovy/org/gradle/performance/regression/android/RealLifeAndroidBuildPerformanceTest.groovy
#	subprojects/performance/src/performanceTest/groovy/org/gradle/performance/regression/java/JavaFirstUsePerformanceTest.groovy | Refresh released version list | Merge pull request #21449 Merge release into master

Co-authored-by: Balint Hegyi <bhegyi@gradle.com> | Rework test to demonstrate usage of module-source-path

Issue #21399 | Publish 7.5.1-20220805123425+0000 | Merge pull request #21451 Publish 7.5.1-20220805123425+0000

Publish 7.5.1-20220805123425+0000

Co-authored-by: bot-teamcity <bot-teamcity@gradle.com> | Finalize release notes for patch release | Merge pull request #21446 Revert introduction of sourcePath for Javadoc options

Co-authored-by: Louis Jacomet <louis@gradle.com> | Merge pull request #21453 Finalize release notes for patch release

Co-authored-by: Louis Jacomet <louis@gradle.com> | Merge pull request #21448 Remove false claim from plugins user manual chapter

This is simply not true in practice and confuses readers.

Co-authored-by: Paul Merlin <paul@gradle.com> | Merge remote-tracking branch 'origin/release'

* origin/release:
  Finalize release notes for patch release
  Rework test to demonstrate usage of module-source-path
  accept archunit test changes
  Demonstrate workaround for #19726
  Add test that excludes packages from Javadoc
  Remove getSourcePath from javadoc options | Refreeze arch unit expectations | Merge pull request #21457 Merge release into master

Co-authored-by: Sterling Greene <sterling@gradle.com> | Publish 7.5.1-20220805205215+0000 | Update to 7.5.1 | Merge remote-tracking branch 'origin/release'

* origin/release:
  Update to 7.5.1 | Merge pull request #21459 Publish 7.5.1-20220805205215+0000

Publish 7.5.1-20220805205215+0000

Co-authored-by: bot-teamcity <bot-teamcity@gradle.com> | Not show build scan link for `updateReleasedVersions` task

Because it's part of promotion build. | Merge pull request #21468 Not show build scan link for `updateReleasedVersions` task

Because it's part of promotion build.

Co-authored-by: Bo Zhang <bo@gradle.com> | Update wrapper to latest nightly | Merge pull request #21470 Update wrapper to latest nightly

Co-authored-by: Donat Csikos <donat@gradle.com> | Merge pull request #21347 Make aborting the promotion build due to release not ready cleaner in TC

Add a new first task to promotions which checks if we're ready to promote by calling the checkReadyToPromote task in a separate gradle build.

This PR should merge with [this PR in the promotion project](https://github.com/gradle/gradle-promote/pull/94).

[Example build in the experimental pipeline](https://builds.gradle.org/buildConfiguration/Gradle_Experimental_Promotion_SnapshotFromQuickFeedback/54241426?buildTab=log&focusLine=281&logView=linear&linesState=241.337.361.419).

Co-authored-by: Thomas Tresansky <ttresansky@gradle.com> | Clean accepted API changes | Update version to 7.7 | Clean release notes and welcome message for 7.7 | Update library versions in build init to latest for 7.7 | Update released version to latest snapshot | Revert changes to TestNG default version | Merge pull request #21473 Prepare master for next version

Prepare master for next version

Co-authored-by: Sterling Greene <big-guy@users.noreply.github.com> | Update the version of the Grgit plugin used in a test

- Old plugin unsupported for quite a while, this is the currently maintained replacement. | Downgrade to version which doesn't require Java 11 | Add placeholders for 8.0 upgrade guide | Merge pull request #21478 Add placeholders for 8.0 upgrade guide

Co-authored-by: Sterling Greene <sterling@gradle.com> | Correct vcs root id for release/experimental branches

Previously, Kotlin DSL on all branches uses `GradleMaster` vcs root id,
which breaks builds on release branch. | Merge pull request #21477 Update the version of the Grgit plugin used in a test

- Old plugin unsupported for quite a while, this is the currently maintained replacement.

Co-authored-by: Thomas Tresansky <ttresansky@gradle.com> | Clarify docs wording for type-safe deps API | Merge pull request #21482 Clarify docs wording for type-safe deps API

<!--- The issue this PR addresses -->
Small wording fix, thank you to @hythloda for reporting this issue to me.

### Context
The old version of this sentence isn't grammatically correct English. It's difficult to understand what it's trying to say.
Also reworded a section intro from passive -> active voice for consistency with other sections.

### Contributor Checklist
- [X] [Review Contribution Guidelines](https://github.com/gradle/gradle/blob/master/CONTRIBUTING.md)
- [X] Make sure that all commits are [signed off](https://git-scm.com/docs/git-commit#Documentation/git-commit.txt---signoff) to indicate that you agree to the terms of [Developer Certificate of Origin](https://developercertificate.org/).
- [X] Make sure all contributed code can be distributed under the terms of the [Apache License 2.0](https://github.com/gradle/gradle/blob/master/LICENSE), e.g. the code was written by yourself or the original code is licensed under [a license compatible to Apache License 2.0](https://apache.org/legal/resolved.html).
- [X] Check [""Allow edit from maintainers"" option](https://help.github.com/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork/) in pull request so that additional changes can be pushed by Gradle team
- [X] Provide integration tests (under `<subproject>/src/integTest`) to verify changes from a user perspective
- [X] Provide unit tests (under `<subproject>/src/test`) to verify logic
- [X] Update User Guide, DSL Reference, and Javadoc for public-facing changes
- [X] Ensure that tests pass sanity check: `./gradlew sanityCheck`
- [X] Ensure that tests pass locally: `./gradlew <changed-subproject>:quickTest`

### Gradle Core Team Checklist
- [ ] Verify design and implementation
- [ ] Verify test coverage and CI build status
- [ ] Verify documentation
- [ ] Recognize contributor in release notes

Co-authored-by: Nathan Contino <ncontino@protonmail.com> | Merge branch 'master' into tt/80/must-should-finalized-by-other-project-is-error | Add note to upgrade guide about included build dependencies becoming an error from a warning"
gradle/gradle,25858,https://github.com/gradle/gradle/pull/25858,Fix incoming dir path for GE plugin performance testing,Fixes https://github.com/gradle/gradle-private/issues/3912,2023-07-26T07:04:36Z,bot-gradle,alllex,easier to read,"🥷 Code experts: no user but you matched threshold 10
alllex has most 👩‍💻 activity in the files.
ldaley has most 🧠 knowledge in the files.
 
 See details
platforms/enterprise/enterprise-plugin-performance/build.gradle.kts
Activity based on git-commit:




alllex




JUL
69 additions & 0 deletions


JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:
ldaley: 33%
platforms/enterprise/enterprise-plugin-performance/src/testFixtures/groovy/org/gradle/performance/AbstractBuildScanPluginPerformanceTest.groovy
Activity based on git-commit:




alllex




JUL
105 additions & 0 deletions


JUN



MAY



APR



MAR



FEB




Knowledge based on git-blame:

To learn more about /:\ gitStream - Visit our Docs | This PR appears to be lacking tests.  Consider adding tests to cover the new functionality. | This PR is 86.84% new code.
There are 33 added lines and 5 deleted lines. | I've triggered a build for you. Click here to see all failures if there's any. | Pre-tested commit build failed. Click here to see all build failures. | @bot-gradle test and merge | I've triggered a build for you. Click here to see all failures if there's any.",Fix incoming dir path for GE plugin performance testing | Expect system property with path to GE plugin info directory | Use project property for configuring GE plugin info directory | Use explicit class instead of anonymous
gradle/gradle,24831,https://github.com/gradle/gradle/pull/24831,New introduction for the Gradle BT manual,"This is a Documentation change ONLY.

### Context
This is an improvement upon the Introduction section of the Gradle User Manual.
- Updated ""What is Gradle?"" section
- Renamed main section to ""Overview""
- Removed fundamentals section in What_is_Gradle.adoc
- Added ""About the Manual"" section
- Cleaned up syntax and grammatical errors

Additionally, @lkasso, the Gradle Technical writer, has been added as a CODEOWNER for docs in this PR.

### Snapshot
BEFORE:
<img width=""1590"" alt=""Screenshot 2023-05-15 at 4 21 25 PM"" src=""https://github.com/gradle/gradle/assets/5684849/55c09fb0-9a9a-4e72-b370-dc2dbd4c3d8c"">

AFTER:
<img width=""1590"" alt=""Screenshot 2023-05-15 at 4 20 10 PM"" src=""https://github.com/gradle/gradle/assets/5684849/52a72386-e2cb-4e30-9890-aa0ba728386a"">



",2023-05-16T22:33:53Z,bot-gradle,lkasso,easier to read,"@akshayabd @lkasso FYI, since we've branched for 8.2 today, you'll need to retarget the 'release' branch for these changes to land in 8.2. | Changing base for 8.3 release to give more time for review during WM2023 with @pioterj | I've triggered a build for you. | Pre-tested commit build failed. | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | Your PR is queued. See the queue page for details. | I've triggered a build for you.",update 1 to Gradle intro in doc | update introduction in the user manual | add lkasso as codeowner for docs | additional changes based on feedback to intro | remove Gradle Enterprise link from BT userguide menu | further changes to introduction including new fundamental graphic | changes based on comments from eskatos and update to single lines | peer changes | update CODEOWNERS file and Gradle intro | remove unused pic | updates based on community feedback | update Gradle name and move menu change to another PR | update based on ljacomet feedback | Merge branch 'release' into lkasso/documentation/new-intro | Merge branch 'master' into lkasso/documentation/new-intro | changes based on WM discussion | remove unused pic | Merge branch 'master' into lkasso/documentation/new-intro
gradle/gradle,23592,https://github.com/gradle/gradle/pull/23592,Scheduled artifact transform build operations for execution and their nodes in the execution plan,"Summary of the changes:
- Task graph build operation result is extended with a more generic execution plan of `PlannedNode`s that for now can be either a task or a transform.
- Scheduled transform execution build operation is extended with additional info that eventually is shown to the users.

Historically, the task graph computed from the calculated execution plan contained only `PlannedTask` nodes. However, the execution plan also contains other nodes that may be of interest, such as nodes related to artifact transforms. More specifically, `TransformationNode` is a node in the execution plan that corresponds to a scheduled transform. Such a node can take considerable build time, previously it will not show up in the execution plan exposed in the build operations.

In order to make artifact transforms better analyzable, their detail information is now included into the relevant build operations.

Similarly to task nodes, transformation nodes cannot be identified only by their properties. As such, transformation nodes now also contain a unique opaque identifier that serves as a tiebreaker in terms of the transformation identity.

Unlike the tasks, transformations require much more properties to get identified reasonably:
- (consumer) `buildPath`
- <REST TBD>",2023-02-17T17:40:49Z,bot-gradle,alllex,easier to read,"We've settled on the Build Operations API. The rest of the implementation is Gradle-internal, so we'll do follow-up PRs to make quality of life improvements and add more test coverage then. | @bot-gradle test and merge | I've triggered a build for you.","Provide planned transformation nodes in build operation for execution graph | Add build path to TransformationIdentity | Add TransformationIdentity to TransformationNode execution build operation | Serialize transformation identity for execution build operation | Refactor planned node conversion | Provide attributes for TransformIdentity as string maps | Move node identity computation into ToPlannedNodeConverter | Add ComponentIdentifier model | Move up identity interfaces | Make component model serializable into build trace | Revert ""Make component model serializable into build trace""

This reverts commit dbf27bef62e24d1d8a888926e7f0671e575d74e9. | Add dependencies configuration identity to transformation identity | Add stubs for configuration identity in CC | Introduce ComponentVariantIdentifier | Pass configuration identity as an extra parameter | Handle missing project path for configuration identity | Provide execution plan for specific node types | Fix task dependencies resolution in execution plan | Polish converters and docs | Add a single-transform integration test for transform-related build operations | Move PlannedTransform to dependency-management | Add Javadoc to new types | Move ExecuteScheduledTransformationStepBOT to enterprise-operations | Rename {Unknown->Opaque}ComponentIdentifier | Rename Converter{Factory->Registry} | Move RESULT to build operation implementation | Ensure task paths are lexicographically sorted | Store configuration identity in configuration cache | Remove CalculateTaskGraphBuildOperationType.Details.getExecutionPlan()

Only the method providing the types should be called. | Add a custom serializer for CalculateTaskGraphResult

so we can assert the `getExecutionGraph` ""property"" in tests. | Support grouped transform output assertions | Add transform build-ops tests | Fix GroupedOutputFixtureTest | Fix ArtifactTransformBuildOperationIntegrationTest | Correctly fix ArtifactTransformBuildOperationIntegrationTest | Use nested `with` from Spock to shorten assertions | Fix RichConsoleBasicGroupedTaskLoggingFunctionalTest | Merge remote-tracking branch 'origin/alllex/scans/link-transform-to-task' into alllex/scans/link-transform-to-task | Merge branch 'master' into alllex/scans/link-transform-to-task

# Conflicts:
#	subprojects/internal-integ-testing/src/main/groovy/org/gradle/integtests/fixtures/logging/GroupedOutputFixture.java
#	subprojects/internal-integ-testing/src/main/groovy/org/gradle/integtests/fixtures/logging/GroupedTransformationOutputFixture.java
#	subprojects/internal-integ-testing/src/main/groovy/org/gradle/integtests/fixtures/logging/GroupedWorkOutputFixture.java
#	subprojects/internal-integ-testing/src/test/groovy/org/gradle/integtests/fixtures/logging/GroupedOutputFixtureTest.groovy | Add test for transforming multiple artifacts from a transform up the chain | Flatten target component variant in the transformation identity | Polish ArtifactTransformBuildOperationIntegrationTest | Remove extra newline"
gradle/gradle,22778,https://github.com/gradle/gradle/pull/22778,Add Upgrade Note for Strongly Typed Dependencies in Test Suites,"<!--- The issue this PR addresses -->
Fixes #22720

",2022-11-17T23:08:03Z,bot-gradle,tresat,easier to read,"@bot-gradle test bd | OK, I've already triggered the following builds for you:

BuildDistributions build | I tried to combine and apply Nate and Sterling's suggestions, thank you both. | @bot-gradle test and merge | OK, I've already triggered a build for you.","Add upgrade note for project -> project() change in test suite dependencies block | Slight formatting changes | Update subprojects/docs/src/docs/userguide/migration/upgrading_version_7.adoc

Co-authored-by: nate contino <ncontino@gradle.com> | Update subprojects/docs/src/docs/userguide/migration/upgrading_version_7.adoc

Co-authored-by: nate contino <ncontino@gradle.com> | Various rewording changes as suggested by reviewers | Update subprojects/docs/src/docs/userguide/migration/upgrading_version_7.adoc

Co-authored-by: nate contino <ncontino@gradle.com> | Update subprojects/docs/src/docs/userguide/migration/upgrading_version_7.adoc

Co-authored-by: nate contino <ncontino@gradle.com> | Update subprojects/docs/src/docs/userguide/migration/upgrading_version_7.adoc

Co-authored-by: nate contino <ncontino@gradle.com> | Correct typo, phrasing"
gradle/gradle,18843,https://github.com/gradle/gradle/pull/18843,Allow the build scan plugin to read environment variables and reenable test,"Some versions of the build scan plugin read environment variables during the configuration phase but do not take decisions upon them, so recording these variables as inputs to the configuration cache is a false positive.
Similar to system properties, we skip these accesses. The escape hatch to disable the workarounds is provided to make fixing the issues in the scan plugin easier in the future. Workarounds for environment variables and system properties can be turned off separately.",2021-11-03T21:35:08Z,bot-gradle,mlopatkin,easier to read,"OK, I've already triggered a build for you. | Pre-tested commit build failed. | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | OK, I've already triggered a build for you.","Allow the build scan plugin to read environment variables

Some versions of the build scan plugin read environment variables during
configuration phase but do not take decisions upon them, so recording
these variables as inputs to the configuration cache is false positive.
Similar to system properties, we skip these access. The escape hatch to
disable the workarounds is provided to make fixing the issues in the
scan plugin easier in the future. Workarounds for environment variables
and system properties can be turned off separately. | Revert ""Ignore failing tests on macOS""

This reverts commit 2d39d2941bcbe8ab2a4ab42c45c05b6f4c825498.

The test is now fixed by ignoring env reads in the build scan plugin."
gradle/gradle,19111,https://github.com/gradle/gradle/pull/19111,Refine configuration-cache report after the addition of configuration inputs,"Fix the report layout moving inputs into their own tab, remove now irrelevant filter buttons, rework the title/summary of the report to account for inputs, display the requested tasks for the report so they are easier to identify, plus various enhancements.

![image](https://user-images.githubusercontent.com/132773/143285836-a9ff4352-8376-477e-b92b-f836f38fea4a.png)
",2021-11-25T16:15:33Z,bot-gradle,eskatos,easier to read,"@bot-gradle test ACC | OK, I've already triggered AllConfigCacheTestsReadyForMerge build for you. | The report looks great! 👍 | @bot-gradle test and merge | Your PR is queued. See the queue page for details. | OK, I've already triggered a build for you.","Move inputs into first tab

Signed-off-by: Paul Merlin <paul@gradle.com> | Fix singular message

Signed-off-by: Paul Merlin <paul@gradle.com> | On report display, if no problem show inputs else problems by message

Signed-off-by: Paul Merlin <paul@gradle.com> | Disable empty tabs and simplify inputs view

Signed-off-by: Paul Merlin <paul@gradle.com> | Remove display filters as they are of no use now

This simplifies the page, code and css

Signed-off-by: Paul Merlin <paul@gradle.com> | Rework report title/summary

to display info about both inputs and problems

Signed-off-by: Paul Merlin <paul@gradle.com> | Display requested tasks in the report title

Signed-off-by: Paul Merlin <paul@gradle.com> | Prefer when {} over if/else if/else

Signed-off-by: Paul Merlin <paul@gradle.com> | Display 'No problem was found' instead of '0 problems were found'

and 'No build logic input was found'
instead of '0 build logic input were found'

Signed-off-by: Paul Merlin <paul@gradle.com> | s/build logic input/build configuration input/g

to match the CC documentation

Signed-off-by: Paul Merlin <paul@gradle.com> | Fix report generation when invoking default tasks

Signed-off-by: Paul Merlin <paul@gradle.com>"
gradle/gradle,19311,https://github.com/gradle/gradle/pull/19311,Log4j follow up,,2021-12-14T21:57:56Z,bot-gradle,ljacomet,easier to read,"OK, I've already triggered a build for you. | Pre-tested commit build failed. | @bot-gradle test and merge | OK, I've already triggered a build for you. | Could these log4j changes be backported to the Gradle 6.9 branch and added to the 6.9.2 milestone? | That is actually the plan","Rework buildscript classpath log4j fix

This now uses a combination of require and reject instead of a strictly,
which will allow updates beyond the 2.x line. The previous solution was
effectively preventing that with no way for the user to change that.

Issue #19300 | Rework Zinc log4j fix

This now uses a combination of require and reject instead of a strictly,
which will allow updates beyond the 2.x line. The previous solution was
effectively preventing that with no way for the user to change that.

Issue #19300 | Mark Log4j 2.15 as vulnerable

Follows publication of
https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-45046

Issue #19300 | Fix tests following changes to constraint

Issue #19300"
gradle/gradle,18931,https://github.com/gradle/gradle/pull/18931,Tag CodeQL Builds with `CODEQL` Build Scan Tag,"### Context

This will make it easier to identify CodeQL builds in build scans",2021-11-12T16:21:42Z,bot-gradle,JLLeitschuh,easier to read,"@bot-gradle test and merge | OK, I've already triggered a build for you.",Tag CodeQL Builds with `CODEQL` Build Scan Tag | Don't check all environment vars for isCodeQl | Fix compilation error in BuildEnvironment | Fix formatting issue in BuildEnvironment
gradle/gradle,16537,https://github.com/gradle/gradle/pull/16537,Fix potential `NoClassDefFoundError`,"This commit workarounds a classloading issue reported by a user
by reusing the supplied `generatedMethodDetector`, which is
actually already verifying that the method is annotated with
Groovy's `@Generated` method.

## Context

Fixes (?) this exception reported by a user. I'm actually not sure _why_ this change would make this error go away but working on a fix I realized that the injected `generatedMethodDetector` is actually already testing for Groovy's `@Generated` annotation. However for some reason because it's injected it doesn't seem to trigger a bug. Or maybe it's because it's not called?

```
java.lang.NoClassDefFoundError: groovy/transform/Generated
	at org.gradle.internal.reflect.annotations.impl.DefaultTypeAnnotationMetadataStore.isIgnoredGeneratedGroovyMethod(DefaultTypeAnnotationMetadataStore.java:377)
	at org.gradle.internal.reflect.annotations.impl.DefaultTypeAnnotationMetadataStore.shouldIgnore(DefaultTypeAnnotationMetadataStore.java:361)
	at org.gradle.internal.reflect.annotations.impl.DefaultTypeAnnotationMetadataStore.processMethodAnnotations(DefaultTypeAnnotationMetadataStore.java:381)
	at org.gradle.internal.reflect.annotations.impl.DefaultTypeAnnotationMetadataStore.extractPropertiesFrom(DefaultTypeAnnotationMetadataStore.java:245)
	at org.gradle.internal.reflect.annotations.impl.DefaultTypeAnnotationMetadataStore.createTypeAnnotationMetadata(DefaultTypeAnnotationMetadataStore.java:215)
	at org.gradle.internal.reflect.annotations.impl.DefaultTypeAnnotationMetadataStore.lambda$getTypeAnnotationMetadata$1(DefaultTypeAnnotationMetadataStore.java:185)
	at org.gradle.cache.Cache.lambda$get$0(Cache.java:31)
	at org.gradle.cache.internal.DefaultCrossBuildInMemoryCacheFactory$AbstractCrossBuildInMemoryCache.get(DefaultCrossBuildInMemoryCacheFactory.java:134)
	at org.gradle.cache.Cache.get(Cache.java:31)
	at org.gradle.internal.reflect.annotations.impl.DefaultTypeAnnotationMetadataStore.getTypeAnnotationMetadata(DefaultTypeAnnotationMetadataStore.java:185)
	at org.gradle.internal.reflect.annotations.impl.DefaultTypeAnnotationMetadataStore.lambda$visitSuperTypes$22(DefaultTypeAnnotationMetadataStore.java:464)
	at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:658)
	at org.gradle.internal.reflect.annotations.impl.DefaultTypeAnnotationMetadataStore.visitSuperTypes(DefaultTypeAnnotationMetadataStore.java:464)
	at org.gradle.internal.reflect.annotations.impl.DefaultTypeAnnotationMetadataStore.inheritMethods(DefaultTypeAnnotationMetadataStore.java:228)
	at org.gradle.internal.reflect.annotations.impl.DefaultTypeAnnotationMetadataStore.createTypeAnnotationMetadata(DefaultTypeAnnotationMetadataStore.java:211)
	at org.gradle.internal.reflect.annotations.impl.DefaultTypeAnnotationMetadataStore.lambda$getTypeAnnotationMetadata$1(DefaultTypeAnnotationMetadataStore.java:185)
	at org.gradle.cache.Cache.lambda$get$0(Cache.java:31)
	at org.gradle.cache.internal.DefaultCrossBuildInMemoryCacheFactory$AbstractCrossBuildInMemoryCache.get(DefaultCrossBuildInMemoryCacheFactory.java:134)
	at org.gradle.cache.Cache.get(Cache.java:31)
	at org.gradle.internal.reflect.annotations.impl.DefaultTypeAnnotationMetadataStore.getTypeAnnotationMetadata(DefaultTypeAnnotationMetadataStore.java:185)
	at org.gradle.api.internal.tasks.properties.DefaultTypeMetadataStore.createTypeMetadata(DefaultTypeMetadataStore.java:95)
	at org.gradle.cache.internal.DefaultCrossBuildInMemoryCacheFactory$AbstractCrossBuildInMemoryCache.get(DefaultCrossBuildInMemoryCacheFactory.java:134)
	at org.gradle.api.internal.tasks.properties.DefaultTypeMetadataStore.getTypeMetadata(DefaultTypeMetadataStore.java:89)
	at org.gradle.api.internal.tasks.properties.bean.RuntimeBeanNodeFactory.createRoot(RuntimeBeanNodeFactory.java:33)
	at org.gradle.api.internal.tasks.properties.DefaultPropertyWalker.visitProperties(DefaultPropertyWalker.java:38)
	at org.gradle.api.internal.tasks.TaskPropertyUtils.visitProperties(TaskPropertyUtils.java:44)
	at org.gradle.api.internal.tasks.TaskPropertyUtils.visitProperties(TaskPropertyUtils.java:34)
	at org.gradle.api.internal.tasks.DefaultTaskOutputs.getFileProperties(DefaultTaskOutputs.java:143)
	at org.gradle.api.internal.tasks.DefaultTaskOutputs$TaskOutputUnionFileCollection.visitChildren(DefaultTaskOutputs.java:234)
	at org.gradle.api.internal.file.CompositeFileCollection.visitContents(CompositeFileCollection.java:119)
	at org.gradle.api.internal.file.AbstractFileCollection.getFiles(AbstractFileCollection.java:130)
	at org.gradle.api.internal.file.AbstractFileCollection.iterator(AbstractFileCollection.java:176)
	at com.android.build.gradle.internal.tasks.NonIncrementalTaskKt.cleanUpTaskOutputs(NonIncrementalTask.kt:81)
	at com.android.build.gradle.internal.tasks.NonIncrementalGlobalTask$taskAction$$inlined$recordTaskAction$1.invoke(BaseTask.kt:62)
	at com.android.build.gradle.internal.tasks.Blocks.recordSpan(Blocks.java:51)
	at com.android.build.gradle.internal.tasks.NonIncrementalGlobalTask.taskAction(NonIncrementalTask.kt:94)
	at com.android.build.api.variant.impl.TaskBasedOperationsImplTest.asynchronousTest(TaskBasedOperationsImplTest.kt:325)
```",2021-04-20T15:43:45Z,blindpirate,melix,easier to read,"Likely not a bug in Gradle. | OK, I've already triggered a build for you. | Pre-tested commit build failed. | @bot-gradle test and merge | OK, I've already triggered a build for you. | OK, I've already triggered a build for you.","Reuse `generatedMethodDetector`

This commit workarounds a classloading issue reported by a user
by reusing the supplied `generatedMethodDetector`, which is
actually already verifying that the method is annotated with
Groovy's `@Generated` method."
gradle/gradle,16195,https://github.com/gradle/gradle/pull/16195,Upgrade test distribution plugin to 2.0-rc-3,"The TD plugin is converted to a settings plugin and we need to adjust the build script accordingly.

TODOs:
- [x] Make sure that ge-experiment.grdev.net has enough agents
- [x] Once merged, check if the BT builds are using TD
- [x] Once merged, shut down agents on ge.gradle.com
- [x] Let the BT team know how to set up the credentials to use TD locally",2021-02-23T02:43:21Z,blindpirate,donat,easier to read,"I've updated the PR to use 2.0-rc-3 as this includes an important fix, which allows executors become available during test execution can join the test task. | @jjohannes Could you please take a quick look at this PR? | Thanks @donat @jjohannes @snoopcheri I'll start dogfooding now.",Upgrade test distribution plugin to 2.0-rc-2 | Update to rc-3 of the TD Gradle plugin | Apply consistent code style | Remove unnecessary newline | Minimize scope of cast
gradle/gradle,29853,https://github.com/gradle/gradle/pull/29853,Resolve a number of bugs resulting in corrupt dependency graphs,"This PR, among other things, resolves a number of bugs that cause corrupt dependency graphs. These corrupt graph manifest to the user in a number of ways, including cryptic error messages, missing artifacts, or silent errors in the resolved graph. These sorts of errors are unacceptable in Gradle, as stability and correctness should be the highest priority of the tool.

The most notable issue resolved by this commit is to filter the nodes involved in a capability conflict before resolving that conflict. Previously, a capability conflict resolver would be able to choose a non-selected node as the winner of a capability conflict. Since this node would not otherwise be part of the graph, this caused serious issues in the final resolution result.

We include a number of other fixes, including:
- Fix a bug where the pending dependencies of a module are not properly tracked when the module is replaced by another. Previously, these hard edges were not properly tracked and we mistakenly removed edges that we assumed were constraints. This surfaced as many errors, including missing first-level dependencies, silent incorrect graphs, and corrupt serialized resolution result errors.
- Not filtering conflict candidates at the time of registering the conflict, as the graph state can change by the time the conflict on that capability is resolved
- Ensuring all capability conflict resolution causes have reasons
- Short-circuit resolving a node's dependencies if it is involved in a conflict
- Remove ResolutionConflictTracker, which was based on incomplete logic and is no longer needed after these changes
- Collapse capability conflicts on the same capability
- Add documentation throughout. Adjust method names and variable names as necessary

Resolves a number of reproducers submitted to: https://github.com/gradle/gradle/issues/14220
Fixes https://github.com/gradle/gradle/issues/26145

### Reviewing cheatsheet

Before merging the PR, comments starting with 
- ❌ ❓**must** be fixed
- 🤔 💅 **should** be fixed
- 💭 **may** be fixed
- 🎉 celebrate happy things
",2024-07-17T00:52:27Z,jvandort,jvandort,comprehensible,"@bot-gradle test qfl without pts | I've triggered the following builds with parameters: -DenablePredictiveTestSelection=false for you. Click here to see all build failures.

QuickFeedbackLinux build | @bot-gradle test prf apt | I've triggered the following builds for you. Click here to see all build failures.

PullRequestFeedback build
AllPerformanceTestsReadyForRelease build | @bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

PullRequestFeedback build | WARN: Based on labels, this pull request addresses notable issue but no changes to release note found.","Resolve a number of bugs resulting in corrupt dependency graphs

This commit, among other things, resolves a number of bugs that cause corrupt dependency graphs. These
corrupt graph manifest to the user in a number of ways, including cryptic error messages, missing
artifacts, or silent errors in the resolved graph. These sorts of errors are unacceptable.

The most notable issue resolved by this commit is to filter the nodes involved in a capability
conflict before resolving that conflict. Previously, a capability conflict resolver would be able
to select a non-selected node as the winner of a capability conflict. Since this node would not
otherwise be part of the graph, this caused serious issues in the final resolution.

We include a number of other fixes, including:
- Not filtering conflict candidates at the time of registering the conflict, as the graph state can
change by the time the conflict on that capability is resolved
- Ensuring all capability conflict resolution causes have reasons
- Short-circuit resolving a node's dependencies if it is involved in a conflict
- Fix a bug where the pending dependencies of a module is not properly tracked when the module is replaced by another.
Previously, these hard edges were not properly tracked and we mistakenly removed edges that we assumed were constraints.
This surfaced as many errors, including missing first-level dependencies, silent incorrect graphs, and corrupt serialized
resolution result errors.
- Remove ResolutionConflictTracker, which was based on incomplete logic and is no longer needed after these changes
- Collapse capability conflicts on the same capability
- Add documentation throughout. Adjust method names and variable names as necessary | Document another reproducer that produces corrupt graphs | Resolve review suggestions

Move capability conflict issue test to proper file and add @Issue reference
Provide variant name in conflict reason when module dependency wins capability conflict
Use 'edge' terminology instead of 'dependency' terminology when we reference an EdgeState
Documentation improvements"
gradle/gradle,28176,https://github.com/gradle/gradle/pull/28176,Adjust redundant configuration usage change warning,"Previously, we emitted a deprecation warning in some cases when a usage for a configuration was set to a value it already had. This was only emitted in certain cases -- for non-legacy, non-detached configurations, and when the value-being-set was to enable the usage.

The goal of this warning is to push users to not try to set roles for configurations that already have their roles set upon creation. However, the previous implementation is not ideal for the two following reasons

1. The warning message was worded in a way to dissuade users from redundantly changing a configuration's usage, making it seem this was a sort of correctness deprecation All other cases of non-redundant changes to roles were handled with a different deprecation warning
2. We only warned about this redundant change in very narrow circumstances -- when the usage's value was being set to true and for configurations with certain names This effectively defeats the purpose of this deprecation. The name matching avoided our own smoke tests from emitting this warning, but there are scenarios that users were hitting where this name matching did not cover.

Really, the goal here is for build logic to not call setCanBeX methods on configurations where the role is already set upon creation. For this reason, we update redundant calls to setCanBeX to emit the same warning as calling these methods non-redundantly. However, since KMP still redundantly calls setCanBeX, we hide the emitting of this deprecation behind a system property. We plan to file an issue with KMP to stop calling setCanBeX on all configurations that already exist -- or those that are managed by Gradle / created via source sets -- and tell them to verify their changes by enabling this system property. Until then, we will not warn for redundant usage changes.

Fixes: https://github.com/gradle/gradle/issues/25609

### Reviewing cheatsheet

Before merging the PR, comments starting with 
- ❌ ❓**must** be fixed
- 🤔 💅 **should** be fixed
- 💭 **may** be fixed
- 🎉 celebrate happy things
",2024-03-27T23:16:54Z,jvandort,jvandort,comprehensible,"🥷 Code experts: tresat
tresat, jvandort have most 👩‍💻 activity in the files.
tresat, jvandort have most 🧠 knowledge in the files.
 
 See details
platforms/documentation/docs/src/docs/userguide/releases/upgrading/upgrading_version_8.adoc
Activity based on git-commit:




tresat
jvandort




MAR

107 additions & 43 deletions


FEB

15 additions & 1 deletions


JAN

178 additions & 129 deletions


DEC

172 additions & 9 deletions


NOV




OCT





Knowledge based on git-blame:
jvandort: 32%
tresat: 2%
platforms/software/dependency-management/src/integTest/groovy/org/gradle/integtests/resolve/api/ConfigurationRoleUsageIntegrationTest.groovy
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN

3 additions & 15 deletions


DEC




NOV




OCT
702 additions & 1 deletions




Knowledge based on git-blame:
tresat: 14%
jvandort: 13%
platforms/software/dependency-management/src/main/java/org/gradle/api/internal/artifacts/configurations/DefaultConfiguration.java
Activity based on git-commit:




tresat
jvandort




MAR

34 additions & 14 deletions


FEB

11 additions & 5 deletions


JAN

277 additions & 424 deletions


DEC
5 additions & 5 deletions
155 additions & 228 deletions


NOV




OCT
2367 additions & 0 deletions




Knowledge based on git-blame:
jvandort: 23%
tresat: 3%
subprojects/internal-integ-testing/src/main/groovy/org/gradle/integtests/fixtures/ConfigurationUsageChangingFixture.groovy
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT





Knowledge based on git-blame:
subprojects/internal-integ-testing/src/main/groovy/org/gradle/integtests/fixtures/executer/ResultAssertion.java
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN




DEC




NOV




OCT





Knowledge based on git-blame:
subprojects/smoke-test/src/smokeTest/groovy/org/gradle/smoketests/WithKotlinDeprecations.groovy
Activity based on git-commit:




tresat
jvandort




MAR




FEB




JAN

12 additions & 0 deletions


DEC




NOV




OCT





Knowledge based on git-blame:
tresat: 60%
jvandort: 5%

To learn more about /:\ gitStream - Visit our Docs | This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 14 days if no further activity occurs. If you don't want the stale bot to close it, then set a milestone for it. | Keep this open | @bot-gradle test this | I've triggered the following builds for you. Click here to see all build failures.

PullRequestFeedback build | The merge queue build has started. Click here to see all failures if any.","Adjust redundant configuration usage change warning

Previously, we emitted a deprecation warning in some cases when a usage for a configuration was set to a value it already had.
This was only emitted in certain cases -- for non-legacy, non-detached configurations, and when the value-being-set was to enable the usage.

The goal of this warning is to push users to not try to set roles for configurations that already have their roles set upon creation.
However, the previous implementation is not ideal for the two following reasons

1. The warning message was worded in a way to dissuade users from redundantly changing a configuration's usage, making it seem this was a sort of style/linting deprecation as opposed to a correctness one
   All other cases of non-redundant changes to roles were handled with a different deprecation warning
2. We only warned about this redundant change in very narrow circumstances -- when the usage's value was being set to true and for configurations with certain names
   This effectively defeats the purpose of this deprecation. The name matching avoided our own smoke tests from emitting this warning, but there are scenarios that users were hitting where this name matching did not cover.

Really, the goal here is for build logic to not call setCanBeX methods on configurations where the role is already set upon creation.
For this reason, we update redundant calls to setCanBeX to emit the same warning as calling these methods non-redundantly.
However, since KMP still redundantly calls setCanBeX, we hide emitting of this deprecation behind a system property.
We plan to file an issue with KMP to stop calling setCanBeX on all configurations that already exist -- or those that are managed by Gradle / created via source sets -- and tell them to verify their changes by enabling this system property.
Until then, we will not warn for redundant usage changes. | Do not print warning if we will fail later | Revise javadoc and message content"
gradle/gradle,26684,https://github.com/gradle/gradle/pull/26684,Add support for multiple features in one component,"Allow Jvm components to track multiple features and test suites. Test suites should eventually become features so these can be tracked together.

This allows us to pull logic for creating the main feature and default test suite outside of the default jvm component. This moves the component more towards a data-only model, leaving the actual configuration to the java plugin.

Additionally, we update registerFeature and the test fixtures plugin to register their features with the component. Previously, these features were detached from the component, however now the component owns these features as it should

Significant clean-up to DefaultJvmSoftwareComponentTest -- all feature-specific functionality is now tested in DefaultJvmFeatureTest Restore many tests in DefaultJvmSoftwareComponentIntegrationTest which were previously removed. These can be added back now that the constructor does not require a feature instance.

### Reviewing cheatsheet

Before merging the PR, comments starting with 
- ❌ ❓**must** be fixed
- 🤔 💅 **should** be fixed
- 💭 **may** be fixed
- 🎉 celebrate happy things
",2023-10-17T23:33:43Z,bot-gradle,jvandort,comprehensible,I've triggered a build for you. Click here to see all failures if there's any. | Pre-tested commit build failed. Click here to see all build failures. | @bot-gradle test and merge | I've triggered a build for you. Click here to see all failures if there's any.,"Add support for multiple features in one component

Allow Jvm components to track multiple features and test suites. Test suites should eventually become features so these can be
tracked together.

This allows us to pull logic for creating the main feature and default test suite outside of the default jvm component.
This moves the component more towards a data-only model, leaving the actual configuration to the java plugin.

Additionally, we update registerFeature and the test fixtures plugin to register their features with the component. Previously,
these features were detached from the component, however now the component owns these features as it should

Significant clean-up to DefaultJvmSoftwareComponentTest -- all feature-specific functionality is now tested in DefaultJvmFeatureTest
Restore many tests in DefaultJvmSoftwareComponentIntegrationTest which were previously removed. These can be added back now that
the constructor does not require a feature instance. | Apply review suggestions

Ensure features are present in component after adding them
Ensure registerFeature can be called multiple times and the features added to the component
Ensure withers are called on feature when verifying that multiple features can exist in one project
Move logic to link registerFeature features with component into the java plugin extension | Restrict withSourcesJar and withJavadocJar to main feature

Otherwise, this would change behavior to make these calls affect test fixtures and registerFeature features | Depend on test-suites-base since classes were moved"
netty/netty,15517,https://github.com/netty/netty/pull/15517,Empty chunks cannot be used while allocating from the shared q,"Motivation:

Once an adaptive chunk is empty it cannot be used for any allocation, including 0 sized buffers. If it happens, the chunk complains that there are no available segments.

Modifications:

Retire the empty chunk if there's no capacity left

Result:

No more empty shared chunks",2025-08-06T12:41:31Z,franz1981,franz1981,"readability, readable","Nice, I'll go merging it, thanks!","Empty chunks cannot be used while allocating from the shared q

Motivation:

Once an adaptive chunk is empty it cannot be used for any allocation, including 0 sized buffers.
If it happens, the chunk complains that there are no available segments.

Modifications:

Retire the empty chunk if there's no capacity left

Result:

No more empty shared chunks"
netty/netty,13955,https://github.com/netty/netty/pull/13955,Refactor file descriptor handling in openFileDescriptors(),"Motivation:

The motivation behind this change is to enhance the readability and maintainability of the codebase by refactoring the file descriptor handling in the openFileDescriptors method. By extracting the logic for closing file descriptors into a separate method, closeFileDescriptor, 

Modification:

In this commit, I have refactored the openFileDescriptors method by extracting the code responsible for closing file descriptors into a separate method, closeFileDescriptor. 

Result:

This PR introduces improved code organization and readability by separating the concerns of opening and closing file descriptors in the openFileDescriptors method. It does not introduce any functional changes.
",2024-04-10T16:04:58Z,chrisvest,Java4ye,"readability, readable","Did you sign the ICLA? https://netty.io/wiki/developer-guide.html

Yes, I have completed the ICLA process. | Looks like you messed up the rebase and entangled your changes with the merge of #13942. Please fix. | Looks like you messed up the rebase and entangled your changes with the merge of #13942. Please fix.

Fixed.
Thanks. | Thanks!",Refactor file descriptor handling in openFileDescriptors() | code style | code style | Merge branch 'netty:4.1' into 4.1-clean-code
netty/netty,13747,https://github.com/netty/netty/pull/13747,Speedup pseudoheader lookup,"Motivation:

Pseudoheader's could be cached within Hpack static table, but when they are added into Http2HeadersSink they are validated, verifying if they are pseudo-headers. Given that the AsciiStrings used within PseudoHeaderName and HpackStaticTable, it always requires to perform `AsciiString's equality.
For the same reason, if user code lookup those pseudo-headers using (as expected) PseudoHeaderName, the same equality check will always be performed.

Modification:

Speed-up pseudo-header lookup algorithm using a simple switch and ensuring that PseudoHeader's name are used to populate HpackStaticTable, to remove the need of a content equality check.

Result:

Faster pseudo-header lookup",2024-01-31T22:10:05Z,normanmaurer,franz1981,"readability, readable, easier to read","@normanmaurer FYI

This is just a tiny fraction of the whole cost (it's no more than 5-10% then the whole), but I'm trying hard to reduce the overhead of lookup to the bare minimum for the most used headers names. | Currently the switch approach is about twice as fast of the existing one using the map, in pretty much any case but with normal strings, where it is just 10% faster.
The approach I have used for the pseudo headers in the static table could be reused with the other known header names (eg content-type) which are currently in HttpHeaderNames and would allow faster lookups from user code which use them, skipping a wasteful content equality and just perform a reference equality, wdyt @normanmaurer ?
This approach can guarantee a very important performance improvement which extend beyond the internals of Netty, to users (and framework built on top of it) | this PR push a significant change in case the Hpack static table is used, which would impact the speed of the mentioned code path at 
  
    
      netty/codec-http2/src/main/java/io/netty/handler/codec/http2/HpackDecoder.java
    
    
        Lines 559 to 561
      in
      ee9bb63
    
  
  
    

        
          
           headers.add(name, value); 
        

        
          
           if (validateHeaders) { 
        

        
          
               previousType = validateHeader(streamId, name, value, previousType); 
        
    
  

 given that both
headers.add and getPseudoHeader (on validateHeader) relies on AsciiString.contentEquals which would be speedup by not performing content equality but just performing a quick reference equality (after seeking the proper slot via AsciiString::hashCode, which is a const for the headers in the static table :P).
I've performed a similar treatment to the other known HttpHeaders.
@normanmaurer @chrisvest let me know how it looks like, if I've understood correctly what the static HPack table is used for... | I think I could further improve if I can detect that headers/pseudo headers are ""hydrated"" from the static/dynamic table and save part of the validation to happen.
For the header's map insertion, it could be great to save any additional header name validation to happen for ""trusted"" header names (or, just obtained from hpack tables, especially static ones). Wdyt @idelpivnitskiy ?
Don't want to cause CVEs with this, eh, but I see revalidation as a waste of cycles, if that happen over and over for already validated header names. | @normanmaurer another problem I see with the flamegraphs at #13747 (comment) is that despite how fast I could make a hit while checking if an header name is a pseudo-header, the code would make it happen twice.
Instead, we would like to:

Detect when we already know that the header name is a pseudo header and which one is eg if rehydrated from a static table
If we don't know if, performing a lookup to find it out
Use a proper http2header ""unsafe"" API which trust the information obtained from the previous points

This would help reducing a lot useless lookups, and performing it just once, at worse, or none, at best. | @chrisvest @normanmaurer I've changed my mind, and I've improved the lookup in every single case, see
before c1aca1202044a69a37adca48953b2f7b146a01b5
Benchmark                                                        (hits)  (same)  Mode  Cnt    Score    Error  Units
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true    true  avgt   20   16.072 ±  0.080  ns/op
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true   false  avgt   20   43.031 ±  0.267  ns/op
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true    true  avgt   20   95.165 ± 13.537  ns/op
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true   false  avgt   20  105.938 ±  5.767  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader            true    true  avgt   20   82.462 ±  1.555  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader            true   false  avgt   20   82.244 ±  0.151  ns/op

on c1aca1202044a69a37adca48953b2f7b146a01b5
Benchmark                                                        (hits)  (same)  Mode  Cnt   Score    Error  Units
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true    true  avgt   20   7.350 ±  0.212  ns/op
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true   false  avgt   20  16.102 ±  0.075  ns/op
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true    true  avgt   20  38.207 ±  3.491  ns/op
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true   false  avgt   20  27.765 ± 10.314  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader            true    true  avgt   20  23.331 ±  0.481  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader            true   false  avgt   20  23.517 ±  0.167  ns/op

The use case, thanks to this PR aiming to reuse pseudoheader's AsciiString, which care the most, is the very first one (getAsciiStringPseudoHeader with same and hits), given that after the changes in this PR we reuse the same AsciiString of the pseudo header name's enum, but we're mildly interested in the rest.
The hits = false case is not interesting (unelss for CVEs), because HTTP 2 validators prevent using others pseudoheader(s).
The String-based uses cases are interesting in case users uses APIs to query pseudoheader(s) which make uses of String literals, conversely they're gonna pay the cost of an intrinsified String::hashCode which should be still decent.
I'm now adding a further commit to fix the java 1.6 compatibility issue at
Error:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on project netty-codec-http2: Compilation failure
Error:  /home/runner/work/netty/netty/codec-http2/src/main/java/io/netty/handler/codec/http2/Http2Headers.java:[269,19] error: strings in switch are not supported in -source 1.6
Error:    (use -source 7 or higher to enable strings in switch)
Error:  -> [Help 1]
Error:  
Error:  To see the full stack trace of the errors, re-run Maven with the -e switch.
Error:  Re-run Maven using the -X switch to enable full debug logging.
Error:  
Error:  For more information about the errors and possible solutions, please read the following articles:
Error:  [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
Error:  
Error:  After correcting the problems, you can resume the build with the command
Error:    mvn <args> -rf :netty-codec-http2
Error: Process completed with exit code 1.

Which is easy to be solved (with some ugly code, as usual). | @idelpivnitskiy @chrisvest PTAL | @idelpivnitskiy @chrisvest PTAL | @bryce-anderson PTAL | These are the 3 benchmark results (from #13747 (comment)):
before c1aca1202044a69a37adca48953b2f7b146a01b5
Benchmark                                                        (hits)  (same)  Mode  Cnt    Score    Error  Units
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true    true  avgt   20   16.072 ±  0.080  ns/op
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true   false  avgt   20   43.031 ±  0.267  ns/op
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true    true  avgt   20   95.165 ± 13.537  ns/op
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true   false  avgt   20  105.938 ±  5.767  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader            true    true  avgt   20   82.462 ±  1.555  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader            true   false  avgt   20   82.244 ±  0.151  ns/op

on c1aca1202044a69a37adca48953b2f7b146a01b5
Benchmark                                                        (hits)  (same)  Mode  Cnt   Score    Error  Units
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true    true  avgt   20   7.350 ±  0.212  ns/op
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true   false  avgt   20  16.102 ±  0.075  ns/op
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true    true  avgt   20  38.207 ±  3.491  ns/op
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true   false  avgt   20  27.765 ± 10.314  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader            true    true  avgt   20  23.331 ±  0.481  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader            true   false  avgt   20  23.517 ±  0.167  ns/op

and the last one is, by using your suggestions (if I got them right) (at f0da758)
Benchmark                                                        (hits)  (same)  Mode  Cnt   Score   Error  Units
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true    true  avgt   20   8.242 ± 0.466  ns/op
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true   false  avgt   20  26.543 ± 0.056  ns/op
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true    true  avgt   20  56.179 ± 5.559  ns/op
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true   false  avgt   20  55.942 ± 0.624  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader            true    true  avgt   20  20.651 ± 0.114  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader            true   false  avgt   20  20.424 ± 0.066  ns/op

the Strings suggestion seems to work very good (even better, actually, because we avoid the hash), but:

getNewAsciiStringPseudoHeader which force creating a new AsciiString  (which can contains the same byte array of a pseudo-header, or a copy of it) to make it compute hash code, forcibly, is receiving a performance penaltly, as expected, although not as bad as it was in 4.1 now.
getAsciiStringPseudoHeader which is using a cached AsciiString (ie which hashcode is already computed), and is a pseudo-header despite not being the same (ref equality) of any existing, it has received a perf penality, meaning that saving the char-by-char equality (eg 
  
    
      netty/codec-http2/src/main/java/io/netty/handler/codec/http2/Http2Headers.java
    
    
        Lines 179 to 196
      in
      88ac731
    
  
  
    

        
          
           private static PseudoHeaderName getMethodSchemeStatusIfEqualsNoPrefix(byte[] array, int offset) { 
        

        
          
               byte first = array[offset + 1]; 
        

        
          
               if (first == 'm') { 
        

        
          
                   if (array[offset + 2] == 'e' && array[offset + 3] == 't' && array[offset + 4] == 'h' && 
        

        
          
                       array[offset + 5] == 'o' && array[offset + 6] == 'd') { 
        

        
          
                       return METHOD; 
        

        
          
                   } 
        

        
          
               } else if (first == 's') { 
        

        
          
                   if (array[offset + 2] == 'c' && array[offset + 3] == 'h' && array[offset + 4] == 'e' && 
        

        
          
                       array[offset + 5] == 'm' && array[offset + 6] == 'e') { 
        

        
          
                       return SCHEME; 
        

        
          
                   } else if (array[offset + 2] == 't' && array[offset + 3] == 'a' && array[offset + 4] == 't' && 
        

        
          
                              array[offset + 5] == 'u' && array[offset + 6] == 's') { 
        

        
          
                       return STATUS; 
        

        
          
                   } 
        

        
          
               } 
        

        
          
               return null; 
        

        
          
           } 
        
    
  

) in favour of using AsciiString::equals didn't work as good, could be either due to array bound checking, increased number of comparisons or because we don't split the equality checks based the uncommon letters first.

Both cases looks to me like ""uncommon"" expected slow paths, especially after this PR, but none prevent users to use different cached AsciiStrings without using the existing ones, unless (@normanmaurer please confirm/reject)
HPack could skip using the static table for pseudo-header(s), meaning that we risk to fall into both above cases.
If that's the case, I would reconsider improving things, by using the char-by-char strategy. | f0da758 is really close to what I was suggesting but may still suffer from the unnecessary computation of hash-codes which is really an artifact of AsciiString.equals(..). Also, unnecessary may not be all that unnecessary since as you pointed out this is part of a larger operation of adding the header to a hashmap and therefore we'll compute the hash regardless for the hot path.
This patch to f0da758 is what I mean:
diff --git a/codec-http2/src/main/java/io/netty/handler/codec/http2/Http2Headers.java b/codec-http2/src/main/java/io/netty/handler/codec/http2/Http2Headers.java
index a2b214f952..c554ddd1be 100644
--- a/codec-http2/src/main/java/io/netty/handler/codec/http2/Http2Headers.java
+++ b/codec-http2/src/main/java/io/netty/handler/codec/http2/Http2Headers.java
@@ -134,15 +134,6 @@ public interface Http2Headers extends Headers<CharSequence, CharSequence, Http2H
                     return "":path"".contentEquals(header)? PATH : null;
                 case 7:
                     // :method, :scheme, :status
-                    if ("":method"" == header) {
-                        return METHOD;
-                    }
-                    if ("":scheme"" == header) {
-                        return SCHEME;
-                    }
-                    if ("":status"" == header) {
-                        return STATUS;
-                    }
                     if ("":method"".contentEquals(header)) {
                         return METHOD;
                     }
@@ -177,16 +168,6 @@ public interface Http2Headers extends Headers<CharSequence, CharSequence, Http2H
                     // :path
                     return PATH.value().equals(header) ? PATH : null;
                 case 7:
-                    if (header == METHOD.value()) {
-                        return METHOD;
-                    }
-                    if (header == SCHEME.value()) {
-                        return SCHEME;
-                    }
-                    if (header == STATUS.value()) {
-                        return STATUS;
-                    }
-                    // :method, :scheme, :status
                     if (METHOD.value().equals(header)) {
                         return METHOD;
                     }
diff --git a/common/src/main/java/io/netty/util/AsciiString.java b/common/src/main/java/io/netty/util/AsciiString.java
index da06a1dc49..997e8e0c4a 100644
--- a/common/src/main/java/io/netty/util/AsciiString.java
+++ b/common/src/main/java/io/netty/util/AsciiString.java
@@ -1136,16 +1136,16 @@ public final class AsciiString implements CharSequence, Comparable<CharSequence>
 
     @Override
     public boolean equals(Object obj) {
-        if (obj == null || obj.getClass() != AsciiString.class) {
-            return false;
-        }
         if (this == obj) {
             return true;
         }
+        if (obj == null || obj.getClass() != AsciiString.class) {
+            return false;
+        }
 
         AsciiString other = (AsciiString) obj;
         return length() == other.length() &&
-               hashCode() == other.hashCode() &&
+                (hash == 0 || other.hash == 0 || hash == other.hash) && // don't trigger hash code computation: the follow up equality check is still necessary either way so checking hash is only a shortcut for the non-equal case.
                PlatformDependent.equals(array(), arrayOffset(), other.array(), other.arrayOffset(), length());
     }
 

I dropped the reference equality checks since they should happen anyway if we get lucky and the .equals(..) call gets inlined but it's hard to say without running it. | @bryce-anderson
re the patch, it's not good to avoid the 3 ref equalilty fast-path eg

getPseudoHeader("":status"") is called
"":method"".equals("":status"") would perform the ref equality check AND fail at the second iteration char (m != s); the equality could be performed by using a vectorized intrinsics, but I think, due to the string length to NOT benefit of it, and just use byte per byte comparisons (with bound checks) - i will take a look anyway
"":scheme"".equals("":status"") will do the same, but failing at the third char (meaning it has to perform, again, 3 chars comparison AND the ref-equality check)
finally "":status"".equals("":status"") win the ref-equality check and complete

In total, forgetting any optimization, we have to perform:

3 ref checks, where 1 has succeeded
2 + 3 chars comparisons and potential equality calls which could NOT be inlined for different reasons (eg ""already compiled into a big method"" or just an unfortunate inlining depth)

The optimized version, previously done, instead, just perform:

3 ref checks, where 1 has succeeded

I'm running a benchmark run with your suggestion just on AsciiString, anyway, although the microbenchmark is the best case scenario.
The benchmark is running just against non-new ascii strings (meaning no hash code need to be computed):
Benchmark                                                     (hits)  (same)  Mode  Cnt   Score   Error  Units
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader    true    true  avgt   20  10.484 ± 0.515  ns/op
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader    true   false  avgt   20  26.013 ± 0.221  ns/op

And the result is not great actually, if compared to what could be achieved with char-by-char comparisons and ref-check fast-paths ie
Benchmark                                                        (hits)  (same)  Mode  Cnt   Score    Error  Units
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true    true  avgt   20   7.350 ±  0.212  ns/op
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true   false  avgt   20  16.102 ±  0.075  ns/op

In short, the suggested changes will improve the readability, but would harm the achieved maximum performance (still in absolute term, eh!), by:

getting 50% slower than the best performance achievable in the AsciiString fast (and common)-path, but still being 40% faster then what it was on master
getting ~60% slower than the best performance achievable in the AsciiString slow-path, but still 40% faster than what it was on master


I dropped the reference equality checks since they should happen anyway if we get lucky and the .equals(..) call gets inlined but it's hard to say without running it.

This is going to help on the getNewAsciiStringPseudoHeader so I'm happy to apply it, although, I don't like we could have a wider than expected performance impact on unrelated areas (other than HTTP 2)
eg issuing equals between non-hashed and non-conflicting AsciiStrings is not sped-up on the second equals attempt (which is ok and expected to me, TBH).
I will still apply your suggestion, but I hope we won't observe evident performance regressions elsewhere and report the full results of benchmarks again. | Applying your full patch I got
Benchmark                                                        (hits)  (same)  Mode  Cnt   Score   Error  Units
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true    true  avgt   20  18.340 ± 0.160  ns/op
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader       true   false  avgt   20  30.618 ± 0.383  ns/op
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true    true  avgt   20  17.721 ± 0.310  ns/op
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true   false  avgt   20  27.735 ± 0.382  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader            true    true  avgt   20  20.201 ± 0.958  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader            true   false  avgt   20  21.198 ± 0.536  ns/op

which show a regression in getAsciiStringPseudoHeader which I would like to save, so I'm creating a new branch picking the parts which i think are acceptable as a performance degradation, and will report it here. | @franz1981 lots of interesting data. I'll admit I'm biased toward readability when we're talking about differences of about 10 ns but thats obviously a subjective opinion. I'll leave it to you, @normanmaurer, and anybody else to make a final decision. All options presented so far look correct to me which is the most important thing.
Regardless of what ya'll decided to do, this is a very detailed performance analysis which is pretty cool. | Thanks @bryce-anderson for completeness I can tell that these values at my previous comment
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true    true  avgt   20  17.721 ± 0.310  ns/op
Http2PseudoHeadersLookupBenchmark.getNewAsciiStringPseudoHeader    true   false  avgt   20  27.735 ± 0.382  ns/op

related your full patch, doesn't seem correct and despite i have a very stable box (isolcpu/numactl isolated/turbo boost off) it has produced once these values, which is not fun...
Anyway, exactly like you, IDK what could be done best although I could send a final commit which try to get the best of both worlds | @franz1981 yeah lets do a final commit with best of both worlds please | In the next days @normanmaurer I promise we can close this :P
But happy about the interaction with @bryce-anderson, thanks again 🙏 | @franz1981 thanks... just ping me when ready | Ready to go @normanmaurer and @bryce-anderson  PTAL
some numbers
I've modified the benchmark to include a case which have the insertion in the headers map as well, which should perform a validation over the headers etc etc to emulate what HPack does, but specifically for pseudoheaders.
I've removed the benchmark cases which we allocating strings/asciistring just for the sake of forcing hash code computation: this is the slowest path we try hard to avoid, with this PR, so I see no points into checking it.
The baseline on 4.1 looks like:
Benchmark                                                              (same)  Mode  Cnt    Score    Error  Units
Http2PseudoHeadersLookupBenchmark.addAsciiStringInHttp2Headers           true  avgt   20  144.130 ± 12.018  ns/op
Http2PseudoHeadersLookupBenchmark.addAsciiStringInHttp2Headers          false  avgt   20  191.564 ±  8.842  ns/op
Http2PseudoHeadersLookupBenchmark.addStringInHttp2Headers                true  avgt   20  352.912 ± 12.620  ns/op
Http2PseudoHeadersLookupBenchmark.addStringInHttp2Headers               false  avgt   20  337.322 ± 15.688  ns/op
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader             true  avgt   20   19.153 ±  0.227  ns/op
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader            false  avgt   20   42.377 ±  0.120  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader                  true  avgt   20   82.420 ±  0.087  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader                 false  avgt   20   82.356 ±  0.040  ns/op

While 7cc2195 :
Benchmark                                                              (same)  Mode  Cnt    Score    Error  Units
Http2PseudoHeadersLookupBenchmark.addAsciiStringInHttp2Headers           true  avgt   20  128.374 ± 11.856  ns/op
Http2PseudoHeadersLookupBenchmark.addAsciiStringInHttp2Headers          false  avgt   20  186.544 ± 11.011  ns/op
Http2PseudoHeadersLookupBenchmark.addStringInHttp2Headers                true  avgt   20  192.007 ±  8.835  ns/op
Http2PseudoHeadersLookupBenchmark.addStringInHttp2Headers               false  avgt   20  224.983 ± 11.036  ns/op
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader             true  avgt   20    8.630 ±  0.054  ns/op
Http2PseudoHeadersLookupBenchmark.getAsciiStringPseudoHeader            false  avgt   20   26.566 ±  0.016  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader                  true  avgt   20   10.036 ±  0.098  ns/op
Http2PseudoHeadersLookupBenchmark.getStringPseudoHeader                 false  avgt   20   20.590 ±  0.281  ns/op

It's clear that ""in the global picture"" of adding to the map (which include removing as well, so it's not completely fair), the improvement still exists but is way more interesting for Strings vs AsciiStrings, despite the former get an hit while added to the header's map (which is not completely fair, given that HPack decoders will never do that!), because their ascii hash code cannot be cached.
The most interesting perrformance increase IMO, is that we now always hit the fastest path in the real world ie HPack decoded AsciiStrings in the static table are the same as the pseudo-headers constants AND users could use them too, to perform the fastest lookup/insertion into headers map.
the code complaxity has been further reduced thanks to @bryce-anderson suggestions, although not the possible peak performence, but as said, in the global picture involving maps operations, is just matter of a bunch of nanoseconds.
@bryce-anderson
It could be possible to further simplify this, but complications happens when we have to perform the fast-path reference checks (either for AsciiString/CharSequence (which tends to be Strings for the most)), unless I modify AsciiStrings adding a new method to perform the ref equality only for both (AsciiString and AsciiString::toString while present). | done @normanmaurer ;) | And thank you for your patience @franz1981, I do appreciate our back and forth: I've learned a lot every time. | @franz1981 thanks a lot for all the work on this!",Speedup pseudoheader lookup | adding ref test check | Addressing comments from norman
netty/netty,13022,https://github.com/netty/netty/pull/13022,Search Domains and DNS Resolver on Windows,"Motivation:
On Windows with JDK 9+ the search domain and dns servers are missing.

Modification:

This PR uses JNI to fetch the DNS Servers from the operating system.

Result:

Fixes #12712 #11885. ",2023-01-27T13:29:14Z,normanmaurer,flx5,"readability, readable","More JNI 😐 | @flx5 please ping us when the comments were addressed | @normanmaurer @chrisvest Sorry, didn't get around to this for quite some time. Although we are using this project (and other open source projects) at work, I am not allowed to work on open source during work hours.
So writing this PR on my own time which can therefore be a little bit slow going.

Do you think it would make more sense to first introduce the hawtjni in another PR before this PR?
Do you think it has a relevant performance / readability impact to use arrays instead of lists in the native code? | @normanmaurer @chrisvest Sorry, didn't get around to this for quite some time. Although we are using this project (and other open source projects) at work, I am not allowed to work on open source during work hours.
So writing this PR on my own time which can therefore be a little bit slow going.

@flx5 no worries...


Do you think it would make more sense to first introduce the hawtjni in another PR before this PR?


Yes please.....


Do you think it has a relevant performance / readability impact to use arrays instead of lists in the native code?


We usually prefer to return arrays. | Hi @normanmaurer,
as discussed the hawtjni update has been placed in #13128.
I've also replaced the linked list with arrays and fixed some build issues.
Could you please review these changes?
Thank you! | @flx5 seems like you didn't sign our ICLA yet ? https://netty.io/s/icla ? | @normanmaurer Sorry, did not see that. I've signed it now. | @flx5 thanks so much ... this is amazing","Windows Native Dns Resolver | Use Windows Native Dns Resolver | Add tests for windows native dns resolver | Migrate Windows Native resolver to C | Add license headers | Update snapshot version | Replace CMake with Hawtjni | Fix Windows Dns resolver | Shading support for Windows Native Resolver | Make Windows NetworkAdapter class package private | Replace explicit native exception with generic runtime exception | Fix build with new windows dns resolver | Merge remote-tracking branch 'upstream/4.1' into feature/resolver-dns-native-windows | Fix parent pom version | Use arrays instead of linked lists for jni | Place msbuild in path | Target MSVC 2019 | Fix windows build | Merge remote-tracking branch 'upstream/4.1' into feature/resolver-dns-native-windows | Allow blocking call for loading the dns dll | Update Copyright

Co-authored-by: Norman Maurer <norman_maurer@apple.com> | Use https for xml namespaces

Co-authored-by: Norman Maurer <norman_maurer@apple.com> | Fix whitespace

Co-authored-by: Norman Maurer <norman_maurer@apple.com> | Remove redefinition of JNI Version"
netty/netty,12869,https://github.com/netty/netty/pull/12869,"Apply 'VisibleForTesting', 'TestOnly' annotations","Motivation:

Start to use `@VisibleForTesting` in Netty 5 to make it easier to users to understand what to expect.

Modification:

Applied `@VisibleForTesting`

Added Notice

Result:

Better code readability & IDE support",2022-10-06T20:57:38Z,chrisvest,jchrys,"readability, readable",Thanks! | Thank you very much! 👍,"Apply 'VisibleForTesting', 'TestOnly' annotations

Motivation:

Start to use @VisibleForTesting in Netty 5 to make it easier to users to understand what to expect.

Modification:

Applied @VisibleForTesting

Added Notice

Result:

Better code readability & IDE Support"
netty/netty,12670,https://github.com/netty/netty/pull/12670,Support for pkcs1,"Signed-off-by: Craig Perkins <cwperx@amazon.com>

Motivation:

Motivated by a stale [PR](https://github.com/netty/netty/pull/7451) that was closed. This change adds support for keys in the PKCS#1 format. Currently netty only supports PKCS#8 keys.

Modification:

This change introduces a class called `BouncyCastlePemReader` which is only used if BouncyCastle is available on the classpath and uses BouncyCastle's PEMParser to parse the private keys. See list of supported types [here](https://www.bouncycastle.org/docs/pkixdocs1.5on/org/bouncycastle/openssl/PEMParser.html). 

Tests are added in `SSLContextTest` including tests with PKCS#8 keys (encrypted + unencrypted) to show these working with BouncyCastle.

Result:

Fixes [#7323](https://github.com/netty/netty/issues/7323)
",2022-08-10T13:58:00Z,normanmaurer,cwperks,"readability, readable","@normanmaurer Thank you for the review. I have addressed the code review comments and moved this block:
if (!isAvailable()) {
    if (logger.isDebugEnabled()) {
        logger.debug(""Bouncy castle provider is unavailable."", unavailabilityCause());
    }
    return null;
}

into the public static PrivateKey BouncyCastlePemReader.getPrivateKey(...) methods and exit early if BC is unavailable. | @cwperks thanks a lot!","Add support for pkcs#1

Signed-off-by: Craig Perkins <cwperx@amazon.com> | Add tests with pkcs#8 keys as well to show BouncyCastle supports both

Signed-off-by: Craig Perkins <cwperx@amazon.com> | Add genrsa statements for both pksc8 and pkcs1

Signed-off-by: Craig Perkins <cwperx@amazon.com> | Return early is BC provider is unavailable and remove assignment in if clause

Signed-off-by: Craig Perkins <cwperx@amazon.com> | Switch from PrivilegedAction<Boolean> to PrivilegedAction<Void>

Signed-off-by: Craig Perkins <cwperx@amazon.com> | Move BouncyCastlePemReader to io.netty.handler.ssl

Signed-off-by: Craig Perkins <cwperx@amazon.com>"
netty/netty,11685,https://github.com/netty/netty/pull/11685, Add new Compression / Decompression API which not depends on the Channel API,"Motivation:

At the moment all our compression implementations are written by implement the ChannelHandler interface. Due of this re-using these in other codecs (like for example HTTP1/HTTP2) makes things very heavy weight. It would be much better if we implement these by only depend on ByteBuf and some API contract. This way it will be easier to re-use things and make it easier to optimize in the future.

Modifications:

  - Add Compressor and Decompressor interfaces
  - Use these interfaces for all our compression implementations
  - Add CompressionHandler which uses a Compressor to do the compression
  - Add DecompressionHandler which uses a Decompressor to doo the decompression
  - Adjust tests

  Result:

  More fine grained API.",2021-09-24T06:14:43Z,normanmaurer,normanmaurer,"readability, readable","As a followup I will adjust the various compression handlers (like for http2 / http1) to use the API directly and so not need the EmbeddedChannel | @NiteshKant PTAL again... I addressed your comment related to throwing after close etc. | Had mostly stylistic comments. Not a fan of the nested switch/case clauses with some loops in between, but also don't know how much work would be involved in refactoring that to look nicer, since some of them are relying on local control flow that wouldn't work if the code was extracted to a separate method.

I think with the extracting of the code to a separate method it is much cleaner now. Thanks for the feedback, addressed all of it. | The extracted methods somehow don't have newlines after them, so there's no gap between them and the next method, but other than that I think this looks good.

fixed","Add new Compression / Decompression API which not depends on the Channel API

Motivation:

At the moment all our compression implementations are written by implement the ChannelHandler interface. Due of this re-using these in other codecs (like for example HTTP1/HTTP2) makes things very heavy weight. It would be much better if we implement these by only depend on ByteBuf and some API contract. This way it will be easier to re-use things and make it easier to optimize in the future.

Modifications:

- Add Compressor and Decompressor interfaces
- Use these interfaces for all our compression implementations
- Add CompressionHandler which uses a Compressor to do the compression
- Add DecompressionHandler which uses a Decompressor to doo the decompression
- Update lz4 version to fix bug related to heap buffers
- Adjust tests

Result:

More fine grained API. | docs | more fixes | Address comments | More comments | Add isClosed() | Throw when trying to compress after close | Throw after closed | Allow to configure the timeout | Allow if bytes should be discarded or not | More comments | Add empty lines | Address last comment"
netty/netty,11560,https://github.com/netty/netty/pull/11560,Add support for client-side TCP FastOpen to KQueue MacOS,"Motivation:
The MacOS-specific `connectx(2)` system call make it possible to establish client-side connections with TCP FastOpen.

Modification:
Add support for TCP FastOpen to the KQueue transport, and add the `connectx(2)` system call to `BsdSocket`.

Result:
It's now possible to use TCP FastOpen when initiating connections on MacOS.",2021-08-12T11:38:46Z,chrisvest,chrisvest,"readability, readable, easier to read","@Scottmitch addressed your comments, please take a look. | @Scottmitch @NiteshKant @normanmaurer I've verified that this works locally; was able to establish a TFO connection from a MacOS host, running with kqueue, to a Linux guest VM running with Epoll, and see the SslHandlers ClientHello get transferred with the SYN packet on repeated connections. | @Scottmitch yeah, been working on adding this to the kqueue test permutations","Add `connectx(2)` to BsdSocket | Draft of adding client-side support for TCP FastOpen to the MacOS KQueue transport | Make one of the BsdSocket.connectx private | Cache the CONNECT_RESUME_ON_READ_WRITE and CONNECT_DATA_IDEMPOTENT values as class constants in Native | Explain the ""unspecified source interface"" value used with connectx | Improve BsdSocket.connectx null handling | Throw exceptions from BsdSocket.connectx if address conversion fails | Fix java compatibility error in BsdSocket | Fix a number of issues with the KQueue TFO support | When using connectx for TFO on non-blocking sockets, it will return EINPROGRESS

Make sure we handle that as a successful call. | Add TCP FastOpen to the KQueue test permutations

Also fix a bug where using TFO with KQueue would prematurely consider the socket connected.
Instead, the socket should assume to be connect-in-progress when using TFO since all our sockets are non-blocking. | Accurately determine if a connection is in-progress or not, when using connectx/TCP FastOpen on MacOS"
netty/netty,15700,https://github.com/netty/netty/pull/15700,Native: Include error value when throwing NativeIoException in the me…,"…ssage

Motivation:

We should include the error value (which is a negative errno) in the message of the NativeIoException. This allows easier debugging as it is much easier to lookup the errno value and its cause then the human readable message.

Modification:

Include error in the message of the exception

Result:

Easier to debug failed native ops",2025-09-24T06:28:37Z,normanmaurer,normanmaurer,readable,I think this is a candidate to back port to 4.1 as well.,"Native: Include error value when throwing NativeIoException in the message

Motivation:

We should include the error value (which is a negative errno) in the message of the NativeIoException. This allows easier debugging as it is much easier to lookup the errno value and its cause then the human readable message.

Modification:

Include error in the message of the exception

Result:

Easier to debug failes native ops"
netty/netty,15181,https://github.com/netty/netty/pull/15181,implement readable/writeable array accessors on MemSegBuffer,"Motivation:
The `readableArray` and `writableArray` methods (and related) are currently not implemented for MemorySegment backed buffers, even if the MemorySegmentt is backed by a byte array.

Modification:
Implement the methods.

Result:
The `readableArray` and `writableArray` methods (and related) are useable with MemorySegment based buffers like they are with all other buffer types.

Fixes #15164
",2025-05-15T03:45:05Z,chrisvest,derklaro,readable,@derklaro Thanks!,implement readable/writeable array accessors on MemSegBuffer | add missing assertion | cleanup
netty/netty,15216,https://github.com/netty/netty/pull/15216,WebSocket08FrameEncoder improvements,"While checking https://github.com/netty/netty/issues/15157, I found a few small improvements that could be done for `WebSocket08FrameEncoder`:

- `opcode` resolving moved to its own method to make the code more readable
- removed unnecessary `isReadable()` call and replaced it with `length > 0` check
- replaced modulo operations with bit operations which are cheaper",2025-05-20T13:18:04Z,normanmaurer,doom369,readable,@doom369 thanks!,WebSocket08FrameEncoder improvements | avoid unnecessary class cast
netty/netty,14246,https://github.com/netty/netty/pull/14246,"In some scenarios, avoid unnecessary memory allocation","**Motivation:**
When the number of readable bytes in the headerBlock is less than or equal to maxFrameSize, it will result in an unnecessary allocation of ByteBuf, as well as logical operations on this ByteBuf.



 ",2024-09-11T12:11:53Z,normanmaurer,CLFutureX,readable,"Please check #13783 as well | @normanmaurer hey,please take a look | @franz1981 I think we can merge this one before #13783... WDYT ? | @idelpivnitskiy PTAL as well | @CLFutureX thanks",reduce the number of byteBuf allocations | reduce the number of byteBuf | reduce the number of byteBuf | reduce the number of byteBuf | reduce the number of byteBuf | reduce the number of byteBuf | Merge branch '4.1' into 4.1_reduce_byteBuf_alloc | avoid unnecessary memory allocation | avoid unnecessary memory allocation
netty/netty,14220,https://github.com/netty/netty/pull/14220,Ensure AbstractCoalescingBufferQueue does not end up in inconsistent …,"…state on error

Motivation:

The code in AbstractCoalescingBufferQueue.remove(...) could lead to inconsistent internal state when for example an exception is thrown because of reference counting problems. Because of this is was possible that while the queue was empty it still showed readableBytes() > 0. This could then lead to various issues internally within netty where we depend on that the queue will eventually returns readableBytes() == 0.

Modifications:

- Ensure we always decrement readable bytes, even in the cause of an error
- Also correctly remove the listener if an error happens during handling the buffer.

Result:

Fixes https://github.com/netty/netty/issues/11959
",2024-08-13T04:11:11Z,normanmaurer,normanmaurer,readable,,"Ensure AbstractCoalescingBufferQueue does not end up in inconsistent state on error

Motivation:

The code in AbstractCoalescingBufferQueue.remove(...) could lead to inconsistent internal state when for example an exception is thrown because of reference counting problems. Because of this is was possible that while the queue was empty it still showed readableBytes() > 0. This could then lead to various issues internally within netty where we depend on that the queue will eventually returns readableBytes() == 0.

Modifications:

- Ensure we always decrement readable bytes, even in the cause of an error
- Also correctly remove the listener if an error happens during handling the buffer.

Result:

Fixes https://github.com/netty/netty/issues/11959 | Correctly update state before notify listeners | Simplify by remove type check"
netty/netty,12694,https://github.com/netty/netty/pull/12694,Merge readable and writable buffer component iteration,"Motivation:
It should be possible to iterate the components of a buffer, even when it has no readable or writable bytes.
For the purpose of pointer arithmetic, it should also be possible to get a base pointer for off-heap buffers.
It additionally turns out that unifying ReadableComponent and WritableComponent into a single BufferComponent, allow us to make the iteration entirely allocation free for non-composite buffers.
This in turn allow us to simplify the API by removing the internal iteration methods and only keep a single forEachComponent iteration method.

Modification:
- Remove WritableComponent and ReadableComponent, and their processors.
- And in their place have a single BufferComponent which contain both of the APIs from the readable and writable components.
- Add a baseNativeAddress method to BufferComponent.
- Change all usages sites of both the internal and external versions of forEachReadable/Writable, to use the new forEachComponent.
- Add firstReadable and firstWritable convenience methods to ComponentIterator, which will skip over components that are not readable or writable, respectively.
- Add nextReadable and nextWritable methods to ComponentIterator.Next, which do the same.
- Change places like EngineWrapper, IovArray, etc. to no longer rely on internal iteration but use external iteration instead.
- Remove processor classes that only existed to be used for internal iteration.
- Read-only buffers no longer have any ""writable bytes"".

Result:
Iterating the components of non-composite buffers is now always allocation-free; no capturing-lambda or iterator objects are constructed.
The APIs are also simpler; fewer methods and fewer classes.

Fixes #12609 and closes #12611
",2022-08-16T19:36:53Z,chrisvest,chrisvest,readable,@timothyklim FYI | Looking into the epoll test failure that showed up after the rebase.,"Merge readable and writable buffer component iteration

Motivation:
It should be possible to iterate the components of a buffer, even when it has no readable or writable bytes.
For the purpose of pointer arithmetic, it should also be possible to get a base pointer for off-heap buffers.
It additionally turns out that unifying ReadableComponent and WritableComponent into a single BufferComponent, allow us to make the iteration entirely allocation free for non-composite buffers.
This in turn allow us to simplify the API by removing the internal iteration methods and only keep a single forEachComponent iteration method.

Modification:
- Remove WritableComponent and ReadableComponent, and their processors.
- And in their place have a single BufferComponent which contain both of the APIs from the readable and writable components.
- Add a baseNativeAddress method to BufferComponent.
- Change all usages sites of both the internal and external versions of forEachReadable/Writable, to use the new forEachComponent.
- Add firstReadable and firstWritable convenience methods to ComponentIterator, which will skip over components that are not readable or writable, respectively.
- Add nextReadable and nextWritable methods to ComponentIterator.Next, which do the same.
- Change places like EngineWrapper, IovArray, etc. to no longer rely on internal iteration but use external iteration instead.
- Remove processor classes that only existed to be used for internal iteration.
- Read-only buffers no longer have any ""writable bytes"".

Result:
Iterating the components of non-composite buffers is now always allocation-free; no capturing-lambda or iterator objects are constructed.
The APIs are also simpler; fewer methods and fewer classes.

Fixes #12609 and closes #12611 | Fix spelling in Buffer.forEachComponent javadoc

Co-authored-by: Aayush Atharva <hyperx.pro@outlook.com> | Cleanups and fix adding composite buffers to IovArray | Restore prior interaction with ChannelOutboundBuffer in KQueue and Epoll"
netty/netty,12460,https://github.com/netty/netty/pull/12460,Fix buffer composition where middle buffers have no readable bytes,"Motivation:
Composite buffers are picky with how their internal buffers are structured.
To cope, the buffer components are sanitised and trimmed to make internal offset calculations easier.
We had an issue where buffer components without any readable bytes, were allowed to sit between components that did have readable bytes.
This could cause an exception to be thrown, if a multi-byte access was striding over an empty buffer.
Effectively, we were always expected such accesses to be able to reach at least one byte.

Modification:
Do more thorough component sanitation, such that we prevent buffer components with no readable bytes, from being placed in between components with readable bytes.
This allows the turn accesses to continue to work as before.

Result:
No more exceptions from torn accesses that straddle across an empty buffer component.",2022-06-13T16:14:22Z,chrisvest,chrisvest,"readable, easier to read",@normanmaurer Comments addressed.,"Fix buffer composition where middle buffers have no readable bytes

Motivation:
Composite buffers are picky with how their internal buffers are structured.
To cope, the buffer components are sanitised and trimmed to make internal offset calculations easier.
We had an issue where buffer components without any readable bytes, were allowed to sit between components that did have readable bytes.
This could cause an exception to be thrown, if a multi-byte access was striding over an empty buffer.
Effectively, we were always expected such accesses to be able to reach at least one byte.

Modification:
Do more thorough component sanitation, such that we prevent buffer components with no readable bytes, from being placed in between components with readable bytes.
This allows the turn accesses to continue to work as before.

Result:
No more exceptions from torn accesses that straddle across an empty buffer component. | Address review comments"
netty/netty,14031,https://github.com/netty/netty/pull/14031,Remove edge-triggered support for epoll and just always use level-tri…,"…ggered

Motivation:

We supported edge-triggered and level-triggered modes for our epoll transport. This made things more complex while not really providing much value.

Modifications:

- Remove edge-triggered support and just use level-triggered all the time

Result:

Less complexity. Fixes https://github.com/netty/netty/issues/14007
",2024-05-03T19:46:36Z,normanmaurer,normanmaurer,readable,"That is a nice cleanup but I wonder why was ET ever added if there is no benefit? In my minds eye there are less system calls under some scenarios but it's also possible that we just never hit those in practice.

I added it because in theory it should be less expensive. That said it not really gains us much because of how netty works. We still use ET for eventfd and timerfd tho.","Remove edge-triggered support for epoll and just always use level-triggered

Motivation:

We supported edge-triggered and level-triggered modes for our epoll transport. This made things more complex while not really providing much value.

Modifications:

- Remove edge-triggered support and just use level-triggered all the time

Result:

Less complexity. Fixes https://github.com/netty/netty/issues/14007 | Fix rdhup handling | Make method final"
netty/netty,12388,https://github.com/netty/netty/pull/12388,Add method to get read-only Buffer copy without offsets,"Motivation:
It seems to be the common case that when we want a read-only copy of a buffer, we want a copy of the readable byte range.

Modification:
Add a method specifically for this use case, where we can request a read-only copy, without having to specify the readable byte range explicitly.
Also update every relevant usage site.

Result:
Cleaner code, and easier access to the structural-sharing copy optimisation.",2022-05-12T12:37:36Z,normanmaurer,chrisvest,"readable, easier to read",,"Add method to get read-only Buffer copy without offsets

Motivation:
It seems to be the common case that when we want a read-only copy of a buffer, we want a copy of the readable byte range.

Modification:
Add a method specifically for this use case, where we can request a read-only copy, without having to specify the readable byte range explicitly.
Also update every relevant usage site.

Result:
Cleaner code, and easier access to the structural-sharing copy optimisation."
netty/netty,12047,https://github.com/netty/netty/pull/12047,Buffer native addresses should be off-set,"Motivation:
It's hard to make use of the native addresses when they are always pointing to the base of the allocation, rather than where the readable or writable data actually starts.

Modification:
Make the native addresses point to where the readable, or writable, region starts, respectively.
Also, if Unsafe is not available, we will optimistically fall back to the JNI bypass in the native transport module, iff we can find it, and it's available.

Result:
This makes it possible to do native IO calls using readableNativeAddress() and readableBytes(), or writableNativeAddress() and writableBytes(), respectively.
No funky and fragile external offset computation needed.",2022-01-25T20:40:02Z,chrisvest,chrisvest,readable,"@normanmaurer Addressed, thanks","Buffer native addresses should be off-set

Motivation:
It's hard to make use of the native addresses when they are always pointing to the base of the allocation, rather than where the readable or writable data actually starts.

Modification:
Make the native addresses point to where the readable, or writable, region starts, respectively.
Also, if Unsafe is not available, we will optimistically fall back to the JNI bypass in the native transport module, iff we can find it, and it's available.

Result:
This makes it possible to do native IO calls using readableNativeAddress() and readableBytes(), or writableNativeAddress() and writableBytes(), respectively.
No funky and fragile external offset computation needed. | Address comments"
netty/netty,12237,https://github.com/netty/netty/pull/12237,Fix composite buffer invariant escape,"Motivation:
It was possible to construct a composite buffer that had its internal invariants violated.
Specifically, gaps could be created in the readable and writable byte regions.

Modification:
Fix the gap checking code, which run every time we modify the component structure of a composite buffer.

Result:
It should no longer be possible to create a composite buffer that is internally inconsistent and broken in weird ways.",2022-03-28T08:51:25Z,normanmaurer,chrisvest,readable,,"Fix composite buffer invariant escape

Motivation:
It was possible to construct a composite buffer that had its internal invariants violated.
Specifically, gaps could be created in the readable and writable byte regions.

Modification:
Fix the gap checking code, which run every time we modify the component structure of a composite buffer.

Result:
It should no longer be possible to create a composite buffer that is internally inconsistent and broken in weird ways."
netty/netty,12452,https://github.com/netty/netty/pull/12452,Make buffer composition requirements more lenient,"Motivation:
Our composite buffers required that their components could be concatenated in a gapless fashion,
where there were no already-read or writable regions in between regions of readable memory.
This requirement turned out to be quite restrictive in practice.

Modification:
The buffer composition process now tolerates such gaps, and trim the incoming buffers to its liking in the composition process.

Result:
We can now compose arbitrary buffers without worrying about gaps, which makes it much easier to use this API.",2022-06-09T08:03:44Z,normanmaurer,chrisvest,"readable, easier to read",,"Make buffer composition requirements more lenient

Motivation:
Our composite buffers required that their components could be concatenated in a gapless fashion,
where there were no already-read or writable regions in between regions of readable memory.
This requirement turned out to be quite restrictive in practice.

Modification:
The buffer composition process now tolerates such gaps, and trim the incoming buffers to its liking in the composition process.

Result:
We can now compose arbitrary buffers without worrying about gaps, which makes it much easier to use this API."
netty/netty,12203,https://github.com/netty/netty/pull/12203,Fix a data race bug in copyInto on NioBuffer const buffer children,"Motivation:
The `copyInto` method on `NioBuffer` may mutate the limit and position of the underlying readable `ByteBuffer` instance.
This can lead to rare and weird issues when `copyInto` is called concurrently on const children of the same parent buffer.
For this reason, while const buffers can still share the underlying memory, they will need to do so with independent `ByteBuffer` instances to avoid sharing position and limit.

Modification:
Make sure each `NioBuffer` const child have their own independent duplicate of the parent `ByteBuffer` instance.

Result:
One fewer data race bugs.",2022-03-22T08:42:04Z,normanmaurer,chrisvest,readable,,"Fix a data race bug in copyInto on NioBuffer const buffer children

Motivation:
The `copyInto` method on `NioBuffer` may mutate the limit and position of the underlying readable `ByteBuffer` instance.
This can lead to rare and weird issues when `copyInto` is called concurrently on const children of the same parent buffer.
For this reason, while const buffers can still share the underlying memory, they will need to do so with independent `ByteBuffer` instances to avoid sharing position and limit.

Modification:
Make sure each `NioBuffe` const child have their own independent duplicate of the parent `ByteBuffer` instance.

Result:
One fewer data race bugs."
netty/netty,11665,https://github.com/netty/netty/pull/11665,Buffer should not expose nativeAddress() directly,"Motivation:
Accessing the native address, if a buffer has any, violates the no-aliasing rule for Buffers.
Also, it inherently assumes that a buffer has a single, native memory allocation.
The native address, or addresses, are already available via the Readable- and WritableComponents.
Accessing these via forEachReadable and forEachWritable will also ensure that composite buffers will be handled correctly.

Modification:
Remove the nativeAddress() method from the Buffer API.
Update the ByteBufAdaptor and a few tests to cope with this change.

Result:
Less error prone code, and make unsafe APIs a bit more hidden.
",2021-09-08T18:02:59Z,chrisvest,chrisvest,readable,"How would this work in cases were we use the buffers in JNI ? | The alternative is to get the address via forEachReadable or forEachWritable, depending on the use case.","Buffer should not expose nativeAddress() directly

Motivation:
Accessing the native address, if a buffer has any, violates the no-aliasing rule for Buffers.
Also, it inherently assumes that a buffer has a single, native memory allocation.
The native address, or addresses, are already available via the Readable- and WritableComponents.
Accessing these via forEachReadable and forEachWritable will also ensure that composite buffers will be handled correctly.

Modification:
Remove the nativeAddress() method from the Buffer API.
Update the ByteBufAdaptor and a few tests to cope with this change.

Result:
Less error prone code, and make unsafe APIs a bit more hidden."
netty/netty,12036,https://github.com/netty/netty/pull/12036,ByteBuffer from component iteration must keep unsafe memory alive,"Motivation:
It is possible to use forEachReadable or forEachWritable to create a ByteBuffer that is backed by native memory from a Buffer.
If we are using Unsafe, then we cannot rely on the underlying ByteBuffer implementation to keep the native memory alive for as long as it is accessible.
We must guard against the situation where forEachReadable/Writable is used on an UnsafeBuffer, then a readable/writeableBuffer is created, and this buffer is siphoned out of the iteration, and then kept alive for longer than the UnsafeBuffer.
We can only keep the UnsafeMemory alive, though.
We cannot guard against use-after-free, because we don't have a way to nerf a ByteBuffer once it has been released.

Modification:
Change the PlatformDependent.directBuffer method to take an attachment object, and pass it to the DirectByteBuffer constructor.
Then use this from the UnsafeBuffer *Component implementation to ensure that the underlying UnsafeMemory is kept alive and strongly reachable by any ByteBuffer instances we create.

Result:
Reduce the possibility of causing JVM crash through misuse of the ability to get ByteBuffers out of an UnsafeBuffer.",2022-01-24T08:13:30Z,normanmaurer,chrisvest,readable,"Just curious, why not https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/ref/Reference.html#reachabilityFence(java.lang.Object) in such methods? | @franz1981 Because forEachReadable/Writable is public API, and we cannot control the code around such calls.","ByteBuffer from component iteration must keep unsafe memory alive

Motivation:
It is possible to use forEachReadable or forEachWritable to create a ByteBuffer that is backed by native memory from a Buffer.
If we are using Unsafe, then we cannot rely on the underlying ByteBuffer implementation to keep the native memory alive for as long as it is accessible.
We must guard against the situation where forEachReadable/Writable is used on an UnsafeBuffer, then a readable/writeableBuffer is created, and this buffer is siphoned out of the iteration, and then kept alive for longer than the UnsafeBuffer.
We can only keep the UnsafeMemory alive, though.
We cannot guard against use-after-free, because we don't have a way to nerf a ByteBuffer once it has been released.

Modification:
Change the PlatformDependent.directBuffer method to take an attachment object, and pass it to the DirectByteBuffer constructor.
Then use this from the UnsafeBuffer *Component implementation to ensure that the underlying UnsafeMemory is kept alive and strongly reachable by any ByteBuffer instances we create.

Result:
Reduce the possibility of causing JVM crash through misuse of the ability to get ByteBuffers out of an UnsafeBuffer."
netty/netty,13786,https://github.com/netty/netty/pull/13786,Prevent sharing the index of the continuation frame header ByteBuf.,"Motivation:
The current implementation uses the `byteBuf` for a continuation frame header multiple times if the header length exceeds `3 * maxFrameLength`. However, it fails to slice the `byteBuf` during usage. [Reference](https://github.com/netty/netty/blob/d027ba7320d430743992d613e52596b0182ca854/codec-http2/src/main/java/io/netty/handler/codec/http2/DefaultHttp2FrameWriter.java#L570)

Modification:
- Introduce `ByteBuf.retainedSlice()` for a continuation frame header when it's used to prevent sharing the index.

Result:
- Correctly send continuation frame headers to the remote peer, addressing the issue of reusing the index of the ByteBuf.",2024-01-18T09:24:27Z,normanmaurer,minwoox,readable,"However, it fails to slice the byteBuf during usage

In which occasion it happens? what means ""usage"" in this context? | @franz1981 I think this fix is correct as you need to either call slice() or duplicate() if you pass the same buffer down the pipeline multiple times as otherwise you might end up with a buffer that had its indexes modified by a previous write operation.
@minwoox great catch! Did you sign our icla yet ? https://netty.io/s/icla | Yep @normanmaurer I was just curious to know how he found it and how we didn't have coverage for that (including our vertx test suite) - cc @vietj | hi all, would it be possible to cut a Netty release after this PR is merged? thanks! | Hi @franz1981

I was just curious to know how he found it

This is how we discovered it: line/armeria#5385 | Many thanks for sharing @Lincong  !!! | I think this fix is correct as you need to either call slice() or duplicate() if you pass the same buffer down the pipeline multiple times as otherwise you might end up with a buffer that had its indexes modified by a previous write operation.

I used my debugger to confirm that this is the case.
In AbstractCoalescingBufferQueue, this is the point before sending the first CONTINUATION frame header:

After sending the first CONTINUATION frame header. This is the point before sending the second CONTINUATION frame header. As we can see, there is no more readable bytes in the header:

All readable bytes from the 1st CONTINUATION frame header ByteBuf is fully consumed [here]:(
  
    
      netty/transport/src/main/java/io/netty/channel/AbstractCoalescingBufferQueue.java
    
    
         Line 316
      in
      b194741
    
  
  
    

        
          
           newCumulation.writeBytes(cumulation).writeBytes(next); 
        
    
  

):

After being copied to the cumulation ByteBuf, it contains no readable bytes: | Did you sign our icla yet ? netty.io/s/icla

Done it. 😉
Thanks @Lincong for sharing the issue. 😉

how we didn't have coverage for that (including our vertx test suite)

It only happens when TLS is used and the header length exceeds 3 * maxFrameLength. That's why it isn't found yet. (Armeria also does not cover this case.)
When TLS is used, the byteBuf is read in the queue of the SslHandler as @Lincong illustrated.
When TLS is not used, the reader and writer indexes of the byteBuf are used directly instead of reading the bytebuf in the ChannelOutboudBuffer so we couldn't find that.","Prevent sharing the index of the continuation frame header ByteBuf.
Motivation:
The current implementation uses the `byteBuf` for a continuation frame header multiple times if the header length exceeds `3 * maxFrameLength`. However, it fails to slice the `byteBuf` during usage.
[Reference](https://github.com/netty/netty/blob/d027ba7320d430743992d613e52596b0182ca854/codec-http2/src/main/java/io/netty/handler/codec/http2/DefaultHttp2FrameWriter.java#L570)

Modification:
- Introduce `ByteBuf.retainedSlice()` for a continuation frame header when it's used to prevent sharing the index.

Result:
- Correctly send continuation frame headers to the remote peer, addressing the issue of reusing the index of the ByteBuf."
netty/netty,13789,https://github.com/netty/netty/pull/13789,Updated HTTP2 Reader to fix missing header state,"Motivation:
When a malformed request comes in where a stream error is thrown, data is still left on the reader and not properly cleaned. When a valid request comes in, it is not properly parsed because of the leftover data being included.

Modifications:
Updated DefaultHttp2Reader to clean up leftover data if these types of stream exceptions come in the wire.

Result:
After this change, data should be properly parsed after a malformed request
Fixes #13788 

If there is no issue then describe the changes introduced by this PR.
",2024-02-15T07:23:14Z,normanmaurer,isaacrivriv,readable,"Can you include test case? | @idelpivnitskiy @bryce-anderson PTAL | Added a simple test case that fails before the changes in the PR | @isaacrivriv, I imagine you're busy but I wanted to see what your plan was for this. It's a great find and I'd love to see it fixed. 🙂 | Sorry for the late reply, I was trying to get to this last week but couldn't get it to a state I was happy with. I added a push with the changes we discussed in mind. I was following these steps as my approach

Parsing the framing layer component of the frame (everything all the frame types have in common)
Verifying the data read from the framing layer based off the different types
Slicing the buffer after verifying and processing the payload for this specific frame
Resetting the flags specifically the readingHeaders flag to the default value once a stream reset occurs while processing the headers/payload

The test I added passes and ran all the tests locally for http2 which pass as well. I did some manual testing to replicate garbage requests submitting more data than actually announced in the payloadLength and that still fails as expected. I'm not sure of a way to be able to identify these cases and respond to the client that something may be wrong with their implementation if anyone had anything in mind. | Thanks for the detailed review @bryce-anderson! I'm getting a better understanding on how everything works behind the scene while doing this. I pushed out changes based on the comments. I also had to update the test I added to add dummy data to the buffer for the reader to consume it and continue since the logic to identify if the request is valid or not (extra data or not enough data for the request) is not added. Let me know what you think! | /cc @ejona86 | Let's plan to get this in after we cut the next release just to give it a bit more time to bake and for people like @ejona86 to check it out. | @isaacrivriv thanks a lot! I also cherry-picked into main","Updated HTTP2 Reader to fix missing header state

Motivation:
When a malformed request comes in where a stream error is thrown, data
is still left on the reader and not properly cleaned. When a valid
request comes in, it is not properly parsed because of the leftover data
being included.

Modifications:
Updated DefaultHttp2Reader to clean up leftover data if these types of
stream exceptions come in the wire.

Result:
After this change, data should be properly parsed after a malformed
request | Added test case for malformed frame | Alternative approach for processing stream resets
- Preprocess frame data for hadling flags between frame types
- Run check on header depending on the frame type
- Process payload data only on data being read by slicing buffer and reset header state afterwards
- Reset reading flags if a stream reset occurs | Fixed review comments
- Undid read order since after slicing it's no longer necessary
- Cleaned up method signatures to match necessary data
- Changed ordering and usage of readHeader flag
- Updated test to include data in dummy byte buff | Further clean up and review comments
- Remove lines that further slice the already sliced buffer
- Worked on using cleaner method calls | Fixed review comments for assertions and naming consistency"
netty/netty,13451,https://github.com/netty/netty/pull/13451,Use Two-Way for finding the delimiter in DelimiterBasedFrameDecoder,"Motivation:
We were using the naive and inefficient O(nm) algorithm here.

Modification:
Remove the naive algorithm and instead call out to the optimised Two-Way search we already have in ByteBufUtil.

Result:
The DelimiterBasedFrameDecoder now searches for delimiters using the fastest algorithm available to us.

Fixes #13313",2023-06-16T06:41:20Z,normanmaurer,chrisvest,readable,"Ok I see that is a smart method indeed: 
  
    
      netty/buffer/src/main/java/io/netty/buffer/ByteBufUtil.java
    
    
         Line 252
      in
      22bf43b
    
  
  
    

        
          
           return haystack.indexOf(haystack.readerIndex(), haystack.writerIndex(), 
        
    
  


Meaning that for single bytes delimiters the boost will be very high as well (now idea how much it is common) | I suspect single-byte delimiters are fairly common, e.g. a single newline character. | Nice, so let's wait the user feedback...
What's good about SWAR wasn't the batchy behaviour but that It amortizes mispredictions reducing the branches...I am super curious 🧐 | Just some before I forgot about them...
Given that the delimiter shouldn't change, maybe would be worthy to preprocess the needle as required by the algorithm and change the current two way impl to skip bound checks when possible
In the case of multi bytes delimiters, this should further increase performance for such a frequent op | Nice one @chrisvest | @chrisvest can you also port this to main if needed ? | For what it's worth, downstream we have ended up doing https://git.opendaylight.org/gerrit/c/netconf/+/101892/1/netconf/netconf-netty-util/src/main/java/org/opendaylight/netconf/nettyutil/handler/NetconfEOMAggregator.java
The point is that the base class is not @sharable and therefore has context. While the two-way is a nice optimization, I think remembering previously-checked ranges is more beneficial. Our use case entails relatively small message chunks (couple of KiB) being aggregated to large messages (10s of MiB).
EOM marker is multi-byte in this case. | @rovarga we love PRs... If you have some ideas how to improve things just open a PR :) | @rovarga in your commit you are indeed making use of ByteBuf::indexOf(byte) that has been recently improved, but you don't use the two way one for the isEom method: ByteBuf:;getByte while performed by user code is both performing bound and accessibility checks, while many of the internal search API can skip those.","Use Two-Way for finding the delimiter in DelimiterBasedFrameDecoder

Motivation:
We were using the naive and inefficient O(nm) algorithm here.

Modification:
Remove the naive algorithm and instead call out to the optimised Two-Way search we already have in ByteBufUtil.

Result:
The DelimiterBasedFrameDecoder now searches for delimiters using the fastest algorithm available to us.

Fixes #13313"
netty/netty,12760,https://github.com/netty/netty/pull/12760,Reject HTTP/2 header values with invalid characters,"This PR builds upon #12755.

Motivation:
In https://datatracker.ietf.org/doc/html/rfc7540#section-10.3 it says that only certain characters are valid in a header value:

> Any request or response that contains a character not permitted
> in a header field value MUST be treated as malformed (Section 8.1.2.6).
> Valid characters are defined by the ""field-content"" ABNF rule in
> Section 3.2 of [RFC7230].

Modification:
Add a header value validation step to HpackDecoder.

Result:
Header values are now validated against the Section 10.3, etc. rules.",2022-09-21T16:10:05Z,chrisvest,chrisvest,readable,"@ejona86 PTAL as well | @normanmaurer Comments addressed. | @chrisvest let me know once this is ready for re-review | @normanmaurer This is ready for a second review now. | @normanmaurer @idelpivnitskiy I've consolidated the validation and pushed it down into the headers implementation, except for the validation that HTTP/2 headers are decoded pseudo-headers first, since I don't think we can compatibly place such a temporal restriction on the Http2Headers interface. | @normanmaurer @idelpivnitskiy Addressed your comments. Please take a look. | Ship it...","Validate HTTP/2 header values

Motivation:
In https://datatracker.ietf.org/doc/html/rfc7540#section-10.3 it says that only certain characters are valid in a header value:

> Any request or response that contains a character not permitted
> in a header field value MUST be treated as malformed (Section 8.1.2.6).
> Valid characters are defined by the ""field-content"" ABNF rule in
> Section 3.2 of [RFC7230].

Modification:
Add a header value validation step to HpackDecoder.

Result:
Header values are now validated against the Section 10.3, etc. rules. | Specialize HPACK header validation for AsciiString and CharSequence | Address some comments from #12755 | Fix HTTP/2 header value validation bug caused by signed bytes | Include the offending byte and index when HTTP/2 header value validation fails | Make header value validation a separate toggle | Fix backwards compatibility | Address some small review comments | Specialize validation in HpackDecoder since it only produce AsciiString header names and values | Address some more comments | Make illegal http header char tests parameterized | Consolidate HTTP header value and name validation | Small javadoc update | Address PR review comments | Address nit"
netty/netty,12384,https://github.com/netty/netty/pull/12384,Use Lambdas and Method references,"Motivation:
We can use Lambdas expressions and Method references to make good short and readable.

Modification:
Shifted code to Lambdas expressions and Method references

Result:
Short and readable code",2022-05-12T13:38:40Z,normanmaurer,hyperxpro,readable,@hyperxpro thanks,Use Lambdas | Merge branch 'main' into lambda-5x
netty/netty,12382,https://github.com/netty/netty/pull/12382,Don't use explicit types in diamond operator,"Motivation:
We don't need unnecessary explicit type declarations in diamond operators in Java 8+. Since Netty5 is Java 11 based, we can get rid of them and make the code look clean,

Modification:
Removed explicit types and formatted code as per style.

Result:
Clean and more readable code.
",2022-05-10T22:36:26Z,chrisvest,hyperxpro,readable,Thanks!,Don't use explicit types | Fix checkstyle
netty/netty,12238,https://github.com/netty/netty/pull/12238,Avoid gaps in `ByteToMessageDecoderForBuffer` composite cumulation,"Motivation:
`CompositeBuffers` do not like to have gaps in their readable and writable byte regions.

Modification:
Add several more checks to prevent gaps between both reader-offsets and writer-offsets in the composite cumulator.
Also move some of those checks from `channelRead` into the cumulator, since the merge cumulator do not have the same restrictions.
Some bugs in this area were introduced by the call to `compact()` in `discardSomeReadBytes`, so the implementation of this has been moved to the cumulator.
A test has also been added, that processes enough data to have `discardSomeReadBytes` called, and also stress all implementations.

Result:
Several bugs fixed in `ByteToMessageDecoderForBuffer`.",2022-03-28T08:52:16Z,normanmaurer,chrisvest,readable,,"Avoid gaps in `ByteToMessageDecoderForBuffer` composite cumulation

Motivation:
`CompositeBuffers` do not like to have gaps in their readable and writable byte regions.

Modification:
Add several more checks to prevent gaps between both reader-offsets and writer-offsets in the composite cumulator.
Also move some of those checks from `channelRead` into the cumulator, since the merge cumulator do not have the same restrictions.
Some bugs in this area were introduced by the call to `compact()` in `discardSomeReadBytes`, so the implementation of this has been moved to the cumulator.
A test has also been added, that processes enough data to have `discardSomeReadBytes` called, and also stress all implementations.

Result:
Several bugs fixed in `ByteToMessageDecoderForBuffer`."
netty/netty,12142,https://github.com/netty/netty/pull/12142,Just return early when forEach* method is called and the Buffer is no…,"…t readable / writable

Motivation:

We should just return when there is nothing to read or write and not throw when forEach* methods are called.

Modifications:

- Add early returns
- Add unit tests

Result:

More consistent and less surpising behaviour
",2022-03-03T21:45:30Z,chrisvest,normanmaurer,readable,"@normanmaurer I'm working on porting SslHandler to buffer, and it uses empty buffers a lot when doing handshakes. It turns out it would be convenient for it, if the callbacks always fired, but got empty/zero-capacity buffers. The existing SSLEngines already understands the meaning of such buffers.
What do you think? Would that be reasonable alternative behaviour? | I guess that could be fine as well ... | I had a go at that, but it turns out to be pretty weird behaviour for composite buffers. Let's keep it as is for now, and I'll think of something else for SslHandler.","Just return early when forEach* method is called and the Buffer is not readable / writable

Motivation:

We should just return when there is nothing to read or write and not throw when forEach* methods are called.

Modifications:

- Add early returns
- Add unit tests

Result:

More consistent and less surpising behaviour | Address comments"
netty/netty,12426,https://github.com/netty/netty/pull/12426,Migrate Compression API to new Buffer API,"Motivation:

We introduced a new Buffer API in netty 5 but didnt migrate the Compression
to use it yet

Modifications:

- Port Compression API and implementations to use new Buffer API
- Adjust tests

Result:

Use new Buffer API in compression implementations",2022-06-01T22:43:00Z,chrisvest,normanmaurer,readable,This also includes #12424 which I will remove from this PR once #12424 is merged | Something is wrong with gzip handling ... investigating | @normanmaurer I'm merging this. We can do cleanups based on #12439 in a separate PR.,"Migrate Compression API to new Buffer API

Motivation:

We introduced a new Buffer API in netty 5 but didnt migrate the Compression to use it yet

Modifications:

- Port Compression API and implementations to use new Buffer API
- Adjust tests

Result:

Use new Buffer API in compression implementations | Apply suggestions from code review

Co-authored-by: Chris Vest <christianvest_hansen@apple.com> | Address comments | Fixes | WIP | Fixes | Fixing buffer leaks | Fix todo | Address comments"
netty/netty,12059,https://github.com/netty/netty/pull/12059,Use ObjectUtil#checkInRange instead of if-else to make the code look better,"Motivation:
We should use `ObjectUtil#checkInRange` instead of if-else to make the code look better and easier to understand.

Modification:
Replaced if-else with `ObjectUtil#checkInRange`

Result:
More readable and better code
",2022-01-31T11:12:43Z,normanmaurer,hyperxpro,readable,Thanks a lot!,Use ObjectUtil#checkInRange instead of if-else
netty/netty,11565,https://github.com/netty/netty/pull/11565,Inline variables to make code more readable,"Motivation:
There are lots of redundant variable declarations which should be inlined to make good look better.

Modification:
Made variables inlined.

Result:
Less redundant variable and more readable code.",2021-08-11T15:05:49Z,chrisvest,hyperxpro,readable,,Inline variables | Remove 1 useless line.
netty/netty,11367,https://github.com/netty/netty/pull/11367,Use Two way algorithm to optimize ByteBufUtil.indexOf() method,"Motivation:

ByteBufUtil.indexOf can be inefficient for substring search on
ByteBuf, in terms of algorithm complexity (O(needle.readableBytes * haystack.readableBytes)), consider using the Two Way  algorithm to optimize the ByteBufUtil.indexOf() method

Modification:

Use the Two Way algorithm to optimize ByteBufUtil.indexOf() method.

Result:

The performance of the ByteBufUtil.indexOf() method is higher than the original implementation
",2021-06-28T09:07:17Z,chrisvest,skyguard1,readable,"Thanks for the pr!
The kmp algorithm is already available in Netty by using some specific byte processor and the stealth allocation of this version in the pr is not welcome IMO, given that the version using byte processor allow to cache the jump table (see #9955 for more info).
I would suggest instead, to use the new SWAR index of (byte) to speed up this index of while leaving the allocation behaviour as it is. | @franz1981, Thanks for your suggestion,I have seen #9955 and its implementation, but I think it is still necessary to implement it on ByteBufUtil.indexOf(). Here are the reasons:

When using KmpSearchProcessorFactory, you need to create KmpSearchProcessorFactory through AbstractSearchProcessorFactory.newKmpSearchProcessorFactory() first, and users need to call the ByteBuf.getBytes() method to pass in a byte array, which is a bit complicated for users
The user needs to understand each string matching algorithm and implementation in order to create the corresponding SearchProcessor, but this is not a good choice for the user (just a personal opinion), and use ByteBufUtil.indexOf()  for comparison is much simpler
Caching the next table can improve performance. However, in actual use, the probability of multiple comparisons of the same needle is not certain (just a personal opinion). In fact, the number of times the user actually compares the same byteBuf multiple times may not as much as we think
These are just my personal opinions, can you explain the optimization points further, thanks | Reading your comments I see that the existing searching API is not simple enough for users and I would rather suggest to improve it: indexOf method should obey the https://en.m.wikipedia.org/wiki/Principle_of_least_astonishment and, although is personal, I don't think allocation (O(n)) while searching to be the expected behaviour here, but as said, maybe it's just me and other users are perfectly fine with it.
Regarding the optimization of the index of byte (SWAR), it's not performing a linear search byte by byte hence could be used to start the needle first byte search or just to fail fast. | Using firstIndexOf (SWAR) might fall flat if the first byte turns out to be very common in the inputs.
I don't think it's a huge problem that indexOf allocates. At least, I think most usages of it are limited in how many times they call it. On the other hand, we also have people with very high throughputs, who wish to keep allocations to a minimum because of the non-local effects on response time and stability.
The Two-Way string matching algorithm has similar performance characteristics to KMP and BM, but does not require any table allocation. Perhaps that will be a better choice for our indexOf method?
https://en.wikipedia.org/wiki/Two-way_string-matching_algorithm
http://www-igm.univ-mlv.fr/~lecroq/string/node26.html
http://www-igm.univ-mlv.fr/~mac/Articles-PDF/CP-1991-jacm.pdf | @chrisvest as one of such performance critical allocation control freak users I can say that's a sensible answer :)
Thanks for sharing https://en.wikipedia.org/wiki/Two-way_string-matching_algorithm I didn't know about it, it's even used in glibc! | Happy about it well done!
Just few things I am curious about:

what happened if the needle is larger then the haystack? Or better...when the remaining needle to match is larger then the available haystack?
why the predictable case is slower then the unpredictable one?

For this last question I am going to review the results and the bench: please print the results with turbo boost disabled, fixed proc frequency and using ""perfnorm"" profiler | @franz1981, When needle.readableBytes()> haystack.readableBytes(), it will directly return -1, thanks for the suggestion | And what about
""when the remaining needle to match is larger then the available haystack?""
This can happen if the first N matches on haystack are negative and there are just M < needle::readableBytes left. It should fail fast but I am not sure the original algorithm account for it | According to my understanding of this algorithm, there is no optimization operation that supports this situation | Looks like Two Way is doing even better than KMP? That's awesome! | @skyguard1

According to my understanding of this algorithm, there is no optimization operation that supports this situation

Yep, I know, but failing fast although won't impact big O analysis still impact real use cases, especially with ""large"" needles: I suggest to try modify the algorithm to do it. I've done something similar for the kmp variation we use on https://github.com/franz1981/activemq-artemis/blob/388809a583267de4e0599a4e1e2c76317029eb45/artemis-commons/src/main/java/org/apache/activemq/artemis/utils/algo/KMPNeedle.java#L78 and it proved to be effective in many real use cases, despite the additional check.
As said, not a must-have in term of correctness but still a welcome behavior IMO :)
@chrisvest

Looks like Two Way is doing even better than KMP? That's awesome!

That's why I've suggested to look at the perfnorm data: just to look at the branch mispredict and cache friendliness of both (thanks to the reduced allocation and that we just look at 2 different locations, although in different parts, the cache is better used): I believe that if we stress the bench with the right data the perf should confirm the known algorithm complexities. | @skyguard1 I expect -perf perfnorm to produce many numbers for each benchmarks related cache misses, cpu cycles, mispredictions...where it is?
output of perfnorm should look like
AFUBench.plain:·CPI                       avgt    5   0.318 ±  0.012   #/op
AFUBench.plain:·L1-dcache-load-misses     avgt    5  ≈ 10⁻³            #/op
AFUBench.plain:·L1-dcache-loads           avgt    5  17.368 ±  3.469   #/op
AFUBench.plain:·L1-dcache-store-misses    avgt    5  ≈ 10⁻⁴            #/op
AFUBench.plain:·L1-dcache-stores          avgt    5   4.345 ±  0.874   #/op
AFUBench.plain:·branch-misses             avgt    5  ≈ 10⁻⁴            #/op
AFUBench.plain:·branches                  avgt    5   5.775 ±  1.114   #/op
AFUBench.plain:·cycles                    avgt    5  11.472 ±  2.525   #/op
AFUBench.plain:·instructions              avgt    5  36.073 ±  6.926   #/op

A couple of suggestions re the previous benchmark numbers..
never banchmark on busy machines (eg laptop) unless you can:

disable turbo boost/reliably disable any energy saving feature/have fixed frequency
isolate cores/cpus from handling desktop activities (and not only)

And, be prepared that server class machines behave very differently from desktop: numbers are not comparable and often not reliable enough to conclude anything. Use Linux unless MacOS is a target for a specific optimization and just not use Window, unless forced to. | Using firstIndexOf (SWAR) might fall flat if the first byte turns out to be very common in the inputs.

In my experience this is correct - e.g. these are the average search times for text drawn from different distributions (markov generated English and German with transition probabilities inferred from bibles in each language, uniformly random text) for a search implementation which uses SWAR to find the start of the needle before commencing a search using the shift-or algorithm. The distribution of the data has a much larger effect than even the length of the needle (termlength). | Sorry, I re-run the benchmark, as can be seen in the result, the performance of the two way is indeed improved, but I only have the linux virtual machine, and linux perf does not support the detection of cpu cache and other information | @skyguard1 numbers look pretty weird...but it's a bare metal machine or is a virtual env eg AWS? | It runs on our own cloud platform. I also have a virtual machine on physical machine, but perf does not support reading cpu cache information, sorry but I currently don't have a physical machine's linux | @skyguard1 in this case better to not trust any of these numbers...
The change is ok per se, considering that it's better then the original implementation so I am already happy with that; I just want to check how it behave on real machines.
I will try to run it on some physical hw by the next week, before going on vacation. | @skyguard1 One more thing I just noticed: the indexOf is meant to operate on the readable bytes, but it doesn't take the readerIndex into account. There are also no tests with non-zero readerIndex. | @skyguard1 One more thing I just noticed: the indexOf is meant to operate on the readable bytes, but it doesn't take the readerIndex into account. There are also no tests with non-zero readerIndex.

The case where the readerIndex of needle and haystack is not 0 has been considered | @normanmaurer PTAL, please see if there is anything that needs to be modified here, thanks | A small thing: The two ints returned from maxSuf could be returned as one packed long value, to guarantee no allocation takes place. | @chrisvest,I've done it, thanks | @chrisvest, thanks | @normanmaurer, thanks | @trustin, thanks | Once the build has passed I will merge this one | @skyguard1 Thanks! | @chrisvest It is my pleasure | @skyguard1 I think there's a bug in the new implementation. As far as I understand, it returns index relative to reader index instead of absolute index, like before. I see that @chrisvest actually suggested that it might be an issue.","Use Kmp algorithm to optimize ByteBufUtil.indexOf() method and add test case

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | Use the two way algorithm to optimize the ByteBufUtil.indexOf() method

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | Add return when needle.readableBytes()> haystack.readableBytes()

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | return -1 when the remaining needle to match is larger then the available haystack

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | Update by review

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | Add test case

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | Use an array to return the offset value of the maxSuf() method

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | Fix wrong comparison length

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | Use haystack's readerIndex to compare the position

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | Use long instead of array to reduce space allocation

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | Update buffer/src/main/java/io/netty/buffer/ByteBufUtil.java

Co-authored-by: Chris Vest <mr.chrisvest@gmail.com> | Update buffer/src/test/java/io/netty/buffer/ByteBufUtilTest.java

release unused bytebuf

Co-authored-by: Norman Maurer <norman_maurer@apple.com> | Update buffer/src/main/java/io/netty/buffer/ByteBufUtil.java"
netty/netty,11574,https://github.com/netty/netty/pull/11574,Remove the deprecated ThreadDeathWatcher,"Motivation:
The deprecated ThreadDeathWatcher produces more garbage and can delay resource release, when compared to manual resource management.

Modification:
Remove the ThreadDeathWatcher and other deprecated APIs that rely on it.

Result:
Less deprecated code.
",2021-08-16T12:33:58Z,normanmaurer,chrisvest,readable,"Looks like a crash in the Graal compiler. Probably a compiler bug?
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007ff404a0d73b, pid=43261, tid=43274
#
# JRE version: OpenJDK Runtime Environment GraalVM CE 20.1.0 (11.0.7+10) (build 11.0.7+10-jvmci-20.1-b02)
# Java VM: OpenJDK 64-Bit Server VM GraalVM CE 20.1.0 (11.0.7+10-jvmci-20.1-b02, mixed mode, sharing, tiered, jvmci, jvmci compiler, compressed oops, g1 gc, linux-amd64)
# Problematic frame:
# J 215 c1 java.util.ArrayList.add(Ljava/lang/Object;)Z java.base@11.0.7 (25 bytes) @ 0x00007ff404a0d73b [0x00007ff404a0d500+0x000000000000023b]
#
# Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /code/resolver-dns-native-macos/core.43261)
#
# If you would like to submit a bug report, please visit:
#   https://github.com/oracle/graal/issues
#

---------------  S U M M A R Y ------------

Command Line: -XX:+UnlockExperimentalVMOptions -XX:+EnableJVMCIProduct -XX:-UnlockExperimentalVMOptions -XX:ThreadPriorityPolicy=1 -dsa -da -ea:io.netty... -XX:+HeapDumpOnOutOfMemoryError -Xlog:gc -Dio.netty.leakDetectionLevel=paranoid -Dio.netty.leakDetection.targetRecords=32 -D_ -D_ --illegal-access=deny -D_ /code/resolver-dns-native-macos/target/surefire/surefirebooter1411533031050063994.jar /code/resolver-dns-native-macos/target/surefire 2021-08-11T14-04-50_498-jvmRun1 surefire7069040594597368954tmp surefire_236978304828881009263tmp

Host: Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz, 2 cores, 6G, CentOS release 6.10 (Final)
Time: Wed Aug 11 14:31:06 2021 UTC elapsed time: 0 seconds (0d 0h 0m 0s)

---------------  T H R E A D  ---------------

Current thread (0x00007ff41c176800):  JavaThread ""JVMCI-native CompilerThread0"" daemon [_thread_in_Java, id=43274, stack(0x00007ff3fc0cd000,0x00007ff3fc2ce000)]


Current CompileTask:
JVMCI-native:    191  158       4       java.lang.String::charAt (25 bytes)

Stack: [0x00007ff3fc0cd000,0x00007ff3fc2ce000],  sp=0x00007ff3fc2cbd30,  free space=2043k
Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code)
J 215 c1 java.util.ArrayList.add(Ljava/lang/Object;)Z java.base@11.0.7 (25 bytes) @ 0x00007ff404a0d73b [0x00007ff404a0d500+0x000000000000023b]
j  org.graalvm.compiler.hotspot.management.SVMMBean$Factory.signal(J)V+8 jdk.internal.vm.compiler.management
j  org.graalvm.compiler.hotspot.management.SVMToHotSpotEntryPoints.signal(Lorg/graalvm/compiler/hotspot/management/SVMMBean$Factory;J)V+2 jdk.internal.vm.compiler.management
v  ~StubRoutines::call_stub
V  [libjvm.so+0x87fd39]  JavaCalls::call_helper(JavaValue*, methodHandle const&, JavaCallArguments*, Thread*)+0x3b9
V  [libjvm.so+0x9062d2]  jni_invoke_static(JNIEnv_*, JavaValue*, _jobject*, JNICallType, _jmethodID*, JNI_ArgumentPusher*, Thread*) [clone .isra.65] [clone .constprop.219]+0x212
V  [libjvm.so+0x9074bb]  jni_CallStaticVoidMethodA+0xcb


siginfo: si_signo: 11 (SIGSEGV), si_code: 1 (SEGV_MAPERR), si_addr: 0x000000e2ff410000

Register to memory mapping:

RAX=0x0000000000000001 is an unknown value
RBX=0x00007ff3fc2cbd30 is pointing into the stack for thread: 0x00007ff41c176800
RCX=0x0000000000000002 is an unknown value
RDX=0x0000000000000210 is an unknown value
RSP=0x00007ff3fc2cbd30 is pointing into the stack for thread: 0x00007ff41c176800
RBP=0x00007ff3fc2cbe98 is pointing into the stack for thread: 0x00007ff41c176800
RSI=0x000000e2ff410000 is an unknown value
RDI=0x000000009a115da0 points into unknown readable memory: 02 00 05 00 00 00 00 00
R8 =0x00007ff3ef905998 is pointing into metadata
R9 =0x0000000000000004 is an unknown value
R10=0x00007ff4253672a0: <offset 0x00000000014b82a0> in /jdk/lib/server/libjvm.so at 0x00007ff423eaf000
R11=0x00007ff404a0d540 is at entry_point+64 in (nmethod*)0x00007ff404a0d310
R12=0x0 is NULL
R13=0x00007ff404a0d6c4 is at entry_point+452 in (nmethod*)0x00007ff404a0d310
R14=0x00007ff3fc2cbd20 is pointing into the stack for thread: 0x00007ff41c176800
R15=0x00007ff41c176800 is a thread


Registers:
RAX=0x0000000000000001, RBX=0x00007ff3fc2cbd30, RCX=0x0000000000000002, RDX=0x0000000000000210
RSP=0x00007ff3fc2cbd30, RBP=0x00007ff3fc2cbe98, RSI=0x000000e2ff410000, RDI=0x000000009a115da0
R8 =0x00007ff3ef905998, R9 =0x0000000000000004, R10=0x00007ff4253672a0, R11=0x00007ff404a0d540
R12=0x0000000000000000, R13=0x00007ff404a0d6c4, R14=0x00007ff3fc2cbd20, R15=0x00007ff41c176800
RIP=0x00007ff404a0d73b, EFLAGS=0x0000000000010246, CSGSFS=0x002b000000000033, ERR=0x0000000000000004
  TRAPNO=0x000000000000000e

Top of Stack: (sp=0x00007ff3fc2cbd30)
0x00007ff3fc2cbd30:   00007ff3fc2cbe30 00007ff404450e12
0x00007ff3fc2cbd40:   00007ff40443b490 00007ff400000000
0x00007ff3fc2cbd50:   00007ff3fc2cbe98 00007ff3fc2cbe30
0x00007ff3fc2cbd60:   0000000800064e48 00000008002be4e0 

Instructions: (pc=0x00007ff404a0d73b)
0x00007ff404a0d63b:   81 e1 f8 ff 7f 00 83 f9 00 0f 84 80 04 00 00 48
0x00007ff404a0d64b:   8b c6 48 b9 30 57 90 ef f3 7f 00 00 48 83 81 48
0x00007ff404a0d65b:   01 00 00 01 48 8b c3 ff c0 48 b9 98 59 90 ef f3
0x00007ff404a0d66b:   7f 00 00 44 8b 81 04 01 00 00 41 83 c0 08 44 89
0x00007ff404a0d67b:   81 04 01 00 00 41 81 e0 f8 ff 7f 00 41 83 f8 00
0x00007ff404a0d68b:   0f 84 5a 04 00 00 48 8b ce 49 b8 98 59 90 ef f3
0x00007ff404a0d69b:   7f 00 00 49 83 80 48 01 00 00 01 48 8b d0 48 8b
0x00007ff404a0d6ab:   c6 48 8b f0 89 9c 24 c0 00 00 00 48 89 84 24 b8
0x00007ff404a0d6bb:   00 00 00 90 e8 41 05 00 00 48 be 98 59 90 ef f3
0x00007ff404a0d6cb:   7f 00 00 48 83 86 e0 01 00 00 01 48 be 48 5f 90
0x00007ff404a0d6db:   ef f3 7f 00 00 8b 96 04 01 00 00 83 c2 08 89 96
0x00007ff404a0d6eb:   04 01 00 00 81 e2 f8 ff 7f 00 83 fa 00 0f 84 0e
0x00007ff404a0d6fb:   04 00 00 48 8b bc 24 b0 00 00 00 48 be 48 5f 90
0x00007ff404a0d70b:   ef f3 7f 00 00 48 83 86 48 01 00 00 01 48 8b bc
0x00007ff404a0d71b:   24 b0 00 00 00 8b 77 08 48 c1 e6 03 49 bc 00 00
0x00007ff404a0d72b:   00 00 08 00 00 00 49 03 f4 4d 33 e4 48 8b 76 70
0x00007ff404a0d73b:   48 8b 0e 48 be 48 5f 90 ef f3 7f 00 00 48 83 86
0x00007ff404a0d74b:   e0 01 00 00 01 48 8b f7 48 8b d0 90 e8 b9 04 00
0x00007ff404a0d75b:   00 41 0f be 77 30 83 fe 00 48 8b b4 24 b8 00 00
0x00007ff404a0d76b:   00 0f 85 c0 03 00 00 4c 8b d0 44 89 56 14 48 8b
0x00007ff404a0d77b:   fe 48 33 f8 48 c1 ef 14 48 83 ff 00 0f 85 c0 03
0x00007ff404a0d78b:   00 00 48 8b f8 8b 9c 24 c0 00 00 00 48 8b 94 24
0x00007ff404a0d79b:   c8 00 00 00 3b 5f 0c 0f 83 c7 03 00 00 48 83 fa
0x00007ff404a0d7ab:   00 75 16 48 b9 40 54 90 ef f3 7f 00 00 80 89 f9
0x00007ff404a0d7bb:   01 00 00 01 e9 72 02 00 00 8b 47 08 48 c1 e0 03
0x00007ff404a0d7cb:   49 bc 00 00 00 00 08 00 00 00 49 03 c4 4d 33 e4
0x00007ff404a0d7db:   8b 4a 08 48 c1 e1 03 49 bc 00 00 00 00 08 00 00
0x00007ff404a0d7eb:   00 49 03 cc 4d 33 e4 48 8b 80 e8 00 00 00 48 3b
0x00007ff404a0d7fb:   c8 0f 84 2a 00 00 00 44 8b 40 10 4a 3b 04 01 0f
0x00007ff404a0d80b:   84 1c 00 00 00 41 83 f8 20 0f 85 05 02 00 00 51
0x00007ff404a0d81b:   50 e8 5f 97 b1 ff 59 58 83 f8 00 0f 84 f3 01 00
0x00007ff404a0d82b:   00 48 b9 40 54 90 ef f3 7f 00 00 8b 42 08 48 c1 


Stack slot to memory mapping:
stack at sp + 0 slots: 0x00007ff3fc2cbe30 is pointing into the stack for thread: 0x00007ff41c176800
stack at sp + 1 slots: 0x00007ff404450e12 is at code_begin+114 in an Interpreter codelet
invokeinterface  185 invokeinterface  [0x00007ff404450da0, 0x00007ff404451da0]  4096 bytes
stack at sp + 2 slots: 0x00007ff40443b490 is at code_begin+-112 in 
[CodeBlob (0x00007ff40443b490)]
Framesize: 0
BufferBlob (0x00007ff40443b490) used for Interpreter
stack at sp + 3 slots: 0x00007ff400000000 points into unknown readable memory: 20 00 00 00 f4 7f 00 00
stack at sp + 4 slots: 0x00007ff3fc2cbe98 is pointing into the stack for thread: 0x00007ff41c176800
stack at sp + 5 slots: 0x00007ff3fc2cbe30 is pointing into the stack for thread: 0x00007ff41c176800
stack at sp + 6 slots: 0x0000000800064e48 is pointing into metadata
stack at sp + 7 slots: 0x00000008002be4e0 is pointing into metadata



---------------  P R O C E S S  ---------------

Threads class SMR info:
_java_thread_list=0x00007ff3c81075b0, length=12, elements={
0x00007ff41c027000, 0x00007ff41c16c800, 0x00007ff41c16e800, 0x00007ff41c174000,
0x00007ff41c176800, 0x00007ff41c178800, 0x00007ff41c17a800, 0x00007ff41c187000,
0x00007ff41c1bd000, 0x00007ff41c232800, 0x00007ff41c252000, 0x00007ff3c8106000
}

Java Threads: ( => current thread )
  0x00007ff41c027000 JavaThread ""main"" [_thread_blocked, id=43264, stack(0x00007ff42606e000,0x00007ff42616f000)]
  0x00007ff41c16c800 JavaThread ""Reference Handler"" daemon [_thread_blocked, id=43271, stack(0x00007ff4040af000,0x00007ff4041b0000)]
  0x00007ff41c16e800 JavaThread ""Finalizer"" daemon [_thread_blocked, id=43272, stack(0x00007ff3fc3cf000,0x00007ff3fc4d0000)]
  0x00007ff41c174000 JavaThread ""Signal Dispatcher"" daemon [_thread_blocked, id=43273, stack(0x00007ff3fc2ce000,0x00007ff3fc3cf000)]
=>0x00007ff41c176800 JavaThread ""JVMCI-native CompilerThread0"" daemon [_thread_in_Java, id=43274, stack(0x00007ff3fc0cd000,0x00007ff3fc2ce000)]
  0x00007ff41c178800 JavaThread ""C1 CompilerThread0"" daemon [_thread_blocked, id=43275, stack(0x00007ff3ef5ff000,0x00007ff3ef800000)]
  0x00007ff41c17a800 JavaThread ""Sweeper thread"" daemon [_thread_blocked, id=43276, stack(0x00007ff3ef4fe000,0x00007ff3ef5ff000)]
  0x00007ff41c187000 JavaThread ""Common-Cleaner"" daemon [_thread_blocked, id=43277, stack(0x00007ff3ef3fd000,0x00007ff3ef4fe000)]
  0x00007ff41c1bd000 JavaThread ""Service Thread"" daemon [_thread_blocked, id=43278, stack(0x00007ff3ef2fc000,0x00007ff3ef3fd000)]
  0x00007ff41c232800 JavaThread ""surefire-forkedjvm-command-thread"" daemon [_thread_in_native, id=43280, stack(0x00007ff3ecdff000,0x00007ff3ecf00000)]
  0x00007ff41c252000 JavaThread ""surefire-forkedjvm-ping-30s"" daemon [_thread_blocked, id=43281, stack(0x00007ff3ec8f3000,0x00007ff3ec9f4000)]
  0x00007ff3c8106000 JavaThread ""Libgraal MBean Registration"" daemon [_thread_blocked, id=43283, stack(0x00007ff3ec7f2000,0x00007ff3ec8f3000)]

Other Threads:
  0x00007ff41c169800 VMThread ""VM Thread"" [stack: 0x00007ff4041b1000,0x00007ff4042b1000] [id=43270] _threads_hazard_ptr=0x00007ff3c81075b0
  0x00007ff41c1bf800 WatcherThread [stack: 0x00007ff3ef1fc000,0x00007ff3ef2fc000] [id=43279]
  0x00007ff41c03e800 GCTaskThread ""GC Thread#0"" [stack: 0x00007ff422cdf000,0x00007ff422ddf000] [id=43265]
  0x00007ff3e4001000 GCTaskThread ""GC Thread#1"" [stack: 0x00007ff3ec6f2000,0x00007ff3ec7f2000] [id=43284]
  0x00007ff41c06a800 ConcurrentGCThread ""G1 Main Marker"" [stack: 0x00007ff42047a000,0x00007ff42057a000] [id=43266]
  0x00007ff41c06c000 ConcurrentGCThread ""G1 Conc#0"" [stack: 0x00007ff420379000,0x00007ff420479000] [id=43267]
  0x00007ff41c0d0000 ConcurrentGCThread ""G1 Refine#0"" [stack: 0x00007ff420076000,0x00007ff420176000] [id=43268]
  0x00007ff41c0d2000 ConcurrentGCThread ""G1 Young RemSet Sampling"" [stack: 0x00007ff404333000,0x00007ff404433000] [id=43269]

Threads with active compile tasks:
JVMCI-native CompilerThread0    197  158       4       java.lang.String::charAt (25 bytes)

VM state:synchronizing (normal execution)

VM Mutex/Monitor currently owned by a thread:  ([mutex/lock_event])
[0x00007ff41c023cf0] Safepoint_lock - owner thread: 0x00007ff41c169800
[0x00007ff41c023d80] Threads_lock - owner thread: 0x00007ff41c169800

Heap address: 0x0000000093400000, size: 1740 MB, Compressed Oops mode: 32-bit
Narrow klass base: 0x0000000800000000, Narrow klass shift: 3
Compressed class space size: 1073741824 Address: 0x0000000840000000

Heap:
 garbage-first heap   total 114688K, used 3796K [0x0000000093400000, 0x0000000100000000)
  region size 1024K, 2 young (2048K), 1 survivors (1024K)
 Metaspace       used 2943K, capacity 5251K, committed 5504K, reserved 1056768K
  class space    used 335K, capacity 544K, committed 640K, reserved 1048576K
Heap Regions: E=young(eden), S=young(survivor), O=old, HS=humongous(starts), HC=humongous(continues), CS=collection set, F=free, A=archive, TAMS=top-at-mark-start (previous, next)
|   0|0x0000000093400000, 0x0000000093500000, 0x0000000093500000|100%| O|  |TAMS 0x0000000093400000, 0x0000000093400000| Untracked 
|   1|0x0000000093500000, 0x00000000935fd200, 0x0000000093600000| 98%| O|  |TAMS 0x0000000093500000, 0x0000000093500000| Untracked 
|   2|0x0000000093600000, 0x0000000093600000, 0x0000000093700000|  0%| F|  |TAMS 0x0000000093600000, 0x0000000093600000| Untracked 
|   3|0x0000000093700000, 0x0000000093700000, 0x0000000093800000|  0%| F|  |TAMS 0x0000000093700000, 0x0000000093700000| Untracked 
|   4|0x0000000093800000, 0x0000000093800000, 0x0000000093900000|  0%| F|  |TAMS 0x0000000093800000, 0x0000000093800000| Untracked 
|   5|0x0000000093900000, 0x0000000093900000, 0x0000000093a00000|  0%| F|  |TAMS 0x0000000093900000, 0x0000000093900000| Untracked 
|   6|0x0000000093a00000, 0x0000000093a00000, 0x0000000093b00000|  0%| F|  |TAMS 0x0000000093a00000, 0x0000000093a00000| Untracked 
|   7|0x0000000093b00000, 0x0000000093b00000, 0x0000000093c00000|  0%| F|  |TAMS 0x0000000093b00000, 0x0000000093b00000| Untracked 
|   8|0x0000000093c00000, 0x0000000093c00000, 0x0000000093d00000|  0%| F|  |TAMS 0x0000000093c00000, 0x0000000093c00000| Untracked 
|   9|0x0000000093d00000, 0x0000000093d00000, 0x0000000093e00000|  0%| F|  |TAMS 0x0000000093d00000, 0x0000000093d00000| Untracked 
|  10|0x0000000093e00000, 0x0000000093e00000, 0x0000000093f00000|  0%| F|  |TAMS 0x0000000093e00000, 0x0000000093e00000| Untracked 
|  11|0x0000000093f00000, 0x0000000093f00000, 0x0000000094000000|  0%| F|  |TAMS 0x0000000093f00000, 0x0000000093f00000| Untracked 
|  12|0x0000000094000000, 0x0000000094000000, 0x0000000094100000|  0%| F|  |TAMS 0x0000000094000000, 0x0000000094000000| Untracked 
|  13|0x0000000094100000, 0x0000000094100000, 0x0000000094200000|  0%| F|  |TAMS 0x0000000094100000, 0x0000000094100000| Untracked 
|  14|0x0000000094200000, 0x0000000094200000, 0x0000000094300000|  0%| F|  |TAMS 0x0000000094200000, 0x0000000094200000| Untracked 
|  15|0x0000000094300000, 0x0000000094300000, 0x0000000094400000|  0%| F|  |TAMS 0x0000000094300000, 0x0000000094300000| Untracked 
|  16|0x0000000094400000, 0x0000000094400000, 0x0000000094500000|  0%| F|  |TAMS 0x0000000094400000, 0x0000000094400000| Untracked 
|  17|0x0000000094500000, 0x0000000094500000, 0x0000000094600000|  0%| F|  |TAMS 0x0000000094500000, 0x0000000094500000| Untracked 
|  18|0x0000000094600000, 0x0000000094600000, 0x0000000094700000|  0%| F|  |TAMS 0x0000000094600000, 0x0000000094600000| Untracked 
|  19|0x0000000094700000, 0x0000000094700000, 0x0000000094800000|  0%| F|  |TAMS 0x0000000094700000, 0x0000000094700000| Untracked 
|  20|0x0000000094800000, 0x0000000094800000, 0x0000000094900000|  0%| F|  |TAMS 0x0000000094800000, 0x0000000094800000| Untracked 
|  21|0x0000000094900000, 0x0000000094900000, 0x0000000094a00000|  0%| F|  |TAMS 0x0000000094900000, 0x0000000094900000| Untracked 
|  22|0x0000000094a00000, 0x0000000094a00000, 0x0000000094b00000|  0%| F|  |TAMS 0x0000000094a00000, 0x0000000094a00000| Untracked 
|  23|0x0000000094b00000, 0x0000000094b00000, 0x0000000094c00000|  0%| F|  |TAMS 0x0000000094b00000, 0x0000000094b00000| Untracked 
|  24|0x0000000094c00000, 0x0000000094c00000, 0x0000000094d00000|  0%| F|  |TAMS 0x0000000094c00000, 0x0000000094c00000| Untracked 
|  25|0x0000000094d00000, 0x0000000094d00000, 0x0000000094e00000|  0%| F|  |TAMS 0x0000000094d00000, 0x0000000094d00000| Untracked 
|  26|0x0000000094e00000, 0x0000000094e00000, 0x0000000094f00000|  0%| F|  |TAMS 0x0000000094e00000, 0x0000000094e00000| Untracked 
|  27|0x0000000094f00000, 0x0000000094f00000, 0x0000000095000000|  0%| F|  |TAMS 0x0000000094f00000, 0x0000000094f00000| Untracked 
|  28|0x0000000095000000, 0x0000000095000000, 0x0000000095100000|  0%| F|  |TAMS 0x0000000095000000, 0x0000000095000000| Untracked 
|  29|0x0000000095100000, 0x0000000095100000, 0x0000000095200000|  0%| F|  |TAMS 0x0000000095100000, 0x0000000095100000| Untracked 
|  30|0x0000000095200000, 0x0000000095200000, 0x0000000095300000|  0%| F|  |TAMS 0x0000000095200000, 0x0000000095200000| Untracked 
|  31|0x0000000095300000, 0x0000000095300000, 0x0000000095400000|  0%| F|  |TAMS 0x0000000095300000, 0x0000000095300000| Untracked 
|  32|0x0000000095400000, 0x0000000095400000, 0x0000000095500000|  0%| F|  |TAMS 0x0000000095400000, 0x0000000095400000| Untracked 
|  33|0x0000000095500000, 0x0000000095500000, 0x0000000095600000|  0%| F|  |TAMS 0x0000000095500000, 0x0000000095500000| Untracked 
|  34|0x0000000095600000, 0x0000000095600000, 0x0000000095700000|  0%| F|  |TAMS 0x0000000095600000, 0x0000000095600000| Untracked 
|  35|0x0000000095700000, 0x0000000095700000, 0x0000000095800000|  0%| F|  |TAMS 0x0000000095700000, 0x0000000095700000| Untracked 
|  36|0x0000000095800000, 0x0000000095800000, 0x0000000095900000|  0%| F|  |TAMS 0x0000000095800000, 0x0000000095800000| Untracked 
|  37|0x0000000095900000, 0x0000000095900000, 0x0000000095a00000|  0%| F|  |TAMS 0x0000000095900000, 0x0000000095900000| Untracked 
|  38|0x0000000095a00000, 0x0000000095a00000, 0x0000000095b00000|  0%| F|  |TAMS 0x0000000095a00000, 0x0000000095a00000| Untracked 
|  39|0x0000000095b00000, 0x0000000095b00000, 0x0000000095c00000|  0%| F|  |TAMS 0x0000000095b00000, 0x0000000095b00000| Untracked 
|  40|0x0000000095c00000, 0x0000000095c00000, 0x0000000095d00000|  0%| F|  |TAMS 0x0000000095c00000, 0x0000000095c00000| Untracked 
|  41|0x0000000095d00000, 0x0000000095d00000, 0x0000000095e00000|  0%| F|  |TAMS 0x0000000095d00000, 0x0000000095d00000| Untracked 
|  42|0x0000000095e00000, 0x0000000095e00000, 0x0000000095f00000|  0%| F|  |TAMS 0x0000000095e00000, 0x0000000095e00000| Untracked 
|  43|0x0000000095f00000, 0x0000000095f00000, 0x0000000096000000|  0%| F|  |TAMS 0x0000000095f00000, 0x0000000095f00000| Untracked 
|  44|0x0000000096000000, 0x0000000096000000, 0x0000000096100000|  0%| F|  |TAMS 0x0000000096000000, 0x0000000096000000| Untracked 
|  45|0x0000000096100000, 0x0000000096100000, 0x0000000096200000|  0%| F|  |TAMS 0x0000000096100000, 0x0000000096100000| Untracked 
|  46|0x0000000096200000, 0x0000000096200000, 0x0000000096300000|  0%| F|  |TAMS 0x0000000096200000, 0x0000000096200000| Untracked 
|  47|0x0000000096300000, 0x0000000096300000, 0x0000000096400000|  0%| F|  |TAMS 0x0000000096300000, 0x0000000096300000| Untracked 
|  48|0x0000000096400000, 0x0000000096400000, 0x0000000096500000|  0%| F|  |TAMS 0x0000000096400000, 0x0000000096400000| Untracked 
|  49|0x0000000096500000, 0x0000000096500000, 0x0000000096600000|  0%| F|  |TAMS 0x0000000096500000, 0x0000000096500000| Untracked 
|  50|0x0000000096600000, 0x0000000096600000, 0x0000000096700000|  0%| F|  |TAMS 0x0000000096600000, 0x0000000096600000| Untracked 
|  51|0x0000000096700000, 0x0000000096700000, 0x0000000096800000|  0%| F|  |TAMS 0x0000000096700000, 0x0000000096700000| Untracked 
|  52|0x0000000096800000, 0x0000000096800000, 0x0000000096900000|  0%| F|  |TAMS 0x0000000096800000, 0x0000000096800000| Untracked 
|  53|0x0000000096900000, 0x0000000096900000, 0x0000000096a00000|  0%| F|  |TAMS 0x0000000096900000, 0x0000000096900000| Untracked 
|  54|0x0000000096a00000, 0x0000000096a00000, 0x0000000096b00000|  0%| F|  |TAMS 0x0000000096a00000, 0x0000000096a00000| Untracked 
|  55|0x0000000096b00000, 0x0000000096b00000, 0x0000000096c00000|  0%| F|  |TAMS 0x0000000096b00000, 0x0000000096b00000| Untracked 
|  56|0x0000000096c00000, 0x0000000096c00000, 0x0000000096d00000|  0%| F|  |TAMS 0x0000000096c00000, 0x0000000096c00000| Untracked 
|  57|0x0000000096d00000, 0x0000000096d00000, 0x0000000096e00000|  0%| F|  |TAMS 0x0000000096d00000, 0x0000000096d00000| Untracked 
|  58|0x0000000096e00000, 0x0000000096e00000, 0x0000000096f00000|  0%| F|  |TAMS 0x0000000096e00000, 0x0000000096e00000| Untracked 
|  59|0x0000000096f00000, 0x0000000096f00000, 0x0000000097000000|  0%| F|  |TAMS 0x0000000096f00000, 0x0000000096f00000| Untracked 
|  60|0x0000000097000000, 0x0000000097000000, 0x0000000097100000|  0%| F|  |TAMS 0x0000000097000000, 0x0000000097000000| Untracked 
|  61|0x0000000097100000, 0x0000000097100000, 0x0000000097200000|  0%| F|  |TAMS 0x0000000097100000, 0x0000000097100000| Untracked 
|  62|0x0000000097200000, 0x0000000097200000, 0x0000000097300000|  0%| F|  |TAMS 0x0000000097200000, 0x0000000097200000| Untracked 
|  63|0x0000000097300000, 0x0000000097300000, 0x0000000097400000|  0%| F|  |TAMS 0x0000000097300000, 0x0000000097300000| Untracked 
|  64|0x0000000097400000, 0x0000000097400000, 0x0000000097500000|  0%| F|  |TAMS 0x0000000097400000, 0x0000000097400000| Untracked 
|  65|0x0000000097500000, 0x0000000097500000, 0x0000000097600000|  0%| F|  |TAMS 0x0000000097500000, 0x0000000097500000| Untracked 
|  66|0x0000000097600000, 0x0000000097600000, 0x0000000097700000|  0%| F|  |TAMS 0x0000000097600000, 0x0000000097600000| Untracked 
|  67|0x0000000097700000, 0x0000000097700000, 0x0000000097800000|  0%| F|  |TAMS 0x0000000097700000, 0x0000000097700000| Untracked 
|  68|0x0000000097800000, 0x0000000097800000, 0x0000000097900000|  0%| F|  |TAMS 0x0000000097800000, 0x0000000097800000| Untracked 
|  69|0x0000000097900000, 0x0000000097900000, 0x0000000097a00000|  0%| F|  |TAMS 0x0000000097900000, 0x0000000097900000| Untracked 
|  70|0x0000000097a00000, 0x0000000097a00000, 0x0000000097b00000|  0%| F|  |TAMS 0x0000000097a00000, 0x0000000097a00000| Untracked 
|  71|0x0000000097b00000, 0x0000000097b00000, 0x0000000097c00000|  0%| F|  |TAMS 0x0000000097b00000, 0x0000000097b00000| Untracked 
|  72|0x0000000097c00000, 0x0000000097c00000, 0x0000000097d00000|  0%| F|  |TAMS 0x0000000097c00000, 0x0000000097c00000| Untracked 
|  73|0x0000000097d00000, 0x0000000097d00000, 0x0000000097e00000|  0%| F|  |TAMS 0x0000000097d00000, 0x0000000097d00000| Untracked 
|  74|0x0000000097e00000, 0x0000000097e00000, 0x0000000097f00000|  0%| F|  |TAMS 0x0000000097e00000, 0x0000000097e00000| Untracked 
|  75|0x0000000097f00000, 0x0000000097f00000, 0x0000000098000000|  0%| F|  |TAMS 0x0000000097f00000, 0x0000000097f00000| Untracked 
|  76|0x0000000098000000, 0x0000000098000000, 0x0000000098100000|  0%| F|  |TAMS 0x0000000098000000, 0x0000000098000000| Untracked 
|  77|0x0000000098100000, 0x0000000098100000, 0x0000000098200000|  0%| F|  |TAMS 0x0000000098100000, 0x0000000098100000| Untracked 
|  78|0x0000000098200000, 0x0000000098200000, 0x0000000098300000|  0%| F|  |TAMS 0x0000000098200000, 0x0000000098200000| Untracked 
|  79|0x0000000098300000, 0x0000000098300000, 0x0000000098400000|  0%| F|  |TAMS 0x0000000098300000, 0x0000000098300000| Untracked 
|  80|0x0000000098400000, 0x0000000098400000, 0x0000000098500000|  0%| F|  |TAMS 0x0000000098400000, 0x0000000098400000| Untracked 
|  81|0x0000000098500000, 0x0000000098500000, 0x0000000098600000|  0%| F|  |TAMS 0x0000000098500000, 0x0000000098500000| Untracked 
|  82|0x0000000098600000, 0x0000000098600000, 0x0000000098700000|  0%| F|  |TAMS 0x0000000098600000, 0x0000000098600000| Untracked 
|  83|0x0000000098700000, 0x0000000098700000, 0x0000000098800000|  0%| F|  |TAMS 0x0000000098700000, 0x0000000098700000| Untracked 
|  84|0x0000000098800000, 0x0000000098800000, 0x0000000098900000|  0%| F|  |TAMS 0x0000000098800000, 0x0000000098800000| Untracked 
|  85|0x0000000098900000, 0x0000000098900000, 0x0000000098a00000|  0%| F|  |TAMS 0x0000000098900000, 0x0000000098900000| Untracked 
|  86|0x0000000098a00000, 0x0000000098a00000, 0x0000000098b00000|  0%| F|  |TAMS 0x0000000098a00000, 0x0000000098a00000| Untracked 
|  87|0x0000000098b00000, 0x0000000098b00000, 0x0000000098c00000|  0%| F|  |TAMS 0x0000000098b00000, 0x0000000098b00000| Untracked 
|  88|0x0000000098c00000, 0x0000000098c00000, 0x0000000098d00000|  0%| F|  |TAMS 0x0000000098c00000, 0x0000000098c00000| Untracked 
|  89|0x0000000098d00000, 0x0000000098d00000, 0x0000000098e00000|  0%| F|  |TAMS 0x0000000098d00000, 0x0000000098d00000| Untracked 
|  90|0x0000000098e00000, 0x0000000098e00000, 0x0000000098f00000|  0%| F|  |TAMS 0x0000000098e00000, 0x0000000098e00000| Untracked 
|  91|0x0000000098f00000, 0x0000000098f00000, 0x0000000099000000|  0%| F|  |TAMS 0x0000000098f00000, 0x0000000098f00000| Untracked 
|  92|0x0000000099000000, 0x0000000099000000, 0x0000000099100000|  0%| F|  |TAMS 0x0000000099000000, 0x0000000099000000| Untracked 
|  93|0x0000000099100000, 0x0000000099100000, 0x0000000099200000|  0%| F|  |TAMS 0x0000000099100000, 0x0000000099100000| Untracked 
|  94|0x0000000099200000, 0x0000000099200000, 0x0000000099300000|  0%| F|  |TAMS 0x0000000099200000, 0x0000000099200000| Untracked 
|  95|0x0000000099300000, 0x0000000099300000, 0x0000000099400000|  0%| F|  |TAMS 0x0000000099300000, 0x0000000099300000| Untracked 
|  96|0x0000000099400000, 0x0000000099400000, 0x0000000099500000|  0%| F|  |TAMS 0x0000000099400000, 0x0000000099400000| Untracked 
|  97|0x0000000099500000, 0x0000000099500000, 0x0000000099600000|  0%| F|  |TAMS 0x0000000099500000, 0x0000000099500000| Untracked 
|  98|0x0000000099600000, 0x0000000099600000, 0x0000000099700000|  0%| F|  |TAMS 0x0000000099600000, 0x0000000099600000| Untracked 
|  99|0x0000000099700000, 0x0000000099700000, 0x0000000099800000|  0%| F|  |TAMS 0x0000000099700000, 0x0000000099700000| Untracked 
| 100|0x0000000099800000, 0x0000000099800000, 0x0000000099900000|  0%| F|  |TAMS 0x0000000099800000, 0x0000000099800000| Untracked 
| 101|0x0000000099900000, 0x0000000099900000, 0x0000000099a00000|  0%| F|  |TAMS 0x0000000099900000, 0x0000000099900000| Untracked 
| 102|0x0000000099a00000, 0x0000000099b00000, 0x0000000099b00000|100%| S|CS|TAMS 0x0000000099a00000, 0x0000000099a00000| Complete 
| 103|0x0000000099b00000, 0x0000000099b00000, 0x0000000099c00000|  0%| F|  |TAMS 0x0000000099b00000, 0x0000000099b00000| Untracked 
| 104|0x0000000099c00000, 0x0000000099c00000, 0x0000000099d00000|  0%| F|  |TAMS 0x0000000099c00000, 0x0000000099c00000| Untracked 
| 105|0x0000000099d00000, 0x0000000099d00000, 0x0000000099e00000|  0%| F|  |TAMS 0x0000000099d00000, 0x0000000099d00000| Untracked 
| 106|0x0000000099e00000, 0x0000000099e00000, 0x0000000099f00000|  0%| F|  |TAMS 0x0000000099e00000, 0x0000000099e00000| Untracked 
| 107|0x0000000099f00000, 0x0000000099f00000, 0x000000009a000000|  0%| F|  |TAMS 0x0000000099f00000, 0x0000000099f00000| Untracked 
| 108|0x000000009a000000, 0x000000009a000000, 0x000000009a100000|  0%| F|  |TAMS 0x000000009a000000, 0x000000009a000000| Untracked 
| 109|0x000000009a100000, 0x000000009a1331e0, 0x000000009a200000| 19%| E|  |TAMS 0x000000009a100000, 0x000000009a100000| Complete 
|1736|0x00000000ffc00000, 0x00000000ffc4b000, 0x00000000ffd00000| 29%|OA|  |TAMS 0x00000000ffc00000, 0x00000000ffc00000| Untracked 
|1737|0x00000000ffd00000, 0x00000000ffd6d000, 0x00000000ffe00000| 42%|CA|  |TAMS 0x00000000ffd00000, 0x00000000ffd00000| Untracked 

Card table byte_map: [0x00007ff422410000,0x00007ff422776000] _byte_map_base: 0x00007ff421f76000

Marking Bits (Prev, Next): (CMBitMap*) 0x00007ff41c062948, (CMBitMap*) 0x00007ff41c062980
 Prev Bits: [0x00007ff42057a000, 0x00007ff4220aa000)
 Next Bits: [0x00007ff3fe4d0000, 0x00007ff400000000)

Polling page: 0x00007ff426176000

Metaspace:

Usage:
  Non-class:      4.60 MB capacity,     2.55 MB ( 55%) used,     2.04 MB ( 44%) free+waste,     6.38 KB ( <1%) overhead. 
      Class:    544.00 KB capacity,   335.42 KB ( 62%) used,   204.83 KB ( 38%) free+waste,     3.75 KB ( <1%) overhead. 
       Both:      5.13 MB capacity,     2.87 MB ( 56%) used,     2.24 MB ( 44%) free+waste,    10.12 KB ( <1%) overhead. 

Virtual space:
  Non-class space:        8.00 MB reserved,       4.75 MB ( 59%) committed 
      Class space:        1.00 GB reserved,     640.00 KB ( <1%) committed 
             Both:        1.01 GB reserved,       5.38 MB ( <1%) committed 

Chunk freelists:
   Non-Class:  29.00 KB
       Class:  0 bytes
        Both:  29.00 KB

MaxMetaspaceSize: unlimited
CompressedClassSpaceSize: 1.00 GB

CodeHeap 'non-profiled nmethods': size=120036Kb used=80Kb max_used=80Kb free=119955Kb
 bounds [0x00007ff40befa000, 0x00007ff40c16a000, 0x00007ff413433000]
CodeHeap 'profiled nmethods': size=120032Kb used=569Kb max_used=569Kb free=119462Kb
 bounds [0x00007ff4049c2000, 0x00007ff404c32000, 0x00007ff40befa000]
CodeHeap 'non-nmethods': size=5692Kb used=1100Kb max_used=1100Kb free=4591Kb
 bounds [0x00007ff404433000, 0x00007ff4046a3000, 0x00007ff4049c2000]
 total_blobs=763 nmethods=374 adapters=301
 compilation: enabled
              stopped_count=0, restarted_count=0
 full_count=0

Compilation events (20 events):
Event: 0.175 Thread 0x00007ff41c178800  373       3       jdk.internal.ref.CleanerImpl$PhantomCleanableRef::<init> (12 bytes)
Event: 0.175 Thread 0x00007ff41c178800 nmethod 373 0x00007ff404a4b890 code [0x00007ff404a4ba40, 0x00007ff404a4bc68]
Event: 0.175 Thread 0x00007ff41c178800  375       3       java.io.InputStream::<init> (5 bytes)
Event: 0.175 Thread 0x00007ff41c178800 nmethod 375 0x00007ff404a4bd10 code [0x00007ff404a4bec0, 0x00007ff404a4c090]
Event: 0.176 Thread 0x00007ff41c178800  376       3       java.lang.ClassLoader::loadClass (7 bytes)
Event: 0.176 Thread 0x00007ff41c178800 nmethod 376 0x00007ff404a4c190 code [0x00007ff404a4c340, 0x00007ff404a4c6e8]
Event: 0.176 Thread 0x00007ff41c178800  377       3       jdk.internal.loader.BuiltinClassLoader::loadClass (22 bytes)
Event: 0.177 Thread 0x00007ff41c178800 nmethod 377 0x00007ff404a4c790 code [0x00007ff404a4c960, 0x00007ff404a4cdf8]
Event: 0.177 Thread 0x00007ff41c178800  378       3       java.util.Collections$UnmodifiableCollection$1::next (10 bytes)
Event: 0.177 Thread 0x00007ff41c178800 nmethod 378 0x00007ff404a4cf10 code [0x00007ff404a4d0c0, 0x00007ff404a4d468]
Event: 0.177 Thread 0x00007ff41c178800  379       3       java.util.jar.JarFile::getJarEntry (9 bytes)
Event: 0.177 Thread 0x00007ff41c178800 nmethod 379 0x00007ff404a4d510 code [0x00007ff404a4d720, 0x00007ff404a4e3a8]
Event: 0.177 Thread 0x00007ff41c178800  380       3       java.util.jar.JarFile::getEntry (22 bytes)
Event: 0.177 Thread 0x00007ff41c178800 nmethod 380 0x00007ff404a4e610 code [0x00007ff404a4e820, 0x00007ff404a4f2c8]
Event: 0.191 Thread 0x00007ff41c178800  383       3       jdk.internal.org.objectweb.asm.ByteVector::putInt (74 bytes)
Event: 0.191 Thread 0x00007ff41c178800 nmethod 383 0x00007ff404a4f510 code [0x00007ff404a4f6c0, 0x00007ff404a4f988]
Event: 0.191 Thread 0x00007ff41c178800  382       3       java.lang.invoke.MethodHandles$Lookup::lookupClassOrNull (17 bytes)
Event: 0.191 Thread 0x00007ff41c178800 nmethod 382 0x00007ff404a4fb10 code [0x00007ff404a4fcc0, 0x00007ff404a4feb0]
Event: 0.191 Thread 0x00007ff41c178800  381       3       java.util.zip.ZipUtils::CENSIZ (9 bytes)
Event: 0.191 Thread 0x00007ff41c178800 nmethod 381 0x00007ff404a4ff10 code [0x00007ff404a500e0, 0x00007ff404a50410]

GC Heap History (2 events):
Event: 0.183 GC heap before
{Heap before GC invocations=0 (full 0):
 garbage-first heap   total 114688K, used 6880K [0x0000000093400000, 0x0000000100000000)
  region size 1024K, 7 young (7168K), 0 survivors (0K)
 Metaspace       used 2936K, capacity 5251K, committed 5504K, reserved 1056768K
  class space    used 333K, capacity 544K, committed 640K, reserved 1048576K
}
Event: 0.190 GC heap after
{Heap after GC invocations=1 (full 0):
 garbage-first heap   total 114688K, used 3796K [0x0000000093400000, 0x0000000100000000)
  region size 1024K, 1 young (1024K), 1 survivors (1024K)
 Metaspace       used 2936K, capacity 5251K, committed 5504K, reserved 1056768K
  class space    used 333K, capacity 544K, committed 640K, reserved 1048576K
}

Deoptimization events (0 events):
No events

Classes redefined (0 events):
No events

Internal exceptions (3 events):
Event: 0.070 Thread 0x00007ff41c176800 Exception <a 'java/lang/NoSuchMethodError'{0x000000009a06e5a8}: getJVMCIClassLoader> (0x000000009a06e5a8) thrown at [./src/hotspot/share/prims/jni.cpp, line 1327]
Event: 0.108 Thread 0x00007ff41c027000 Exception <a 'java/lang/NoSuchMethodError'{0x0000000099fbf1f8}: 'java.lang.Object java.lang.invoke.DirectMethodHandle$Holder.invokeStaticInit(java.lang.Object, java.lang.Object, java.lang.Object)'> (0x0000000099fbf1f8) thrown at [./src/hotspot/share/interpreter/linkResolver.cpp, line 773]
Event: 0.179 Thread 0x00007ff41c252000 Exception <a 'java/lang/NoSuchMethodError'{0x0000000099b5b468}: 'java.lang.Object java.lang.invoke.DirectMethodHandle$Holder.invokeStaticInit(java.lang.Object, java.lang.Object, long, java.lang.Object)'> (0x0000000099b5b468) thrown at [./src/hotspot/share/interpreter/linkResolver.cpp, line 773]

Events (20 events):
Event: 0.177 loading class org/apache/maven/surefire/report/RunListener
Event: 0.177 loading class org/apache/maven/surefire/report/RunListener done
Event: 0.178 loading class java/lang/ProcessHandleImpl
Event: 0.178 loading class java/lang/ProcessHandle
Event: 0.178 loading class java/lang/ProcessHandle done
Event: 0.178 loading class java/lang/ProcessHandleImpl done
Event: 0.178 loading class org/apache/maven/surefire/booter/BaseProviderFactory
Event: 0.178 loading class org/apache/maven/surefire/booter/BaseProviderFactory done
Event: 0.178 Executing VM operation: G1CollectForAllocation
Event: 0.180 Thread 0x00007ff3c8106000 Thread added: 0x00007ff3c8106000
Event: 0.181 Protecting memory [0x00007ff3ec7f2000,0x00007ff3ec7f6000] with protection modes 0
Event: 0.190 Executing VM operation: G1CollectForAllocation done
Event: 0.190 loading class org/apache/maven/surefire/booter/DirectoryScannerParametersAware
Event: 0.190 loading class org/apache/maven/surefire/booter/DirectoryScannerParametersAware done
Event: 0.190 loading class org/apache/maven/surefire/booter/ReporterConfigurationAware
Event: 0.190 loading class org/apache/maven/surefire/booter/ReporterConfigurationAware done
Event: 0.190 loading class org/apache/maven/surefire/booter/SurefireClassLoadersAware
Event: 0.190 loading class org/apache/maven/surefire/booter/SurefireClassLoadersAware done
Event: 0.191 loading class org/apache/maven/surefire/booter/TestRequestAware
Event: 0.191 loading class org/apache/maven/surefire/booter/TestRequestAware done


Dynamic libraries:
93400000-9a200000 rw-p 00000000 00:00 0 
9a200000-ffc00000 ---p 00000000 00:00 0 
ffc00000-ffc4b000 rw-p 010e8000 00:35 3115581                            /jdk/lib/server/classes.jsa
ffc4b000-ffd00000 rw-p 00000000 00:00 0 
ffd00000-ffd6d000 rw-p 0107b000 00:35 3115581                            /jdk/lib/server/classes.jsa
ffd6d000-ffe00000 rw-p 00000000 00:00 0 
ffe00000-100000000 ---p 00000000 00:00 0 
800000000-800002000 rwxp 00001000 00:35 3115581                          /jdk/lib/server/classes.jsa
800002000-80038d000 rw-p 00003000 00:35 3115581                          /jdk/lib/server/classes.jsa
80038d000-800a6a000 r--p 0038e000 00:35 3115581                          /jdk/lib/server/classes.jsa
800a6a000-800a6b000 rw-p 00a6b000 00:35 3115581                          /jdk/lib/server/classes.jsa
800a6b000-80107a000 r--p 00a6c000 00:35 3115581                          /jdk/lib/server/classes.jsa
840000000-8400a0000 rw-p 00000000 00:00 0 
8400a0000-880000000 ---p 00000000 00:00 0 
55ac6a227000-55ac6a228000 r-xp 00000000 00:35 3104834                    /jdk/bin/java
55ac6a428000-55ac6a429000 r--p 00001000 00:35 3104834                    /jdk/bin/java
55ac6a429000-55ac6a42a000 rw-p 00002000 00:35 3104834                    /jdk/bin/java
55ac6bb26000-55ac6bb47000 rw-p 00000000 00:00 0                          [heap]
7ff3b8000000-7ff3b8021000 rw-p 00000000 00:00 0 
7ff3b8021000-7ff3bc000000 ---p 00000000 00:00 0 
7ff3c0000000-7ff3c0021000 rw-p 00000000 00:00 0 
7ff3c0021000-7ff3c4000000 ---p 00000000 00:00 0 
7ff3c4000000-7ff3c4021000 rw-p 00000000 00:00 0 
7ff3c4021000-7ff3c8000000 ---p 00000000 00:00 0 
7ff3c8000000-7ff3c810e000 rw-p 00000000 00:00 0 
7ff3c810e000-7ff3cc000000 ---p 00000000 00:00 0 
7ff3cc000000-7ff3cc0f6000 rw-p 00000000 00:00 0 
7ff3cc0f6000-7ff3d0000000 ---p 00000000 00:00 0 
7ff3d0000000-7ff3d0021000 rw-p 00000000 00:00 0 
7ff3d0021000-7ff3d4000000 ---p 00000000 00:00 0 
7ff3d616c000-7ff3dc000000 r--p 00000000 00:35 2095505                    /usr/lib/locale/locale-archive
7ff3dc000000-7ff3dc021000 rw-p 00000000 00:00 0 
7ff3dc021000-7ff3e0000000 ---p 00000000 00:00 0 
7ff3e0000000-7ff3e0021000 rw-p 00000000 00:00 0 
7ff3e0021000-7ff3e4000000 ---p 00000000 00:00 0 
7ff3e4000000-7ff3e4021000 rw-p 00000000 00:00 0 
7ff3e4021000-7ff3e8000000 ---p 00000000 00:00 0 
7ff3e8000000-7ff3e8021000 rw-p 00000000 00:00 0 
7ff3e8021000-7ff3ec000000 ---p 00000000 00:00 0 
7ff3ec345000-7ff3ec6f1000 rw-p 00000000 00:00 0 
7ff3ec6f1000-7ff3ec6f2000 ---p 00000000 00:00 0 
7ff3ec6f2000-7ff3ec7f2000 rw-p 00000000 00:00 0 
7ff3ec7f2000-7ff3ec7f6000 ---p 00000000 00:00 0 
7ff3ec7f6000-7ff3ec8f3000 rw-p 00000000 00:00 0 
7ff3ec8f3000-7ff3ec8f7000 ---p 00000000 00:00 0 
7ff3ec8f7000-7ff3ec9f4000 rw-p 00000000 00:00 0 
7ff3ec9f4000-7ff3ec9f9000 r-xp 00000000 00:35 3115535                    /jdk/lib/libmanagement_ext.so
7ff3ec9f9000-7ff3ecbf8000 ---p 00005000 00:35 3115535                    /jdk/lib/libmanagement_ext.so
7ff3ecbf8000-7ff3ecbf9000 r--p 00004000 00:35 3115535                    /jdk/lib/libmanagement_ext.so
7ff3ecbf9000-7ff3ecbfa000 rw-p 00005000 00:35 3115535                    /jdk/lib/libmanagement_ext.so
7ff3ecbfa000-7ff3ecbfe000 r-xp 00000000 00:35 3115533                    /jdk/lib/libmanagement.so
7ff3ecbfe000-7ff3ecdfd000 ---p 00004000 00:35 3115533                    /jdk/lib/libmanagement.so
7ff3ecdfd000-7ff3ecdfe000 r--p 00003000 00:35 3115533                    /jdk/lib/libmanagement.so
7ff3ecdfe000-7ff3ecdff000 rw-p 00004000 00:35 3115533                    /jdk/lib/libmanagement.so
7ff3ecdff000-7ff3ece03000 ---p 00000000 00:00 0 
7ff3ece03000-7ff3ed000000 rw-p 00000000 00:00 0 
7ff3ed0c0000-7ff3ee983000 r-xp 00000000 00:35 3115530                    /jdk/lib/libjvmcicompiler.so
7ff3ee983000-7ff3eebce000 rw-p 018c3000 00:35 3115530                    /jdk/lib/libjvmcicompiler.so
7ff3eebce000-7ff3eebd1000 r-xp 01b0e000 00:35 3115530                    /jdk/lib/libjvmcicompiler.so
7ff3eebd1000-7ff3eedd0000 ---p 01b11000 00:35 3115530                    /jdk/lib/libjvmcicompiler.so
7ff3eedd0000-7ff3eedd3000 rw-p 01b10000 00:35 3115530                    /jdk/lib/libjvmcicompiler.so
7ff3eedd3000-7ff3eede9000 r-xp 00000000 00:35 3115538                    /jdk/lib/libnet.so
7ff3eede9000-7ff3eefe8000 ---p 00016000 00:35 3115538                    /jdk/lib/libnet.so
7ff3eefe8000-7ff3eefe9000 r--p 00015000 00:35 3115538                    /jdk/lib/libnet.so
7ff3eefe9000-7ff3eefea000 rw-p 00016000 00:35 3115538                    /jdk/lib/libnet.so
7ff3eefea000-7ff3eeffa000 r-xp 00000000 00:35 3115540                    /jdk/lib/libnio.so
7ff3eeffa000-7ff3ef1f9000 ---p 00010000 00:35 3115540                    /jdk/lib/libnio.so
7ff3ef1f9000-7ff3ef1fa000 r--p 0000f000 00:35 3115540                    /jdk/lib/libnio.so
7ff3ef1fa000-7ff3ef1fb000 rw-p 00010000 00:35 3115540                    /jdk/lib/libnio.so
7ff3ef1fb000-7ff3ef1fc000 ---p 00000000 00:00 0 
7ff3ef1fc000-7ff3ef2fc000 rw-p 00000000 00:00 0 
7ff3ef2fc000-7ff3ef300000 ---p 00000000 00:00 0 
7ff3ef300000-7ff3ef3fd000 rw-p 00000000 00:00 0 
7ff3ef3fd000-7ff3ef401000 ---p 00000000 00:00 0 
7ff3ef401000-7ff3ef4fe000 rw-p 00000000 00:00 0 
7ff3ef4fe000-7ff3ef502000 ---p 00000000 00:00 0 
7ff3ef502000-7ff3ef5ff000 rw-p 00000000 00:00 0 
7ff3ef5ff000-7ff3ef603000 ---p 00000000 00:00 0 
7ff3ef603000-7ff3efcc0000 rw-p 00000000 00:00 0 
7ff3efcc0000-7ff3f0000000 ---p 00000000 00:00 0 
7ff3f0000000-7ff3f0021000 rw-p 00000000 00:00 0 
7ff3f0021000-7ff3f4000000 ---p 00000000 00:00 0 
7ff3f4000000-7ff3f4021000 rw-p 00000000 00:00 0 
7ff3f4021000-7ff3f8000000 ---p 00000000 00:00 0 
7ff3f8000000-7ff3f8021000 rw-p 00000000 00:00 0 
7ff3f8021000-7ff3fc000000 ---p 00000000 00:00 0 
7ff3fc0cd000-7ff3fc0d1000 ---p 00000000 00:00 0 
7ff3fc0d1000-7ff3fc2ce000 rw-p 00000000 00:00 0 
7ff3fc2ce000-7ff3fc2d2000 ---p 00000000 00:00 0 
7ff3fc2d2000-7ff3fc3cf000 rw-p 00000000 00:00 0 
7ff3fc3cf000-7ff3fc3d3000 ---p 00000000 00:00 0 
7ff3fc3d3000-7ff3fe688000 rw-p 00000000 00:00 0 
7ff3fe688000-7ff3ffff0000 ---p 00000000 00:00 0 
7ff3ffff0000-7ff3ffff8000 rw-p 00000000 00:00 0 
7ff3ffff8000-7ff400000000 ---p 00000000 00:00 0 
7ff400000000-7ff400021000 rw-p 00000000 00:00 0 
7ff400021000-7ff404000000 ---p 00000000 00:00 0 
7ff4040af000-7ff4040b3000 ---p 00000000 00:00 0 
7ff4040b3000-7ff4041b0000 rw-p 00000000 00:00 0 
7ff4041b0000-7ff4041b1000 ---p 00000000 00:00 0 
7ff4041b1000-7ff404332000 rw-p 00000000 00:00 0 
7ff404332000-7ff404333000 ---p 00000000 00:00 0 
7ff404333000-7ff404433000 rw-p 00000000 00:00 0 
7ff404433000-7ff4046a3000 rwxp 00000000 00:00 0 
7ff4046a3000-7ff4049c2000 ---p 00000000 00:00 0 
7ff4049c2000-7ff404c32000 rwxp 00000000 00:00 0 
7ff404c32000-7ff40befa000 ---p 00000000 00:00 0 
7ff40befa000-7ff40c16a000 rwxp 00000000 00:00 0 
7ff40c16a000-7ff413433000 ---p 00000000 00:00 0 
7ff413433000-7ff41c000000 r--s 00000000 00:35 3115556                    /jdk/lib/modules
7ff41c000000-7ff41c291000 rw-p 00000000 00:00 0 
7ff41c291000-7ff420000000 ---p 00000000 00:00 0 
7ff420075000-7ff420076000 ---p 00000000 00:00 0 
7ff420076000-7ff420378000 rw-p 00000000 00:00 0 
7ff420378000-7ff420379000 ---p 00000000 00:00 0 
7ff420379000-7ff420479000 rw-p 00000000 00:00 0 
7ff420479000-7ff42047a000 ---p 00000000 00:00 0 
7ff42047a000-7ff420732000 rw-p 00000000 00:00 0 
7ff420732000-7ff42209a000 ---p 00000000 00:00 0 
7ff42209a000-7ff4220a2000 rw-p 00000000 00:00 0 
7ff4220a2000-7ff4220aa000 ---p 00000000 00:00 0 
7ff4220aa000-7ff4220e1000 rw-p 00000000 00:00 0 
7ff4220e1000-7ff42240e000 ---p 00000000 00:00 0 
7ff42240e000-7ff42240f000 rw-p 00000000 00:00 0 
7ff42240f000-7ff422410000 ---p 00000000 00:00 0 
7ff422410000-7ff422447000 rw-p 00000000 00:00 0 
7ff422447000-7ff422774000 ---p 00000000 00:00 0 
7ff422774000-7ff422775000 rw-p 00000000 00:00 0 
7ff422775000-7ff422776000 ---p 00000000 00:00 0 
7ff422776000-7ff4227ad000 rw-p 00000000 00:00 0 
7ff4227ad000-7ff422ada000 ---p 00000000 00:00 0 
7ff422ada000-7ff422adb000 rw-p 00000000 00:00 0 
7ff422adb000-7ff422adc000 ---p 00000000 00:00 0 
7ff422adc000-7ff422cde000 rw-p 00000000 00:00 0 
7ff422cde000-7ff422cdf000 ---p 00000000 00:00 0 
7ff422cdf000-7ff422de4000 rw-p 00000000 00:00 0 
7ff422de4000-7ff422eca000 ---p 00000000 00:00 0 
7ff422eca000-7ff422ecf000 rw-p 00000000 00:00 0 
7ff422ecf000-7ff422fb5000 ---p 00000000 00:00 0 
7ff422fb5000-7ff422fbc000 r-xp 00000000 00:35 3115555                    /jdk/lib/libzip.so
7ff422fbc000-7ff4231bb000 ---p 00007000 00:35 3115555                    /jdk/lib/libzip.so
7ff4231bb000-7ff4231bc000 r--p 00006000 00:35 3115555                    /jdk/lib/libzip.so
7ff4231bc000-7ff4231bd000 rw-p 00007000 00:35 3115555                    /jdk/lib/libzip.so
7ff4231bd000-7ff4231ca000 r-xp 00000000 00:35 2093783                    /lib64/libnss_files-2.12.so
7ff4231ca000-7ff4233c9000 ---p 0000d000 00:35 2093783                    /lib64/libnss_files-2.12.so
7ff4233c9000-7ff4233ca000 r--p 0000c000 00:35 2093783                    /lib64/libnss_files-2.12.so
7ff4233ca000-7ff4233cb000 rw-p 0000d000 00:35 2093783                    /lib64/libnss_files-2.12.so
7ff4233cb000-7ff4233e6000 r-xp 00000000 00:35 3115524                    /jdk/lib/libjimage.so
7ff4233e6000-7ff4235e5000 ---p 0001b000 00:35 3115524                    /jdk/lib/libjimage.so
7ff4235e5000-7ff4235e7000 r--p 0001a000 00:35 3115524                    /jdk/lib/libjimage.so
7ff4235e7000-7ff4235e8000 rw-p 0001c000 00:35 3115524                    /jdk/lib/libjimage.so
7ff4235e8000-7ff423612000 r-xp 00000000 00:35 3115519                    /jdk/lib/libjava.so
7ff423612000-7ff423811000 ---p 0002a000 00:35 3115519                    /jdk/lib/libjava.so
7ff423811000-7ff423812000 r--p 00029000 00:35 3115519                    /jdk/lib/libjava.so
7ff423812000-7ff423814000 rw-p 0002a000 00:35 3115519                    /jdk/lib/libjava.so
7ff423814000-7ff423821000 r-xp 00000000 00:35 3115553                    /jdk/lib/libverify.so
7ff423821000-7ff423a20000 ---p 0000d000 00:35 3115553                    /jdk/lib/libverify.so
7ff423a20000-7ff423a22000 r--p 0000c000 00:35 3115553                    /jdk/lib/libverify.so
7ff423a22000-7ff423a23000 rw-p 0000e000 00:35 3115553                    /jdk/lib/libverify.so
7ff423a23000-7ff423a2a000 r-xp 00000000 00:35 2093795                    /lib64/librt-2.12.so
7ff423a2a000-7ff423c29000 ---p 00007000 00:35 2093795                    /lib64/librt-2.12.so
7ff423c29000-7ff423c2a000 r--p 00006000 00:35 2093795                    /lib64/librt-2.12.so
7ff423c2a000-7ff423c2b000 rw-p 00007000 00:35 2093795                    /lib64/librt-2.12.so
7ff423c2b000-7ff423cae000 r-xp 00000000 00:35 2093773                    /lib64/libm-2.12.so
7ff423cae000-7ff423ead000 ---p 00083000 00:35 2093773                    /lib64/libm-2.12.so
7ff423ead000-7ff423eae000 r--p 00082000 00:35 2093773                    /lib64/libm-2.12.so
7ff423eae000-7ff423eaf000 rw-p 00083000 00:35 2093773                    /lib64/libm-2.12.so
7ff423eaf000-7ff42502d000 r-xp 00000000 00:35 3115583                    /jdk/lib/server/libjvm.so
7ff42502d000-7ff42522c000 ---p 0117e000 00:35 3115583                    /jdk/lib/server/libjvm.so
7ff42522c000-7ff4252eb000 r--p 0117d000 00:35 3115583                    /jdk/lib/server/libjvm.so
7ff4252eb000-7ff425325000 rw-p 0123c000 00:35 3115583                    /jdk/lib/server/libjvm.so
7ff425325000-7ff42537d000 rw-p 00000000 00:00 0 
7ff42537d000-7ff425508000 r-xp 00000000 00:35 2093747                    /lib64/libc-2.12.so
7ff425508000-7ff425707000 ---p 0018b000 00:35 2093747                    /lib64/libc-2.12.so
7ff425707000-7ff42570b000 r--p 0018a000 00:35 2093747                    /lib64/libc-2.12.so
7ff42570b000-7ff42570d000 rw-p 0018e000 00:35 2093747                    /lib64/libc-2.12.so
7ff42570d000-7ff425711000 rw-p 00000000 00:00 0 
7ff425711000-7ff425713000 r-xp 00000000 00:35 2093753                    /lib64/libdl-2.12.so
7ff425713000-7ff425913000 ---p 00002000 00:35 2093753                    /lib64/libdl-2.12.so
7ff425913000-7ff425914000 r--p 00002000 00:35 2093753                    /lib64/libdl-2.12.so
7ff425914000-7ff425915000 rw-p 00003000 00:35 2093753                    /lib64/libdl-2.12.so
7ff425915000-7ff425924000 r-xp 00000000 00:35 3115494                    /jdk/lib/jli/libjli.so
7ff425924000-7ff425b24000 ---p 0000f000 00:35 3115494                    /jdk/lib/jli/libjli.so
7ff425b24000-7ff425b25000 r--p 0000f000 00:35 3115494                    /jdk/lib/jli/libjli.so
7ff425b25000-7ff425b26000 rw-p 00010000 00:35 3115494                    /jdk/lib/jli/libjli.so
7ff425b26000-7ff425b3d000 r-xp 00000000 00:35 2093791                    /lib64/libpthread-2.12.so
7ff425b3d000-7ff425d3d000 ---p 00017000 00:35 2093791                    /lib64/libpthread-2.12.so
7ff425d3d000-7ff425d3e000 r--p 00017000 00:35 2093791                    /lib64/libpthread-2.12.so
7ff425d3e000-7ff425d3f000 rw-p 00018000 00:35 2093791                    /lib64/libpthread-2.12.so
7ff425d3f000-7ff425d43000 rw-p 00000000 00:00 0 
7ff425d43000-7ff425d58000 r-xp 00000000 00:35 1836846                    /lib64/libz.so.1.2.3
7ff425d58000-7ff425f57000 ---p 00015000 00:35 1836846                    /lib64/libz.so.1.2.3
7ff425f57000-7ff425f58000 r--p 00014000 00:35 1836846                    /lib64/libz.so.1.2.3
7ff425f58000-7ff425f59000 rw-p 00015000 00:35 1836846                    /lib64/libz.so.1.2.3
7ff425f59000-7ff425f79000 r-xp 00000000 00:35 2093740                    /lib64/ld-2.12.so
7ff425fbe000-7ff425fc5000 r--s 00000000 00:35 2095767                    /usr/lib64/gconv/gconv-modules.cache
7ff425fc5000-7ff42605f000 rw-p 00000000 00:00 0 
7ff42605f000-7ff426066000 ---p 00000000 00:00 0 
7ff426066000-7ff42606e000 rw-s 00000000 00:35 3104666                    /tmp/hsperfdata_root/43261
7ff42606e000-7ff426072000 ---p 00000000 00:00 0 
7ff426072000-7ff426175000 rw-p 00000000 00:00 0 
7ff426175000-7ff426176000 rw-p 00000000 00:00 0 
7ff426176000-7ff426177000 ---p 00000000 00:00 0 
7ff426177000-7ff426178000 r--p 00000000 00:00 0 
7ff426178000-7ff426179000 rw-p 00000000 00:00 0 
7ff426179000-7ff42617a000 r--p 00020000 00:35 2093740                    /lib64/ld-2.12.so
7ff42617a000-7ff42617b000 rw-p 00021000 00:35 2093740                    /lib64/ld-2.12.so
7ff42617b000-7ff42617c000 rw-p 00000000 00:00 0 
7fff1b0e1000-7fff1b102000 rw-p 00000000 00:00 0                          [stack]
7fff1b159000-7fff1b15d000 r--p 00000000 00:00 0                          [vvar]
7fff1b15d000-7fff1b15f000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]


VM Arguments:
jvm_args: -XX:+UnlockExperimentalVMOptions -XX:+EnableJVMCIProduct -XX:-UnlockExperimentalVMOptions -XX:ThreadPriorityPolicy=1 -dsa -da -ea:io.netty... -XX:+HeapDumpOnOutOfMemoryError -Xlog:gc -Dio.netty.leakDetectionLevel=paranoid -Dio.netty.leakDetection.targetRecords=32 -D_ -D_ --illegal-access=deny -D_ 
java_command: /code/resolver-dns-native-macos/target/surefire/surefirebooter1411533031050063994.jar /code/resolver-dns-native-macos/target/surefire 2021-08-11T14-04-50_498-jvmRun1 surefire7069040594597368954tmp surefire_236978304828881009263tmp
java_class_path (initial): /code/resolver-dns-native-macos/target/surefire/surefirebooter1411533031050063994.jar
Launcher Type: SUN_STANDARD

[Global flags]
     intx CICompilerCount                          = 2                                         {product} {ergonomic}
     uint ConcGCThreads                            = 1                                         {product} {ergonomic}
     bool EnableJVMCIProduct                       = true                                {JVMCI product} {jimage}
     uint G1ConcRefinementThreads                  = 2                                         {product} {ergonomic}
   size_t G1HeapRegionSize                         = 1048576                                   {product} {ergonomic}
    uintx GCDrainStackTargetSize                   = 64                                        {product} {ergonomic}
     bool HeapDumpOnOutOfMemoryError               = true                                   {manageable} {command line}
   size_t InitialHeapSize                          = 115343360                                 {product} {ergonomic}
   size_t MarkStackSize                            = 4194304                                   {product} {ergonomic}
   size_t MaxHeapSize                              = 1824522240                                {product} {ergonomic}
   size_t MaxNewSize                               = 1094713344                                {product} {ergonomic}
   size_t MinHeapDeltaBytes                        = 1048576                                   {product} {ergonomic}
    uintx NonNMethodCodeHeapSize                   = 5825164                                {pd product} {ergonomic}
    uintx NonProfiledCodeHeapSize                  = 122916538                              {pd product} {ergonomic}
    uintx ProfiledCodeHeapSize                     = 122916538                              {pd product} {ergonomic}
    uintx ReservedCodeCacheSize                    = 251658240                              {pd product} {ergonomic}
     bool SegmentedCodeCache                       = true                                      {product} {ergonomic}
     intx ThreadPriorityPolicy                     = 1                                         {product} {jimage}
     bool UseCompressedClassPointers               = true                                 {lp64_product} {ergonomic}
     bool UseCompressedOops                        = true                                 {lp64_product} {ergonomic}
     bool UseG1GC                                  = true                                      {product} {ergonomic}

Logging:
Log output configuration:
 #0: stdout all=warning,gc=info uptime,level,tags
 #1: stderr all=off uptime,level,tags

Environment Variables:
JAVA_HOME=/jdk
PATH=/jdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/root/bin

Signal Handlers:
SIGSEGV: [libjvm.so+0xe6ff50], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGBUS: [libjvm.so+0xe6ff50], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGFPE: [libjvm.so+0xe6ff50], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGPIPE: [libjvm.so+0xc40370], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGXFSZ: [libjvm.so+0xc40370], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGILL: [libjvm.so+0xe6ff50], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGUSR2: [libjvm.so+0xc40210], sa_mask[0]=00100000000000000000000000000000, sa_flags=SA_RESTART|SA_SIGINFO
SIGHUP: [libjvm.so+0xc40570], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGINT: [libjvm.so+0xc40570], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGTERM: [libjvm.so+0xc40570], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO
SIGQUIT: [libjvm.so+0xc40570], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO


---------------  S Y S T E M  ---------------

OS:CentOS release 6.10 (Final)
uname:Linux 5.8.0-1039-azure #42~20.04.1-Ubuntu SMP Thu Jul 15 14:11:07 UTC 2021 x86_64
OS uptime: 0 days 0:30 hours
libc:glibc 2.12 NPTL 2.12 
rlimit: STACK 16384k, CORE infinity, NPROC infinity, NOFILE 1048576, AS infinity, CPU infinity, DATA infinity, FSIZE infinity
load average:1.34 1.54 2.43

/proc/meminfo:
MemTotal:        7120800 kB
MemFree:         3229976 kB
MemAvailable:    5212024 kB
Buffers:          237108 kB
Cached:          1800376 kB
SwapCached:          568 kB
Active:          1879052 kB
Inactive:        1596308 kB
Active(anon):     937300 kB
Inactive(anon):   493428 kB
Active(file):     941752 kB
Inactive(file):  1102880 kB
Unevictable:       26720 kB
Mlocked:           26720 kB
SwapTotal:       4194300 kB
SwapFree:        4191972 kB
Dirty:              5684 kB
Writeback:             0 kB
AnonPages:       1458824 kB
Mapped:           388400 kB
Shmem:              7520 kB
KReclaimable:     247328 kB
Slab:             318232 kB
SReclaimable:     247328 kB
SUnreclaim:        70904 kB
KernelStack:        5088 kB
PageTables:         9184 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:     7754700 kB
Committed_AS:    4243916 kB
VmallocTotal:   34359738367 kB
VmallocUsed:       33156 kB
VmallocChunk:          0 kB
Percpu:             1816 kB
HardwareCorrupted:     0 kB
AnonHugePages:    536576 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
FileHugePages:         0 kB
FilePmdMapped:         0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:               0 kB
DirectMap4k:      157632 kB
DirectMap2M:     5085184 kB
DirectMap1G:     4194304 kB


/proc/sys/kernel/threads-max (system-wide limit on the number of threads):
55571


/proc/sys/vm/max_map_count (maximum number of memory map areas a process may have):
262144


/proc/sys/kernel/pid_max (system-wide limit on number of process identifiers):
4194304



container (cgroup) information:
container_type: cgroupv1
cpu_cpuset_cpus: 0-1
cpu_memory_nodes: 0
active_processor_count: 2
cpu_quota: -1
cpu_period: 100000
cpu_shares: -1
memory_limit_in_bytes: -1
memory_and_swap_limit_in_bytes: -1
memory_soft_limit_in_bytes: -1
memory_usage_in_bytes: 1099603968
memory_max_usage_in_bytes: 4315193344

HyperV virtualization detected
Steal ticks since vm start: 0
Steal ticks percentage since vm start:  0.000

CPU:total 2 (initial active 2) (2 cores per cpu, 1 threads per core) family 6 model 85 stepping 7, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, avx2, aes, clmul, erms, rtm, 3dnowpref, lzcnt, tsc, bmi1, bmi2, adx, evex, fma
CPU Model and flags from /proc/cpuinfo:
model name	: Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt avx512cd avx512bw avx512vl xsaveopt xsavec xsaves md_clear

Memory: 4k page, physical 7120800k(3229976k free), swap 4194300k(4191972k free)

vm_info: OpenJDK 64-Bit Server VM (11.0.7+10-jvmci-20.1-b02) for linux-amd64 JRE (11.0.7+10-jvmci-20.1-b02), built on Apr 21 2020 13:02:22 by ""buildslave"" with gcc 7.3.0

END. | Crashed command:
# Created at 2021-08-11T14:31:07.146
/bin/sh: line 1: 43261 Aborted                 (core dumped) /jdk/bin/java -server -dsa -da -ea:io.netty... -XX:+HeapDumpOnOutOfMemoryError -Xlog:gc -Dio.netty.leakDetectionLevel=paranoid -Dio.netty.leakDetection.targetRecords=32 -D_ -D_ --illegal-access=deny -D_ -jar /code/resolver-dns-native-macos/target/surefire/surefirebooter1411533031050063994.jar /code/resolver-dns-native-macos/target/surefire 2021-08-11T14-04-50_498-jvmRun1 surefire7069040594597368954tmp surefire_236978304828881009263tmp | @JaroslavTulach @vjovanov Does any of the above make sense to you guys? | Let's ship it... | Thanks for reporting. I would try the same with the latest version of GraalVM to see if the issue is still reproducible.","Remove the deprecated ThreadDeathWatcher

Motivation:
The deprecated ThreadDeathWatcher produces more garbage and can delay resource release, when compared to manual resource management.

Modification:
Remove the ThreadDeathWatcher and other deprecated APIs that rely on it.

Result:
Less deprecated code."
netty/netty,11001,https://github.com/netty/netty/pull/11001,Minimize get byte multipart and fix buffer reuse,"# Fix HttpPostMultipartRequestDecoder buffer usages and allocations

## Motivation:

Method `getByte(position)` is too often called within the current implementation
of the HttpPostMultipartRequestDecoder.
This implies too much activities which is visible when PARANOID mode is active.
This is also true in standard mode.

Apply the same fix on buffer from HttpPostMultipartRequestDecoder to HttpPostStandardRequestDecoder
made previously.

Finally in order to ensure we do not rewrite already decoded HttpData when decoding
next ones within multipart, we must ensure the buffers are copied and not a retained slice.

## Modifications:

Use the `bytesBefore(...)` method instead of `getByte(pos)` in order to limit the external
access to the underlying buffer by retrieving iteratively the beginning of a correct start
position.
It is used to find both LF/CRLF and delimiter.
2 methods in HttpPostBodyUtil were created for that.

The undecodedChunk is copied when adding a chunk to a DataMultipart is loaded.
The same buffer is also rewritten in order to release the copied memory part.

## Result:

Just for note, for both Memory or Disk or Mixed mode factories, the release has to be done as:

    for (InterfaceHttpData httpData: decoder.getBodyHttpDatas()) {
        httpData.release();
        factory.removeHttpDataFromClean(request, httpData);
    }
    factory.cleanAllHttpData();
    decoder.destroy();

The memory used is minimal in Disk or Mixed mode. In Memory mode, a big file is still
in memory but not more in the undecodedChunk but its own buffer (copied).

In terms of benchmarking, the results are:

    Original code Benchmark                                                             Mode  Cnt  Score    Error   Units
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  0,152 ±  0,100  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  0,543 ±  0,218  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  0,001 ±  0,001  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  0,615 ±  0,070  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  0,114 ±  0,063  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  0,664 ±  0,034  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,001 ±  0,001  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  0,620 ±  0,140  ops/ms

    New code Benchmark                                                                  Mode  Cnt  Score   Error   Units
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  4,253 ± 0,333  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  4,422 ± 0,250  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  0,877 ± 0,014  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  4,151 ± 0,481  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  2,167 ± 0,098  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  2,520 ± 0,043  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,177 ± 0,003  ops/ms
    HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  2,419 ± 0,061  ops/ms

In short, using big file transfers, this is about 7 times faster with new code, while
using high number of HttpData, this is about 4 times faster with new code when using Simple Level.
When using Paranoid Level, using big file transfers, this is about 800 times faster with new code, while
using high number of HttpData, this is about 170 times faster with new code.

Small improvements are also added, concerning buffer allocation and unsueful check within MixedAttribute and charset extended usage.

# Add some tests to check consistency in Http Codec Multipart

## Motivation:
Underlying buffer usages might be erroneous when releasing them internaly
in HttpPostMultipartRequestDecoder.

2 bugs occurs:
1) Final File upload seems not to be of the right size
2) Memory, even in Disk mode, is increasing continuously, while it shouldn't

## Modification:
Add some tests to check consistency for HttpPostMultipartRequestDecoder.
Add a package protected method for testing purpose only.

## Results:
Without fixes, those tests failed. With the fixes, they passed.
",2021-02-26T13:24:39Z,chrisvest,fredericBregier,readable,"I try to rewrite from the beginning the previous proposal.
I believe it is clearer and also I found some bugs still there (decoding in error, memory allocation in error) in the current actual implementation. | I see that I should have used this PR benchmark to test the effectiveness of #10737 in a more realistic use case too :) | I see that I should have used this PR benchmark to test the effectiveness of #10737 in a more realistic use case too :)

@franz1981
I'm not sure to what point you think it is relevant but if it is using getByte when iterating on a position to find a value is quite consuming (mainly due to access being partially or totally logged within Netty to find bad releasing, but not only limited to that point): using bytesBefore seems more efficient.
If this is this point, yes, trying to not iterate on getByte is quite a good way (as it is in ""almost"" every language on file for instance to not read byte after byte).
In addition, we could implement a specific method within ByteBuf that allows to search for a ""serie of bytes"", but I think it might be too specific in my case so I decide to let this within HttpCodec as a static helper method. | I'm not sure to what point you think it is relevant

If you will use bytesBefore or indexOf you will likely get an additional speedup thanks to the PR I have linked ;) | @fredericBregier does this replace #10982  ? | @normanmaurer yes, I forgot to close the previous one.
It is a rewritten for both original #10623 and following ones. | I forgot to mention: the benchmarks look great, and I like how many parts of the code became simpler. Nice work! | @franz1981 Done, thank you !! | @fredericBregier Thanks! (sorry about the delay) | This change broke the semantics of the decoder API. Will file an issue","Add some tests to check consistency in Http Codec Multipart

Motivation:
Underlying buffer usages might be erroneous when releasing them internaly
in HttpPostMultipartRequestDecoder.

2 bugs occurs:
1) Final File upload seems not to be of the right size
2) Memory, even in Disk mode, is increasing continuously, while it shouldn't

Modification:
Add some tests to check consistency for HttpPostMultipartRequestDecoder.
Add a package protected method for testing purpose only. | Fix HttpPostMultipartRequestDecoder buffer usages and allocations

Motivation:

Method `getByte(position)` is too often called within the current implementation
of the HttpPostMultipartRequestDecoder.
This implies too much activities which is visible when PARANOID mode is active.
This is also true in standard mode.

Apply the same fix on buffer from HttpPostMultipartRequestDecoder to HttpPostStandardRequestDecoder
made previously.

Finally in order to ensure we do not rewrite already decoded HttpData when decoding
next ones within multipart, we must ensure the buffers are copied and not a retained slice.

Modifications:

Use the `bytesBefore(...)` method instead of `getByte(pos)` in order to limit the external
access to the underlying buffer by retrieving iteratively the beginning of a correct start
position.
It is used to find both LF/CRLF and delimiter.
2 methods in HttpPostBodyUtil were created for that.

The undecodedChunk is copied when adding a chunk to a DataMultipart is loaded.
The same buffer is also rewritten in order to release the copied memory part.

Result:

Just for note, for both Memory or Disk or Mixed mode factories, the release has to be done as:

      for (InterfaceHttpData httpData: decoder.getBodyHttpDatas()) {
          httpData.release();
          factory.removeHttpDataFromClean(request, httpData);
      }
      factory.cleanAllHttpData();
      decoder.destroy();

The memory used is minimal in Disk or Mixed mode. In Memory mode, a big file is still
in memory but not more in the undecodedChunk but its own buffer (copied).

In terms of benchmarking, the results are:

Original code Benchmark                                                             Mode  Cnt  Score    Error   Units
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  0,152 ±  0,100  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  0,543 ±  0,218  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  0,001 ±  0,001  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  0,615 ±  0,070  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  0,114 ±  0,063  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  0,664 ±  0,034  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,001 ±  0,001  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  0,620 ±  0,140  ops/ms

New code Benchmark                                                                  Mode  Cnt  Score   Error   Units
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  4,037 ± 0,358  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  4,226 ± 0,471  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  0,875 ± 0,029  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  4,346 ± 0,275  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  2,044 ± 0,020  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  2,278 ± 0,159  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,174 ± 0,004  ops/ms
HttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  2,370 ± 0,065  ops/ms

In short, using big file transfers, this is about 7 times faster with new code, while
using high number of HttpData, this is about 4 times faster with new code when using Simple Level.
When using Paranoid Level, using big file transfers, this is about 800 times faster with new code, while
using high number of HttpData, this is about 170 times faster with new code. | Fixes from Chris Vest revue

Fix typo and naming, Charset usage, while to if, improve comments, smaller tests, factorized them and split tests according to factory type, optimize buffer allocation when given buffer is empty, improve findDelimiter, remove unuseful check in MixedAttribute

Still pending AbstractSearchProcessorFactory usage to check"
netty/netty,15835,https://github.com/netty/netty/pull/15835,Backward Compatible - Http3SettingsFrame abstraction,"## **Motivation**

* [x] Provide a cleaner, more maintainable implementation of `Http3SettingsFrame`.
* [x] Replace the previous untyped `LongObjectHashMap`-based approach with a typed `Http3Settings` abstraction.
* [x] Improve API clarity and future extensibility for HTTP/3 settings handling (similar to Http2Settings)

---

## **Modifications**

* [x] Introduced `Http3Settings` class to encapsulate and manage HTTP/3 settings.
* [x] Deprecated legacy untyped accessors (`get()`, `put()`) in favor of typed methods such as `qpackMaxTableCapacity()` and `enableConnectProtocol()`.
* [x] Updated `DefaultHttp3SettingsFrame` to use the new `Http3Settings` internally.
* [x] Added detailed Javadoc to guide migration to the new typed API.
* [x] Added comprehensive test coverage for `Http3Settings` and its integration with `Http3SettingsFrame`.

---

## **Impact**

* [x] **Backward-compatible** – existing code using `get()`/`put()` still works (marked deprecated).
* [x] **Forward-compatible** – encourages use of typed accessors.
* [x] **No behavioral change** – internal structure and API clarity improved only.

---

## **Result**

Fixes #[15823](https://github.com/netty/netty/issues/15823). 

",2025-11-13T15:07:45Z,normanmaurer,sanjomo,clarity,"@normanmaurer while doing this I have found an issue

https://www.rfc-editor.org/rfc/rfc9114.html#section-7.2.4-9 defines ""An implementation MUST ignore any parameter with an identifier it does not understand."" but we have allowed any custom identifier , so need clarification on this part

    @MethodSource(""data"")
    public void testHttp3SettingsFrame(
            boolean fragmented, int maxBlockedStreams, boolean delayQpackStreams) throws Exception {
        setUp(maxBlockedStreams, delayQpackStreams);
        Http3SettingsFrame settingsFrame = new DefaultHttp3SettingsFrame();
        settingsFrame.put(Http3SettingsFrame.HTTP3_SETTINGS_QPACK_MAX_TABLE_CAPACITY, 100L);
        settingsFrame.put(Http3SettingsFrame.HTTP3_SETTINGS_QPACK_BLOCKED_STREAMS, 1L);
        settingsFrame.put(Http3SettingsFrame.HTTP3_SETTINGS_MAX_FIELD_SECTION_SIZE, 128L);
        // Ensure we can encode and decode all sizes correctly.
        settingsFrame.put(63, 63L);
        settingsFrame.put(16383, 16383L);
        settingsFrame.put(1073741823, 1073741823L);
        settingsFrame.put(4611686018427387903L, 4611686018427387903L);
        testFrameEncodedAndDecoded(
                fragmented, maxBlockedStreams, delayQpackStreams, settingsFrame);
    } | @sanjomo I think we should first merge this one an

@normanmaurer while doing this I have found an issue

https://www.rfc-editor.org/rfc/rfc9114.html#section-7.2.4-9 defines ""An implementation MUST ignore any parameter with an identifier it does not understand."" but we have allowed any custom identifier , so need clarification on this part

    @MethodSource(""data"")
    public void testHttp3SettingsFrame(
            boolean fragmented, int maxBlockedStreams, boolean delayQpackStreams) throws Exception {
        setUp(maxBlockedStreams, delayQpackStreams);
        Http3SettingsFrame settingsFrame = new DefaultHttp3SettingsFrame();
        settingsFrame.put(Http3SettingsFrame.HTTP3_SETTINGS_QPACK_MAX_TABLE_CAPACITY, 100L);
        settingsFrame.put(Http3SettingsFrame.HTTP3_SETTINGS_QPACK_BLOCKED_STREAMS, 1L);
        settingsFrame.put(Http3SettingsFrame.HTTP3_SETTINGS_MAX_FIELD_SECTION_SIZE, 128L);
        // Ensure we can encode and decode all sizes correctly.
        settingsFrame.put(63, 63L);
        settingsFrame.put(16383, 16383L);
        settingsFrame.put(1073741823, 1073741823L);
        settingsFrame.put(4611686018427387903L, 4611686018427387903L);
        testFrameEncodedAndDecoded(
                fragmented, maxBlockedStreams, delayQpackStreams, settingsFrame);
    }
    


Let's discuss this in a separate issue. | @sanjomo please fix checkstyle:
[INFO] Starting audit...
Error:  /home/runner/work/netty/netty/codec-http3/src/main/java/io/netty/handler/codec/http3/DefaultHttp3SettingsFrame.java:37:39: '{' is not preceded with whitespace. [WhitespaceAround]
Error:  /home/runner/work/netty/netty/codec-http3/src/main/java/io/netty/handler/codec/http3/DefaultHttp3SettingsFrame.java:74: two or more consecutive empty lines [RegexpMultiline]
Error:  /home/runner/work/netty/netty/codec-http3/src/main/java/io/netty/handler/codec/http3/Http3Settings.java:305: 'if' construct must use '{}'s. [NeedBraces]
Error:  /home/runner/work/netty/netty/codec-http3/src/main/java/io/netty/handler/codec/http3/Http3Settings.java:306: 'if' construct must use '{}'s. [NeedBraces]
Error:  /home/runner/work/netty/netty/codec-http3/src/main/java/io/netty/handler/codec/http3/Http3Settings.java:334: 'if' construct must use '{}'s. [NeedBraces]
Error:  /home/runner/work/netty/netty/codec-http3/src/main/java/io/netty/handler/codec/http3/Http3SettingsFrame.java:79:31: '{' is not preceded with whitespace. [WhitespaceAround]
Error:  /home/runner/work/netty/netty/codec-http3/src/main/java/io/netty/handler/codec/http3/Http3SettingsFrame.java:108:43: '{' is not preceded with whitespace. [WhitespaceAround] | @vietj PTAL | @sanjomo did you sign our icla yet ? https://netty.io/s/icla | @normanmaurer done | @sanjomo thanks a lot for al the work.","Add support for :protocol pseudo-header in Http3Headers

Introduces the PROTOCOL pseudo-header to Http3Headers, including methods to set and retrieve its value. This enables handling of the :protocol header in HTTP/3 implementations. | Refactor HTTP/3 settings to use Http3Settings class

Introduces a dedicated Http3Settings class for managing HTTP/3 settings, replacing direct map usage in DefaultHttp3SettingsFrame and related classes. Updates all frame, handler, and test code to use the new settings API, providing typed accessors and improved validation. This change improves type safety, maintainability, and clarity of HTTP/3 settings handling throughout the codec. | Merge branch '4.2' of https://github.com/sanjomo/netty into 4.2 | Refactor DefaultHttp3SettingsFrame initialization avoid duplicates

Replaces usage of DefaultHttp3SettingsFrame(Http3Settings.defaultSettings()) with the no-argument constructor throughout the codebase. Updates tests and codec logic to use the new constructor, simplifying settings frame instantiation.
Avoiding duplicate settings | Revert ""Merge branch '4.2' of https://github.com/sanjomo/netty into 4.2""

This reverts commit 1e6f7e0618d50a4b14ec8edd17bbf1184645bd33, reversing
changes made to 10b6dbd50bb72cd02a47a0bf443f073f959761c6. | added missing licence | Refactor Http3Settings to use composition over inheritance

Replaces inheritance from LongObjectHashMap in Http3Settings with internal composition, updating all usages accordingly. Improves encapsulation, adds iterator and equality methods, and updates related classes (DefaultHttp3SettingsFrame, Http3FrameCodec, Http3SettingsFrame) to use the new API. Also fixes a bug in Http3ConnectionHandler where maxTableCapacity was incorrectly set. | Refactor HTTP/3 settings to dedicated Http3Settings class

Moved HTTP/3 settings constants and logic from Http3SettingsFrame to a new Http3Settings class, updating usages throughout the codebase. This improves encapsulation and maintainability of settings management, and updates tests and handlers to use the new class. | Merge branch '4.2' of https://github.com/sanjomo/netty into 4.2 | Refactor Http3SettingsFrame to use Http3Settings

Introduced a settings() method in Http3SettingsFrame and refactored DefaultHttp3SettingsFrame to use an internal Http3Settings instance. Updated get and put methods in Http3SettingsFrame to delegate to the settings object, improving encapsulation and consistency. | Revert ""Refactor Http3SettingsFrame to use Http3Settings""

This reverts commit f550863f81c6ea9834f2d79063dde3587ec06efe. | Refactor Http3SettingsFrame to use Http3Settings object

Introduced a settings() method in Http3SettingsFrame and DefaultHttp3SettingsFrame to provide direct access to the underlying Http3Settings object. Updated get and put methods in the interface to use the settings object by default. Removed exception for non-standard settings in Http3Settings to allow more flexible handling. | Deprecate Http3SettingsFrame constants in favor of Http3Settings

Replaces usage of Http3Settings constants with their deprecated equivalents in Http3SettingsFrame throughout the codebase. Updates imports, references, and test cases to use Http3SettingsFrame for QPACK and other HTTP/3 settings, and marks legacy constants and methods as deprecated to encourage migration to typed accessors. | Implement Iterable for Http3Settings and refactor copy logic

Http3Settings now implements Iterable<Map.Entry<Long, Long>> to support iteration over settings. The copyFrom method is renamed to putAll for clarity, and DefaultHttp3SettingsFrame.copyOf uses putAll for copying settings. These changes improve API consistency and usability. | Enforce non-negative values for custom HTTP/3 settings

Added validation to reject negative values for non-standard HTTP/3 settings in Http3Settings. Updated defaultSettings to set maxFieldSectionSize to Long.MAX_VALUE. Introduced comprehensive unit tests for Http3Settings covering validation, equality, builder pattern, and custom settings. | Update Http3Settings iterator and default maxFieldSectionSize

Refactored Http3Settings iterator to use PrimitiveEntry for improved efficiency and compatibility. Changed the default maxFieldSectionSize to be optional and unlimited, updating related test to expect null instead of Long.MAX_VALUE. | Fix settings copy logic in DefaultHttp3SettingsFrame

Replaces direct map access with the put method when copying settings from another frame, ensuring proper encapsulation and consistency. | Fix spacing in for-each loop declaration

Standardized spacing in the for-each loop declaration within DefaultHttp3SettingsFrame.java for improved code readability. | Deprecate legacy HTTP/3 settings accessors

Marked get and put methods in DefaultHttp3SettingsFrame as deprecated, recommending use of typed accessors and mutators on the settings object. Also added a check in Http3Settings to prevent setting values for keys reserved for HTTP/2. | Add unit tests for DefaultHttp3SettingsFrame

Introduces comprehensive tests for DefaultHttp3SettingsFrame, covering default behavior, put/get operations, reserved settings rejection, settings reference retention, equality, iterator functionality, string representation, deep copy, and deprecated method compatibility. | Add test for duplicate key error in settings frame

Introduces a unit test to verify that inserting a duplicate SETTINGS key in DefaultHttp3SettingsFrame triggers an IllegalStateException, ensuring protocol compliance. | review comment changes

Improved documentation and type safety for Http3Settings and DefaultHttp3SettingsFrame. Added detailed Javadoc comments, clarified validation logic, and updated copyright years. Enhanced test classes with explicit imports and documentation. | Remove unnecessary blank lines in Http3SettingsFrame and test | Set default maxFieldSectionSize to Long.MAX_VALUE

Updated Http3Settings to set maxFieldSectionSize to Long.MAX_VALUE by default, reflecting an unlimited field section size. Adjusted related test to assert the new default value. | review comment changes in java docs | checkstyle issues fixed | Add ENABLE_CONNECT_PROTOCOL to settings frame test

Added HTTP3_SETTINGS_ENABLE_CONNECT_PROTOCOL with value 0L to the settings frame in Http3FrameCodecTest to ensure proper encoding and decoding of this setting. | Merge branch '4.2' into 4.2 | review comment changes

Co-authored-by: Norman Maurer <norman_maurer@apple.com> | Merge branch '4.2' into 4.2 | Merge branch '4.2' into 4.2"
netty/netty,15486,https://github.com/netty/netty/pull/15486,Introduce VarHandle 9 stub,"Motivation:

Unsafe unavailability prevent some basic JIT optimizations while accessing buffers and reference counters, causing some pretty severe performance regression

Modifications:

Expose a VarHandle's 9 stub to enable Ja +9 to use it, if Unsafe is unavailable

Result:

Fixes #15485",2025-07-18T19:59:11Z,franz1981,franz1981,"clarity, easier to read","on JDK 24
Running getByteBatch before, with -pcheckBounds=true -pcheckAccessible=true -pbatchSize=32 -f 1 -pbufferType=HEAP --jvmArgs=""-Dio.netty.noUnsafe=true -Dio.netty.varHandle.enabled=false"" 
Benchmark                                  Score   Error  Units
ByteBufAccessBenchmark.getByteBatch       36.214 ± 0.499  ns/op

with this PR, with  -pcheckBounds=true -pcheckAccessible=true -pbatchSize=32 -f 1 -pbufferType=HEAP --jvmArgs=""-Dio.netty.noUnsafe=true -Dio.netty.varHandle.enabled=true"" 
Benchmark                                  Score   Error  Units
ByteBufAccessBenchmark.getByteBatch       12.446 ± 0.078  ns/op

with unsafe working i.e. -pcheckBounds=true -pcheckAccessible=true -pbatchSize=32 -f 1 -pbufferType=HEAP --jvmArgs=""-Dio.netty.noUnsafe=false -Dio.netty.varHandle.enabled=false"" 
Benchmark                                  Score   Error  Units
ByteBufAccessBenchmark.getByteBatch       13.186 ± 0.842  ns/op

which shows that VarHandle allows the ByteBuf's isAccessiblecheck to NOT use volatile (as expected). We could have usedgetOpaquebut I have decided to not change the existingUnsafe`'s semantic (which was to just treat the read as a plain get). | @franz1981 there are some failing tests... PTAL | @normanmaurer I've added another commit to handle the byte[] case too.
Will look into it (tomorrow) 🙏 | @normanmaurer @chrisvest I see that I could remove the VarHandle in the updater and just  use a MethodHande of private field (that's still requires reflective access since is a java 9+ method, see https://docs.oracle.com/javase/9/docs/api/java/lang/invoke/MethodHandles.html#privateLookupIn-java.lang.Class-java.lang.invoke.MethodHandles.Lookup- - but I will double check) | @normanmaurer I had to remove VarHandle as a return type from the signature of a method, to prevent Java 8 to complain with
java.lang.SecurityException: Prohibited package name: java.lang.invoke

but I'm still getting some error which I cannot reproduce locally e.g.
Error:  io.netty.buffer.UnpooledByteBufAllocatorTest.testBufferWithCapacity -- Time elapsed: 0 s <<< ERROR!
java.lang.invoke.WrongMethodTypeException: cannot convert MethodHandle(VarHandle,AbstractReferenceCountedByteBuf,int)void to (VarHandle,Object[])void
	at java.base/java.lang.invoke.MethodHandle.asTypeUncached(MethodHandle.java:915)
	at java.base/java.lang.invoke.MethodHandle.setAsTypeCache(MethodHandle.java:900)
	at java.base/java.lang.invoke.MethodHandle.asType(MethodHandle.java:873)
	at io.netty.util.internal.ReferenceCountUpdater.setInitialValue(ReferenceCountUpdater.java:74)
	at io.netty.buffer.AbstractReferenceCountedByteBuf.<init>(AbstractReferenceCountedByteBuf.java:61)
	at io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:56)
	at io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledDirectByteBuf.<init>(UnpooledByteBufAllocator.java:225)
	at io.netty.buffer.UnpooledByteBufAllocator.newDirectBuffer(UnpooledByteBufAllocator.java:95)
	at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188)
	at io.netty.buffer.AbstractByteBufAllocator.buffer(AbstractByteBufAllocator.java:124)
	at io.netty.buffer.ByteBufAllocatorTest.testBufferWithCapacity(ByteBufAllocatorTest.java:54)
	at io.netty.buffer.ByteBufAllocatorTest.testBufferWithCapacity(ByteBufAllocatorTest.java:48)
	at java.base/java.lang.reflect.Method.invoke(Method.java:565)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1604)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1604)

now investigating, since I see that the VarHandle in AbstractReferenceCountedByteBuf is called in a generic method signature as
    public void setInitialValue(T instance) {
        final long offset = unsafeOffset();
        if (offset == -1) {
            Object vho = varHandleUpdater();
            int initialValue = initialValue();
            if (vho != null) {
                VarHandle vh = (VarHandle) vho;
                vh.set(instance, (int) initialValue);  // <-------- THIS IS THE ERROR
                VarHandle.storeStoreFence();
            } else {
                updater().set(instance, initialValue);
            }
        } else {
            PlatformDependent.safeConstructPutInt(instance, offset, initialValue());
        }
    }
which enforce the ""right"" type at runtime, not at compile time.
At compile time the invocation is using
42 invokevirtual #43 <java/lang/invoke/VarHandle.set : (Lio/netty/util/ReferenceCounted;I)V>
I'll come up with a solution for that 👍 | There's still something which is surprising: if I write a sample program (using VarHandle) which receiver type is wider at compile time e.g. Object vs what's expected:

I'm not getting any failure
performance seems ok (and assembly too - verified) | On the CI I'm still getting hit by the same error on JDK 24-25
java.lang.invoke.WrongMethodTypeException: cannot convert MethodHandle(VarHandle,AbstractReferenceCountedByteBuf,int)void to (VarHandle,Object[])void
	at java.base/java.lang.invoke.MethodHandle.asTypeUncached(MethodHandle.java:915)
	at java.base/java.lang.invoke.MethodHandle.setAsTypeCache(MethodHandle.java:900)
	at java.base/java.lang.invoke.MethodHandle.asType(MethodHandle.java:873)
	at io.netty.buffer.AbstractReferenceCountedByteBuf$1.setVarHandleRefCnt(AbstractReferenceCountedByteBuf.java:51)
	at io.netty.buffer.AbstractReferenceCountedByteBuf$1.setVarHandleRefCnt(AbstractReferenceCountedByteBuf.java:38)
	at io.netty.util.internal.ReferenceCountUpdater.setInitialValue(ReferenceCountUpdater.java:73)
	at io.netty.buffer.AbstractReferenceCountedByteBuf.<init>(AbstractReferenceCountedByteBuf.java:70)
	at io.netty.buffer.UnpooledHeapByteBuf.<init>(UnpooledHeapByteBuf.java:51)
	at io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledHeapByteBuf.<init>(UnpooledByteBufAllocator.java:160)
	at io.netty.buffer.UnpooledByteBufAllocator.newHeapBuffer(UnpooledByteBufAllocator.java:85)
	at io.netty.buffer.AbstractByteBufAllocator.heapBuffer(AbstractByteBufAllocator.java:169)
	at io.netty.buffer.Unpooled.buffer(Unpooled.java:138)
	at io.netty.buffer.BigEndianHeapByteBufTest.newBuffer(BigEndianHeapByteBufTest.java:31)
	at io.netty.buffer.SimpleLeakAwareByteBufTest.newBuffer(SimpleLeakAwareByteBufTest.java:35)
	at io.netty.buffer.AbstractByteBufTest.newBuffer(AbstractByteBufTest.java:91)
	at io.netty.buffer.AbstractByteBufTest.init(AbstractByteBufTest.java:102)
	at io.netty.buffer.SimpleLeakAwareByteBufTest.init(SimpleLeakAwareByteBufTest.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:565)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1604)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1604)

but the signature now is
        @Override
        protected void setVarHandleRefCnt(VarHandle vh, AbstractReferenceCountedByteBuf instance, int refCnt) {
            vh.set(instance, refCnt);
        }
Looking at VarHandle code path (which I'm not an expert of) - there's some form of caching of types which ""could"" be related the many AbstractReferenceCountedByteBuf subclasses which can call such method - but I have no better guess since I cannot reproduce it ATM | It looks like I can finally reproduce it

Working on it -> putting this in draft again | It could be that the issue comes from the fact that the @PolymorphicSignature annotation is not present on your set method stub, which causes a wrong invokevirtual instruction to be generated (using the actual Object[] type). The javadoc of MethodHandle describes this:

In source code, a call to a signature polymorphic method will compile, regardless of the requested symbolic type descriptor. As usual, the Java compiler emits an invokevirtual instruction with the given symbolic type descriptor against the named method. The unusual part is that the symbolic type descriptor is derived from the actual argument and return types, not from the method declaration. | Is it worthy to give it a shot @derklaro thanks for the suggestion - although ""in theory"" the stub shouldn't even be loaded and from the invocation pov (at byte level) it shouldn't matter | @derklaro and I believe you're right
I just checked a clean example and the bytecode of a proper VarHandle invocation uses strong typed args (and not a Object[]) as the doc you quoted says.
Which means that this is likely the reason of the failure... | If you go the stubbing route here, maybe it's best to just combine it with the JFR stub instead of having two modules. There might come a point where we want eg a StackWalker stub too. | thanks for looking at it @yawkat
I?m stryggling to get this to work ATM due to what @derklaro noticed i.e. the stub has no polymorphic signature and we're  not getting it properly typed
I'm checking what I can do about it - unless you know
But I agree that we would like to have a single stubs modules at this point | @franz1981 I quickly skimmed through the jdk source and found that methods annotated with PolymorphicSignature must be native to be picked up as such, see: https://github.com/openjdk/jdk/blob/c9ecc826668575678f11578a67f125d430ebffad/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Types.java#L1330-L1337 | good catch, making the stub method native makes a simple VH test pass for me | Thanks both @derklaro and @yawkat : I'll now work to improve the way we use VarHandle for the ref cnt updater; I don't like how it is - but the ""stub"" thing is done thanks to you! | @franz1981 we might also be able to use it for io_uring and so remove the ugly ""compile two times hack"" | While working on this I've just noticed that VarHandle performance is NOT impacted by the receiver declared type as long as it is correct at runtime (meaning we could even pass Object as receiver instance). | PTAL @normanmaurer @yawkat
I haven't added tests and is still very...manual
But in order to have decent performance we need methods to be trusted.
I'm now checking if I can do better too; because it's likely that the code base will observe 2 types of rec cnt classes:

Unsafe or VarHandle
Atomic updater

If that will happen, it means:

on hotspot - a mild chance of getting bimorphic calls on some not hot path
on native image CE (@yawkat correct me if I'm wrong) megamorphic too

For the latter I'm pretty worried, because it could impact negatively ByteBuf(s) performance too (which should all use the same type, actually!). | PTAL @derklaro too | I cannot say yet if the failures are due to this PR changes, will take a look tomorrow | @yawkat i see you recently fixed #15354
Let me know if I have to update anything | I'm running few benchmarks to show why I've worked on this ❤️ | Thinking about it some more, I don't think the Configuration approach will work well.

It is not possible for native image to infer reflection metadata for AIFU and VH if the creation can't be statically analyzed
Recomputing the Unsafe offsets will be more painful

Either initialize the fields directly in each class, or use something like franz1981#1 | Thanks for checking @yawkat indeed NI performance is one of my concern (as you might expect ^^):
re

It is not possible for native image to infer reflection metadata for AIFU and VH if the creation can't be statically analyzed

why it cannot be statically analyzed?

or use something like franz1981#1

I would rather use the last, since it retain some centralized decision point re the initialization process/priority | NI cannot infer which field exactly you're referring to when creating those AIFUs and VHs. IME it only works for simple cases like new AtomicIntegerFieldUpdater(X.class, ""refCnt""), not when the class and field name cross method boundaries before being passed to the AIFU constructor | In theory both VarHandle/AIFU fields are visible in the anonymous class declared there and it should know (by running the static clint Configuration code - hence not fully statically) which type will be allocated.
Clearly my wild guess re ""why it cannot work?"" is just..speculation.
Can I gently ping @fniephaus on it? 🙏
Do we have any way to verify it?
In quarkus we don't depend yet by 4.2 so I cannot try it, any chance you can give it a shot with micronaut?
I can help inspecting the assembly if needed | With JDK 24 (batchSize = 64 for the Batch ops)
Unsafe:
Benchmark                                       Score   Error  Units
ByteBufAccessBenchmark.getByteBatch            19.018 ± 0.036  ns/op
ByteBufAccessBenchmark.getBytes                 3.103 ± 0.002  ns/op
ByteBufAccessBenchmark.getBytesConstantOffset   2.515 ± 0.020  ns/op
ByteBufAccessBenchmark.readByteBatch           19.493 ± 0.043  ns/op
ByteBufAccessBenchmark.readBytes                3.240 ± 0.006  ns/op
ByteBufAccessBenchmark.setByteBatch            19.060 ± 0.051  ns/op
ByteBufAccessBenchmark.setBytes                 4.119 ± 0.018  ns/op
ByteBufAccessBenchmark.setBytesConstantOffset   2.989 ± 0.129  ns/op

No Unsafe and No VarHandle:
Benchmark                                       Score   Error  Units
ByteBufAccessBenchmark.getByteBatch            82.388 ± 1.567  ns/op
ByteBufAccessBenchmark.getBytes                 8.145 ± 0.004  ns/op
ByteBufAccessBenchmark.getBytesConstantOffset   8.027 ± 0.012  ns/op
ByteBufAccessBenchmark.readByteBatch          101.012 ± 4.349  ns/op
ByteBufAccessBenchmark.readBytes               10.787 ± 0.038  ns/op
ByteBufAccessBenchmark.setByteBatch            61.919 ± 0.569  ns/op
ByteBufAccessBenchmark.setBytes                 6.167 ± 0.026  ns/op
ByteBufAccessBenchmark.setBytesConstantOffset   5.242 ± 0.021  ns/op

No Unsafe and VarHandle:
Benchmark                                      Score   Error  Units
ByteBufAccessBenchmark.getByteBatch           19.017 ± 0.017  ns/op
ByteBufAccessBenchmark.getBytes                3.719 ± 0.032  ns/op
ByteBufAccessBenchmark.getBytesConstantOffset  2.846 ± 0.017  ns/op
ByteBufAccessBenchmark.readByteBatch          21.370 ± 0.045  ns/op
ByteBufAccessBenchmark.readBytes               3.959 ± 0.016  ns/op
ByteBufAccessBenchmark.setByteBatch           17.956 ± 0.108  ns/op
ByteBufAccessBenchmark.setBytes                4.439 ± 0.017  ns/op
ByteBufAccessBenchmark.setBytesConstantOffset  3.532 ± 0.009  ns/op

In short we have now nearly the performance of Unsafe | Thanks for the ping, @franz1981. VarHandle performance is indeed a bit tricky to get right on Native Image. I have notified our experts and one of them should reach out shortly. | thnks @fniephaus 🙏 In the meantime I'm running few hotspot related benchmarks to make sure it performs as expected there | I have just noticed that checkBounds is making some difference in performance for VarHandle vs Unsafe which shouldn't really happen since byte[] accesses shouldn't have any bound check (for the former) - let's verify it
It was spotted at
Unsafe
Benchmark                                       Score   Error  Units
ByteBufAccessBenchmark.readByteBatch           19.493 ± 0.043  ns/op

VarHandle
Benchmark                                      Score   Error  Units
ByteBufAccessBenchmark.readByteBatch          21.370 ± 0.045  ns/op

which become the same if checkBounds=false meaning that is the bound check which impact it.
I'm preparing a fix | @fniephaus first and foremost I'm concerned about whether VH/AIFU will work at all, they'll need either successful static analysis or explicit reflection metadata. But looking at the perf would be good too | In the meantime I've sent #15495 to fix #15486 (comment) | @fniephaus first and foremost I'm concerned about whether VH/AIFU will work at all, they'll need either successful static analysis or explicit reflection metadata. But looking at the perf would be good too

@yawkat I agree with your concern. The operations performed by any VH that is created at run time will be mapped to reflective and/or unsafe access and thus will probably need reflection metadata.
Native Image tries to automatically fold as much as possible of those calls to constants at image build time.
E.g. it will fold the invocation MethodHandles.Lookup.findVarHandle(SomeClass.class, ""field"") and if that succeeds, reflective access is no longer necessary. But folding only works if arguments are constant.
Further, I'm not sure if folding will succeed as it is done in this change set because MethodHandles.Lookup.findVarHandle itself is invoked using a method handle (in VarHandleFactory.privateFindVarHandle).
Also, the arguments for findVarHandle won't certainly be seen as constants.
I'm not very familiar with netty but I would try to refactor that such that findVarHandle is called directly with constant args.
Performance wise, I fear the VH access will be rather slow.
For run-time created VHs, their lambda form will be interpreted  and the operations will mapped to reflective and/or unsafe operations.
However, Native Image would try to intrinsify any VH such that the access essentially boils down to the bare field access. But the intrinsification will only happen if the VH can be constant-folded at image build time.
I don't think this will succeed as it is written right now. | @fangerer

I'm not very familiar with netty but I would try to refactor that such that findVarHandle is called directly with constant args.

So if we had a
private static final Object REFCNT_FIELD_VH;
private static final ReferenceCountUpdater<AbstractReferenceCountedByteBuf> updater;

static {
     REFCNT_FIELD_VH = tryFindVarHandle(// need to find a proper logic which is using constants); 
     if (REFCNT_FIELD_VH != null) {
           updater = new VarHandleReferenceCountUpdater<AbstractReferenceCountedByteBuf>() {
                    @Override
                    protected VarHandle varHandle() {
                        return (VarHandle) REFCNT_FIELD_VH;
                    }
                };
     }
     // ... etc etc
}
this is going to work fine?
Do we have a chance to verify/test it? | I think the best way is to remove any abstraction that may impede compiler analysis.

Get rid of the Configuration abstraction, create VH directly:

static class VarHandleHolder {
    static final VarHandle VH = MethodHandles.lookup().findVarHandle(MyClass.class, ""refCnt"", int.class);
}

Guard any access to VarHandleHolder with an availability check. This should avoid initialization of the VarHandle as well when unsafe is available
Get rid of ReferenceCountUpdater so that VarHandle ops can be statically resolved and replaced with simple field access

I know that this will lead to some duplicate code, but given that ReferenceCountUpdater is only used by three classes, I think this is a good trade for easier optimization.
I don't think there's a good way to test this with graalvm unfortunately. | In order to have such MethodHandles methods I likely have to stub more stuff. The reason why I had a Object var handle field is because Java 8 will fail due to the prohibited package.
I can see if having the holder solve this
Fully removing any abstraction will really make a messy code there, with lot of duplication :/
Why having the three classes (varhandle/atomic/unsafe) cannot work if the decision can be taken via static analysis? | What exactly is the issue with Java 8? Is it a compile time issue or a runtime issue? If it's runtime only, encapsulating the VarHandle in its own nested class should be enough, since that class will never be loaded on Java 8.
The problem with having ReferenceCountUpdater as the abstraction is that it has at least one megamorphic call site (at least before inlining). This prevents replacing the VarHandle call with a simple field access (with the right semantics). JIT may be able to help here somewhat, but I doubt native image can do very much, so it may fall back on the interpreted VarHandle. It's safer to just not have the abstraction.
I agree that getting rid of ReferenceCountUpdater is not ideal, but is it really that big of a deal? It's a bit over 100 lines, and some of that could be extracted to static methods (just not the field access). Another alternative is franz1981#1 , which removes the need for megamorphic calls, but still doesn't duplicate any code – though it also has its costs. | The issue with Java 8 seems runtime since it would pick the stub in the class loader which has a restriction on the lang.invoke package (with JFR nope, for example), exploding.
i will give a proper more detailed answer tomorrow, where I will try other experiments.
Re

it has at least one megamorphic call site

Let me try to dissect it so I won't forget...
In theory the morphism depends by the observed receivers based on the call site and assuming a worst case without inlining.
In this regard, If we consider a world where only varhandle is used, and only the varhandle updater is used too, there are 3 anonymous subclasses of it (chunk, abstract ref cnt and another one).
But since the call sites are different, if all the chains of calls are inlineable, there should be a single type per each.
The problem (speculating here) is that if statically NI cannot specialize getRawRefCnt to use a single varHandle() implementation, it will fail to inline it.
In addition to it, in order to make varhandle effective (non reflective) it has to know clearly which class and field it is accessing, statically.
If I got it right I could do something similar to what you did in the pr you send @yawkat and fingers crossed, it should be enough.
I will check if I can do a variant too with a constant int switch using var handle/atomic/unsafe but my heart is bleeding while I write it 😭
I hope that the varhandles on byte[] are fine, WDYT @yawkat ? | Thinking about it twice

The problem (speculating here) is that if statically NI cannot specialize getRawRefCnt to use a single varHandle() implementation, it will fail to inline it.

But actually, in this PR I have used the VarHandle implementation only on the abstract ref cnt ByteBuf case which means that there will be a single implementation in that static final -> maybe this can make it to work? | it's still a gamble because the varHandle call is indirect via the varHandle() getter | @yawkat It seems that the stub fall shortly (with a compiler error) if I create a MethodHandles class in the stub defining  byteArrayViewVarHandle or privateLookupIn in order to save calling them reflectively.
The stub approach seems to work only for whole new classes not present before,
It means that the ``byteArrayViewVarHandle VarHandle` requires reflective accesses to be produced, likely hitting

Further, I'm not sure if folding will succeed as it is done in this change set because MethodHandles.Lookup.findVarHandle itself is invoked using a method handle (in VarHandleFactory.privateFindVarHandle).

what suggested by @fangerer
At this point I won't try to make VarHandle possible on native image, but the Unsafe and Atomic to work as expected - agree @yawkat ? | I have added d4f19c999d22723767b3d34926cea57e06556352
but I still lack a way to instruct NativeImage to just use Unsafe or the Atomic updater types since VarHandle is better to not use it - any idea how?
Consider @yawkat that this version is not that different from what we already have right now since there are still 3 subclasses of a chosen base class (e.g. Unsafe, Atomic  variants).
In order to unify all of them I would need a basic single RefCntHolder which Chunk, AbstractReferenceCountedByteBuf and AbstractReferenceCounted should inherit from (which is probably what you suggested in your PR).
I'll experiment with it in the next few hours, but at a first look it looks like that it would affect
AbstractDerivedByteBuf  (and children) object size, since it is a ReferenceCounted which extends AbstractByteBuf but doesn't contains a int refCnt per se, see:
io.netty.buffer.AbstractDerivedByteBuf object internals:
OFF  SZ   TYPE DESCRIPTION                         VALUE
  0   8        (object header: mark)               N/A
  8   4        (object header: class)              N/A
 12   4    int AbstractByteBuf.readerIndex         N/A
 16   4    int AbstractByteBuf.writerIndex         N/A
 20   4    int AbstractByteBuf.markedReaderIndex   N/A
 24   4    int AbstractByteBuf.markedWriterIndex   N/A
 28   4    int AbstractByteBuf.maxCapacity         N/A
Instance size: 32 bytes
Space losses: 0 bytes internal + 0 bytes external = 0 bytes total | Preferring Unsafe and Atomic by default seems to be a native image friendly tradeoff.
I think it would also be possible to initialize classes AbstractReferenceCounted and AbstractReferenceCountedByteBuf at build time which would solve the native image related issues as well. But this implies that netty users need a native image configuration which, I think, is not desired, right? @yawkat | What about Java 25+ when Unsafe becomes available? Is AIFU really as fast as VH then on native image? | In term of speed, I can shred some light:

usually NI CE is not able to inline ByteBuf calls due to the depth and width of morphism, which means that we don't really care if a non volatile accessibility check is available or not (e.g. bound checks are duplicated and the same for buffers field read)
on NI EE is a whole different story since it is able to inline it and having a missing non volatile check hit the performance issue which this same pr is solving for hotspot (and can be quite severe)

Note: the atomic updater is lacking methods to read plain/opaque the refCnt field
It means that lack of Unsafe is going to hit NI EE compared to hotspot in the measure shown in #15486 (comment) | I think this could be ready to go (I'll add some doc to the stub to explain what it does and how).
PTAL @yawkat
I still would like to guide native image to NOT pick VarHandle since we cannot allocate it without reflection | I still would like to guide native image to NOT pick VarHandle since we cannot allocate it without reflection

I've seen that netty already does
if (System.getProperty(""org.graalvm.nativeimage.imagecode"") != null)
in CleanerJava25. I think you could use this check as well. Or do I miss something? | Yep, so i can just disable VarHandle for JDK 25 included for NI, but beware my observation related EE 🙏 | wait - I've re-run the benchmarks and I'm not getting anymore the improvement on #15486 (comment)
need to investigate (drafting it again) | I've added a fix - it was a broken benchmark - now performance is as expected, see dfa88e72a908498f2f940eb7e5137c039fb952f4 | We can leave VarHandle be for NI for now, but the new Configuration-based AIFU initialization has the same problem that NI won't be able to infer the field it belongs to. | Why? When unsafe or VarHandle won't be available it should be known statically and the updater is allocated without reflection and using constants
Check the latest commit which perform the updater allocation in place | Ahh you're right, I thought it was a local method call, not a static import. | @franz1981 so this is ready for review ? | Yep @normanmaurer now adding one condition and some doc but yep | PTAL @normanmaurer added some docs in the varhandle stub to explain why it is how it is and what devs should do while using it | If it's ready to go it can unblock #15504  ptal @chrisvest or @normanmaurer | Thanks @chrisvest 🙏 so, ready to go? | @franz1981 squash and merge when happy with it | @normanmaurer remember, right now I have disabled var handle if unsafe is there, since is a more performant option, but for io_uring maybe you still want to use it even if unsafe is available 🙏 | @normanmaurer remember, right now I have disabled var handle if unsafe is there, since is a more performant option, but for io_uring maybe you still want to use it even if unsafe is available 🙏

Yep will do once this one is merged","Introduce VarHandle 9 stub

Motivation:

Unsafe unavailability prevent some basic JIT optimizations while accessing
buffers and reference counters, causing some pretty severe performance regression

Modifications:

Expose a VarHandle's 9 stub to enable Ja +9 to use it, if Unsafe is unavailable

Result:

Fixes #15485"
netty/netty,12791,https://github.com/netty/netty/pull/12791,Small updates to the cookie parsing,"Motivation:
While migrating improvements back to ServiceTalk, some small improvement possibilities were spotted.

Modification:
The cookie delimiter scan function that check for quotation marks to improve error messages, is now used in more places. The ""unexpected hex value"" error message now prefix the hex value with ""0x"" for clarity.

Result:
Better error reporting.",2022-09-10T06:46:09Z,normanmaurer,chrisvest,clarity,,"Small updates to the cookie parsing

Motivation:
While migrating improvements back to ServiceTalk, some small improvement possibilities were spotted.

Modification:
The cookie delimiter scan function that check for quotation marks to improve error messages, is now used in more places.
The ""unexpected hex value"" error message now prefix the hex value with ""0x"" for clarity.

Result:
Better error reporting."
netty/netty,11220,https://github.com/netty/netty/pull/11220,Decrease visibility of `Http2FrameCodecBuilder` default ctor to `protected`,"Motivation:

`Http2FrameCodecBuilder` defines static factory methods `forClient()`
and `forServer()` that should be used to create a new instance.
The default ctor is useful only when users need to override behavior
of the existing builder. Those users should define another way to create
an instance.

Modifications:

- Decrease visibility of `Http2FrameCodecBuilder` default ctor from
`public` to `protected`;
- Add javadoc to clarity responsibilities;

Result:

Users of `Http2FrameCodecBuilder` are not confused why
`new Http2FrameCodecBuilder().build()` works for the server-side, but
does not work for the client-side.

Follow-up for #11195.",2021-05-04T05:32:17Z,idelpivnitskiy,idelpivnitskiy,clarity,"Users of Http2FrameCodecBuilder are not confused why
new Http2FrameCodecBuilder().build() works for the server-side, but
does not work for the client-side.

@idelpivnitskiy not sure I get this, can you provide a sample code of what does not work?
The intent of the no-arg constructor here is to create a ""clean instance"" where no state is implicitly set as is the case with the Http2FrameCodecBuilder(boolean server). | For this type of the builder, the isServer flag is a required argument. If users do new Http2FrameCodecBuilder().build(), the default isServer impl will fallback to true, which is correct for the server-side, but may confuse users of the client-side.
It's not easy to use this ctor unless you know how to correctly initialize its state. This is true for users who want to extend the builder, but complicated for direct users of the builder. | The server(boolean) method is not exposed on the builder. Users of the default ctor don't have a way to set this flag unless they extend the builder. | I also cherry picked into master
…
 Am 04.05.2021 um 07:32 schrieb Idel Pivnitskiy ***@***.***>:

 ﻿
 Merged #11220 into 4.1.

 —
 You are receiving this because your review was requested.
 Reply to this email directly, view it on GitHub, or unsubscribe.","Decrease visibility of `Http2FrameCodecBuilder` default ctor to `protected`

Motivation:

`Http2FrameCodecBuilder` defines static factory methods `forClient()`
and `forServer()` that should be used to create a new instance.
The default ctor is useful only when users need to override behavior
of the existing builder. Those users should define another way to create
an instance.

Modifications:

- Decrease visibility of `Http2FrameCodecBuilder` default ctor from
`public` to `protected`;
- Add javadoc to clarity responsibilities;

Result:

Users of `Http2FrameCodecBuilder` are not confused why
`new Http2FrameCodecBuilder().build()` works for the server-side, but
does not work for the client-side."
netty/netty,10960,https://github.com/netty/netty/pull/10960,Introduce BrotliDecoder,"Motivation:

Netty lacks client side support for decompressing Brotli compressed response bodies.

Modification:

* Introduce optional dependency to brotli4j by @hyperxpro. It will be up to the user to provide the brotli4j libraries for the target platform in the classpath. brotli4j is currently available for Linux, OSX and Windows, all for x86 only.
* Introduce BrotliDecoder in codec module
* Plug it onto `HttpContentDecompressor` for HTTP/1 and `DelegatingDecompressorFrameListener` for HTTP/2
* Add test in `HttpContentDecoderTest`

Result:

Netty now supports decompressing Brotli compressed response bodies.",2021-05-10T13:25:24Z,normanmaurer,slandelle,clarity,"cc @hyperxpro | @normanmaurer @hyperxpro The PR fails randomly on Linux (first build failed on Java 8, second one failed on Java 11 while Java 8 was successful) with the following error:

2021-01-25T22:09:01.2229566Z 22:09:01.212 [main] DEBUG i.n.handler.codec.compression.Brotli - Failed to load brotli4j; Brotli support will be unavailable.
2021-01-25T22:09:01.2232104Z java.lang.UnsatisfiedLinkError: /tmp/com_aayushatharva_brotli4j_1234063592518/libbrotli.so: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /tmp/com_aayushatharva_brotli4j_1234063592518/libbrotli.so)

Do you guys have any idea? Is it an issue with GH Actions where hosts are inconsistent? It there any way to compile the native lib differntly? | Looks like a dependency error. @normanmaurer Help :D | @slandelle @hyperxpro we use Centos 6 on our CI setup to ensure the maximum compatibility. Your native artefacts were compiled on something which has GLIBC_2.29 which is a lot newer to what cantos 6 offers. For maximum compatibility you should ensure you compile it on a system with a very old GLIBC version. | I used GitHub Actions for building the native library. | @hyperxpro yeah this explains it as they use ubuntu 18 (as minimum). You will need to use something like docker to compile with centos6. Thats what we do | Just a comment: there is no chance to get a JNI-free pure Java impl? | Just a comment: there is no chance to get a JNI-free pure Java impl?

Nope, not yet. | Just a comment: there is no chance to get a JNI-free pure Java impl?

The java implementation has been lagging behind for a very long time. Last release was in May 2017. | @slandelle

The java implementation has been lagging behind for a very long time. Last release was in May 2017.

Recently I see the good trend to get some JNI code turned into Java one, even while accessing native resources and I like it, for many reasons.
I'm looking on https://github.com/google/brotli/tree/master/java/org/brotli if is pure-Java or not... | @franz1981 franz1981
Assuming similar levels of performance and maintenance, I agree I'd rather have a pure Java implementation than a JNI one.
Sadly, so far:

the Brotli team hasn't been actively maintaining the pure Java implementation, on contrary to the C one.
I've never seen a benchmark comparing the performance of the Java based implementation. I would expect the latter to be implemented mostly for the sake of the client side on Android, where performance is not really an issue (low throughput per client instance). | @normanmaurer I've just addressed your comments, thanks!
@hyperxpro The ball is now in your court :) Please chime in once you've fixed the glibc issue (sorry, I can't help with that). | @normanmaurer CentOS 6 has officially reached EOL. You sure we need to compile Linux binary on CentOS 6? | @hyperxpro while it is EOL I still think it is a good idea for max compatibility | @hyperxpro while it is EOL I still think it is a good idea for max compatibility

After some digging, I second @normanmaurer here: the glibc version on the latest Amazon Linux 2 is 2.26, so we definitely can't require 2.29. | Also, this is targeting 4.1, and I don't think we can change the compatibility baseline in a patch release. | What's the GLIBC version I need to compile this with? | @chrisvest
WDYM? This is a new and optional feature. | What's the GLIBC version I need to compile this with?

From distrowatch, CentOS 6 has glibc 2.12 while CentOS 7 has glibc 2.17. | Yes just use cents 6 which uses glib 2.12... OpenJDK compiles against and old GLIBC for the same reasons as well. | @normanmaurer Would you mind helping me build? My docker experience is almost null.
:-| | @hyperxpro sorry I am super busy atm so I can't help a lot here. But basically this should give you some idea:
https://github.com/netty/netty/blob/4.1/docker/Dockerfile.centos6
https://github.com/netty/netty/blob/4.1/docker/docker-compose.yaml
https://github.com/netty/netty/blob/4.1/docker/docker-compose.centos-6.18.yaml | I give up on Docker. WSL 2 is making me cry. @slandelle Can you help in building the binary of Linux? | @slandelle Go ahead with this. I don't have plans to downgrade the GLIBC version. Brotli4jLoader#isAvailable call should be enough. | @hyperxpro I'll try to have someone at Gatling take a look next week at the Docker/GH Action configuration.
As is:

this PR can't be merged as it doesn't pass Netty's CI
brotli4j doesn't work on current major distributions, including Amazon Linux 2. From my (Gatling) PoV, that's a major blocker. | I agree with your point. I also saw some issues with RHEL 8.X. | Some update on this: we (at Gatling) can't manage to compile the original Brotli code on CentOS6 (glibc 2.12.1, cmake >= 3.0.0 => nullptr). We plan on contributing fixes to brotli4j so that Linux binaries work on widespread modern distributions (eg current Amazon Linux) and keep on integrating on our side.
As for contributing upstream to Netty, until Netty revisits its CentOS compatibility requirement, the only solution I see is to disable Brotli related tests on linux-x86_64.
Thoughts? | @slandelle Agree with your points. Also, please do the PR for Brotli4j. | @hyperxpro Yeah, we're working on this. We're still working on the CentOS docker image as it's still quite large. In short, brotli is really not developed with old platforms such as CentOS 6 in mind. I really hope Netty will revisit this requirement in a future version. | Just updated PR with brotli4j 1.4.0, which is compatible with CentOS 6. | @slandelle Do you have plans for Brotli Encoder or I should add it to my list? | Do you have plans for Brotli Encoder or I should add it to my list?

@hyperxpro no, no plans, sorry. In Gatling, we're mainly focused on the client side, so it's very unlikely we'll ever work on the encoder ourselves. And we're still working on trying to make the binary work on CentOS6. | Do you have plans for Brotli Encoder or I should add it to my list?

@hyperxpro no, no plans, sorry. In Gatling, we're mainly focused on the client side, so it's very unlikely we'll ever work on the encoder ourselves. And we're still working on trying to make the binary work on CentOS6.

Alright. I will do PR for the encoder after we're done fixing the binary issue and merger of this PR. | Error:  COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
Error:  /home/runner/work/netty/netty/codec-http/src/test/java/io/netty/handler/codec/http/HttpContentDecoderTest.java:[225,33] error: unreported exception Throwable; must be caught or declared to be thrown
[INFO] 1 error
[INFO] ------------------------------------------------------------- | ByteBuf leak ;/ @slandelle | @normanmaurer Please hold reviews for now. I'm investigating a test failure I'm having a hard time reproducing locally. I have to stay the test suite is very weird, with  static AbstractDecoderTest#compressedBytesLarge actually loaded from ctor. This seems to be some JUnit hack, but this hack crashes with NPE when trying to run one single test, eg mvn -Dtest=io.netty.handler.codec.compression.BrotliDecoderTest test. | @slandelle let me know when I should take a look again | @normanmaurer I'm waiting for this build to pass (looks promising) and then I'll push some clean ups.
I gave up on extending AbstractDecoderTest whose initialization and execution lifecycle I don't trust and that was most likely causing the test failures since this morning.
Do you want me to squash the commits right now for clarity? | @slandelle yep squashing sounds good... Also please open an issue for the ""test init stuff"" and explain what the problem is. Seems like we want to fix that as well | Finally, this PR closes #6899. | @normanmaurer squashed | CI pipeline's still not done after 3hrs??? | @slandelle seems so... it is still ""spinning"" but I also don't get a console output. Maybe I need to bounce it again | GitHub Actions terminal is buggy. Can't see any console output. | Looks like stars are finally aligned!!! | @idelpivnitskiy please have a final look as well | Also, lots of credit go to @ericmolle who did an amazing work for building a toolchain for compiling brotli on CentOS6! | So, is this ready to merge? @normanmaurer | @slandeelle @ericmolle and @hyperxpro thanks so much for doing this ! | Awesome, thanks a lot to @idelpivnitskiy and you for the reviews!
Do you have any ETA for the release so we can plan on our side and remove the duplicated code? | @slandelle end of this week | @normanmaurer Can you give call-out on release page regarding Brotli4j? It'll help people get insight of 'How to use Brotli with Netty?'.","Introduce BrotliDecoder

Motivation:

Netty lacks client side support for decompressing Brotli compressed response bodies.

Modification:

* Introduce optional dependency to brotli4j by @hyperxpro. It will be up to the user to provide the brotli4j libraries for the target platform in the classpath. brotli4j is currently available for Linux, OSX and Windows, all for x86 only.
* Introduce BrotliDecoder in codec module
* Plug it onto `HttpContentDecompressor` for HTTP/1 and `DelegatingDecompressorFrameListener` for HTTP/2
* Add test in `HttpContentDecoderTest`
* Add `BrotliDecoderTest` that doesn't extend `AbstractDecoderTest` that looks flaky

Result:

Netty now support decompressing Brotli compressed response bodies."
netty/netty,16053,https://github.com/netty/netty/pull/16053,Buddy allocation for large buffers in adaptive allocator,"Motivation:

The histogram bump allocating chunks have poor chunk and memory reuse in practice, which leads to higher memory usage than the pooled allocator whenever an application performs enough allocations of buffers that don't fit in our size-classed chunks.

Buddy allocation should in theory reduce memory consumption by allowing memory reuse within a chunk, similar to the size-classed chunks, but for variable power-of-two sized allocations.

We've found that beyond 16k-20k buffer sizes, allocations predominantly comes in power-of-two sizes, hence buddy allocation should be a good fit for this size regime.

Modification:

* Implement a new chunk type that does buddy allocation, based on an array-embedded binary search tree.
* The tree is encoded as a dense byte array, with two bits marking node or child-node usage, and six bits to encode the node size.
* The histogram pointer-bump allocating chunk implementation is removed, which unlocks potential simplifications and optimizations that will benefit both buddy and size-classed chunks.
* The 32k and 64k size classes are kept for the time being, to keep chunk churn under control, but they are planned to be removed in a follow-up PR.

Result:

We generally get improvements to memory usage, because the buddy allocator is able to reuse its chunks before they are fully deallocated.
If the 32k and 64k size-classes are removed, then the improvements continue to hold up, but we see an increase in allocation churn for buddy chunks.
This needs to be investigated and solved before we can remove the 32k and 64k size-classes.
Presumably, it comes down to making better decisions about the size of the buddy chunks, and in picking which chunks to allocate from next once a magazine has exhausted its current chunk.",2026-01-12T22:32:11Z,chrisvest,chrisvest,easier to read,"With the fixes to the freeing code, the buddy allocator is now both faster and use less memory (no longer accidentally keeping memory marked as claimed). However, it's not nearly enough to make up for the removal of the 32k and 64k size classes.
Especially in cases where there's high buffer retention - many buffers alive at the same time. In that case, we end up with many chunks and make poor decisions reusing them because we just a queue per magazine group.
FYI @franz1981 | Here's a simulation run with the e-commerce data:

If I comment out the largeBufferMagazineGroup path, so large buffers are unpooled instead of using the buddy allocator for pooling, then memory usage is cut in half, which gives us a picture of how far away from the limit we are. | @franz1981 I think it'd be better to do chunk picking in a separate PR, so I brought back the 32k and 64k size classes. Those bring the memory usage back to previous levels. I marked this PR ready for review. | @franz1981 @normanmaurer Added one more commit where I fixed a missing piece that was causing the increased memory usage seen in #16053 (comment)
Like the SizeClassedChunks, the BuddyChunks can of course be reused before all their buffers have been freed.
This means it's now technically possible to remove the 32k and 64k. However, doing so introduces a fair bit of chunk churn, as can be seen in this chart:

Getting the churn under control, and making smarter picking decisions, is what I'll work on next in a separate PR. | @chrisvest don't we also want to cherry-pick to 4.1 ? | Yeah, I'll add it.","First draft of implementing buddy allocation in adaptive for large buffers | Remove the histogram-based bump allocating chunk implementation | Fix a bug in buddy allocation computing incorrect offset for the right side of any subtree | Fix a couple of bugs in the buddy allocation | More accurate remaining capacity for buddy allocation

This is temporary. We should do something faster than this. | Add a data consistency test targeting the buddy allocator | Fix a problem in the buddy allocator

Releasing could end up iterating past its sibling pair, so where releasing a node could end up updating a parent path that wasn't a parent of the released node. | Fix a problem in the buddy allocator

When releasing parent nodes we were only considering the siblings one level down, not whether any other child of a given node had been claimed.
This could lead to grand-parents getting marked as free, if we returned from a child that had a free sibling.
Now we return whether the given subtree is free and only consider the sibling if so. | Remove the Graphviz DOT generating functions

They were used for debugging and are no longer needed. | Merge branch '4.2' into 4.2-buddy-alloc | Merge branch '4.2' into 4.2-buddy-alloc | Bring back the 32k and 64k size classes

I think it might be better to look at that problem in a separate follow-up PR.
Don't want each PR to get too big. | Merge branch '4.2' into 4.2-buddy-alloc

# Conflicts:
#	buffer/src/main/java/io/netty/buffer/AdaptivePoolingAllocator.java | Address review comments | BuddyChunks can be released directly back to the group queue

Just like SizeClassedChunks, the BuddyChunks can be released directly back to the shared pool in the magazine group, without waiting for the chunk to be fully freed.
This allows it to be reused much sooner, and reduces memory usage.

This is technically enough to remove the 32k and 64k size classes.
However, if we do that, then it would currently cause an increase in chunk churn, where chunks are allocated and released a lot.
The churn needs to come under control, before those two size classes can be removed."
netty/netty,15974,https://github.com/netty/netty/pull/15974,ChannelOutboundHandler: Remove throws Exception from method signature,"Motivation:

ChannelOutboundHandler methods take a ChannelPromise which should be used to report errors and so it makes no sense to have throws Exception on the method signature.

Modifications:

- Remove throws Exception from method signature
- Adjust code to correctly handle exceptions
- log warn message and close channel if an outbound operation throws an exception but there is not way to report the error (flush and read).

Result:

API cleanup and make clear how errors should be reported back",2025-12-04T20:03:41Z,normanmaurer,normanmaurer,easier to read,,"ChannelOutboundHandler: Remove throws Exception from method signature

Motivation:

ChannelOutboundHandler methods take a ChannelPromise which should be used to report errors and so it makes no sense to have throws Exception on the method signature.

Modifications:

- Remove throws Exception from method signature
- Adjust code to correctly handle exceptions
- log warn message and close channel if an outbound operation throws an exception but there is not way to report the error (flush and read).

Result:

API cleanup and make clear how errors should be reported back | Merge branch '5.0' into outbound_ex"
netty/netty,15676,https://github.com/netty/netty/pull/15676,bump netty5 java version requirement to 25,"Motivation:
With the addition of MemorySegment into the JDK the Unsafe and ByteBuffer based Buffer implementations are no longer needed. Due to a bug on jdk versions before 25 the nio buffers created from memory segments cannot be used for certain operations, for example compression, making the Unsafe/ByteBuffer implementations a requirement when running on versions 22-24 (with the downside that Unsafe will print warnings upon use and the ByteBuffer lifetime not being controlled by netty). Given these reasons it seems to make sense to bump to 25, which would allow to remove the Unsafe/ByteBuffer implementations in a subsequent step as the MemorySegment based buffer is all that's needed.

Modification:
Bump the jdk version used for building to 25 and the compile target to 25.

Result:
Netty 5 requires java 25 instead of 22.",2025-09-25T22:00:24Z,chrisvest,derklaro,easier to read,"Note that graalce and amazonjdk 25 are not yet available via sdkman, therefore these 2 builds are failing. | Java 25 support in CodelQL hasn't been released yet, should be available in a few days | This is the right thing to do. | CodeQL 2.23.1, which adds Java 25 support, was tagged an hour ago. The release will be out very soon, I think. | GraalVM 25 have FFM support, memory, downcall, and upcall: oracle/graal#8113 (comment) | Thanks! | Have there been any experiments so far if the removal of the Unsafe based implementations can be done without major performance degradation for Netty? | Hi @thomaswue we didn't yet tested with native image, but I am working on improving the situation on Hotspot side (via VarHandle and other mechanisms), for 4.2 (not 5). | I'm also curious about the performance implications. In my experiments with using MemorySegment for random unchecked memory access the performance was unacceptable compared to sun.misc.Unsafe. Significantly slower with the simple access methods, and another order of magnitude slower with VarHandles (which are required for any non-opaque access modes). Admittedly, this was a few JVM versions ago. I'm also still using ByteBuffers for native memory buffer reads in perfIO due to performance reasons (as benchmarked on OpenJDK 23). | @franz1981 What is the current situation for HotSpot? Are there any existing profiling results available that would help me identify hot spots (no pun intended ;-)) of the code? | @szeiger I have already pushed a few changes in Netty 4.2 to address what I have found which was a mix of repeated bound checks, bound checks involving computations which was bad (untaken branches are just better) and other stuff like that.
I haven't yet spent time on MemorySegment and on Netty 4.2 we are using VarHandle byteBufferView or byteArrayView (latter better performing on batch accesses).
Another reason why it was bad is because without Unsafe, the ref count check was performing a volatile load, which causes excessive loads and repeated bound checks, losing the chance to hoist loading order or the base address etc etc
I have implemented a VarHandle variant of that, which enable using getOpaque, but IDK the status of this is Netty 5 since ref cnt is not used.
Bound checks instead, are still performed, unless @chrisvest has ported any of my patches | I haven't ported patches. The Netty 5 branch is running behind 4.2, so at some point we'll have to put in some concerted effort to bring it up to speed. | I can take a look what I could bring there too 🙏 ok @chrisvest ? | @thomaswue we can talk about it, we have nothing OSS yet, especially running on stable hardware
I will write you an email 🙏 | @franz1981 Sounds good. Mind you, the adaptive allocator in main in particular is wayyy behind 4.2, so you may want to navigate around that and pick easier targets.",bump required java version to 25 | bump shade plugin to 3.6.1 | force jdependency version | skip osgi | trigger ci (graal-ce should be available now) | .22 -> .25
netty/netty,15472,https://github.com/netty/netty/pull/15472,Use better method in HttpContentDecoder and simplify checks in HttpContentEncoder,"Motivation:

Just a cleanup.

Modification:

- Replaced `String.indexOf(string)` with `String.indexOf(char)` in `HttpContentDecoder`
- Simplified checks in `HttpContentEncoder`

Result:

Easier to read code.
",2025-07-11T07:04:46Z,normanmaurer,doom369,easier to read,@doom369 thanks,use better method in HttpContentDecoder | Merge branch '4.2' into use_better_method
netty/netty,15353,https://github.com/netty/netty/pull/15353,Simplify code in JdkZlibDecoder/Encoder,"Motivation:

JdkZlibDecoder/Encoder uses many manual `readByte()` calls, which could be replaced with a single `readIntLE` and alternative commands.

Modification:

Replace multiple `readByte()` commands with a single `readIntLE`

Result:

Code is easier to read and more effective, + added a bunch of tests.",2025-06-18T16:59:28Z,normanmaurer,doom369,easier to read,"@chrisvest @franz1981 can you have a look as well | My only concern here is that the existing code:
        // read ISIZE and verify
        int dataLength = 0;
        for (int i = 0; i < 4; ++i) {
            dataLength |= in.readUnsignedByte() << i * 8;
        }

reads dataLength as int while it should be unsigned int:

ISIZE (Input SIZE)
This contains the size of the original (uncompressed) input
data modulo 2^32.

I guess nobody decompresses large inputs, so it was never a case in real code.",simplify code in JdkZlibDecoder/Encoder | use lambda | Merge branch '4.2' into simplications_in_jdk_zlib_decoder
netty/netty,15221,https://github.com/netty/netty/pull/15221,Wrap streams into try/catch blocks,"Motivation:

Netty now supports Java 8+, so we don't have to close streams manually.

Modification:

Wraps all Input and Output streams into try/catch blocks. Removed manual `close() `calls.
Also removed `close()` calls for `ByteArrayInputStream` and `ByteArrayOutputStream` as this is not necessary, as they don't hold any resources and will be GCed.

Result:

Fewer `final` sections, easier to read.",2025-06-19T08:32:40Z,normanmaurer,doom369,easier to read,"@doom369 seems like there are some test-failures. PTAL | @normanmaurer fixed. | @chrisvest thanks! fixed. | Its getting some test failures with illegal reference counts, which sounds like it could be related. | @chrisvest looks like random failure | @doom369 thanks",wrap streams into try/catch blocks | checkstyle fix | checkstyle fix 2 | checkstyle fix 3 | checkstyle fix 4 | Merge branch '4.2' into wrap_streams_in_try_catch | revert changes for SelfSignedCertificate | Merge branch '4.2' into wrap_streams_in_try_catch | Merge branch '4.2' into wrap_streams_in_try_catch | address comments | Merge branch '4.2' into wrap_streams_in_try_catch | Merge branch '4.2' into wrap_streams_in_try_catch | Merge branch '4.2' into wrap_streams_in_try_catch | Merge branch '4.2' into wrap_streams_in_try_catch | Merge branch '4.2' into wrap_streams_in_try_catch
netty/netty,15253,https://github.com/netty/netty/pull/15253,Remove regexp usage from HttpPostRequestEncoder,"Motivation:

Regexp can be easily replaced with custom code. Makes code easier to read and simpler.

Modification:

Regexp replaced with simple `replace` methods chain.

Result:

No more regexp usage in `HttpPostRequestEncoder`
",2025-05-23T23:36:41Z,chrisvest,doom369,easier to read,"@Fork(value = 1)
@BenchmarkMode(Mode.AverageTime)
@State(Scope.Benchmark)
@Warmup(iterations = 3)
@Measurement(iterations = 3)
@OutputTimeUnit(TimeUnit.NANOSECONDS)
public class PercentEncodingBenchmark {
    private static final Map.Entry<Pattern, String>[] percentEncodings = new Map.Entry[] {
            new AbstractMap.SimpleImmutableEntry<>(Pattern.compile(""\\*""), ""%2A""),
            new AbstractMap.SimpleImmutableEntry<>(Pattern.compile(""\\+""), ""%20""),
            new AbstractMap.SimpleImmutableEntry<>(Pattern.compile(""~""), ""%7E"")
    };

    private static final String ASTERISK = ""*"";
    private static final String PLUS = ""+"";
    private static final String TILDE = ""~"";
    private static final String ASTERISK_REPLACEMENT = ""%2A"";
    private static final String PLUS_REPLACEMENT = ""%20"";
    private static final String TILDE_REPLACEMENT = ""%7E"";

    @Param({
            """",
            ""hello_world"",
            ""hello+world*~ok"",
            ""hello world ok hello world ok hello world ok hello+world ok hello world ok hello world ok hello world ok""
    })
    private String rawInput;

    private String encoded;

    @Setup(Level.Trial)
    public void setup() throws Exception {
        encoded = URLEncoder.encode(rawInput, StandardCharsets.UTF_8);
    }

    @Benchmark
    public String regexBased() {
        String result = encoded;
        for (Map.Entry<Pattern, String> entry : percentEncodings) {
            result = entry.getKey().matcher(result).replaceAll(entry.getValue());
        }
        return result;
    }

    @Benchmark
    public String stringReplaceChain() {
        return encoded
                .replace(ASTERISK, ASTERISK_REPLACEMENT)
                .replace(PLUS, PLUS_REPLACEMENT)
                .replace(TILDE, TILDE_REPLACEMENT);
    }
    
}

Benchmark                                                                                                                                    (rawInput)  Mode  Cnt    Score    Error  Units
PercentEncodingBenchmark.regexBased                                                                                                                      avgt    3   70.173 ± 10.470  ns/op
PercentEncodingBenchmark.regexBased                                                                                                         hello_world  avgt    3   77.232 ±  3.408  ns/op
PercentEncodingBenchmark.regexBased                                                                                                     hello+world*~ok  avgt    3  114.883 ± 11.056  ns/op
PercentEncodingBenchmark.regexBased            hello world ok hello world ok hello world ok hello+world ok hello world ok hello world ok hello world ok  avgt    3  508.603 ± 24.402  ns/op
PercentEncodingBenchmark.stringReplaceChain                                                                                                              avgt    3    0.524 ±  0.007  ns/op
PercentEncodingBenchmark.stringReplaceChain                                                                                                 hello_world  avgt    3    6.633 ±  0.522  ns/op
PercentEncodingBenchmark.stringReplaceChain                                                                                             hello+world*~ok  avgt    3   28.584 ±  3.169  ns/op
PercentEncodingBenchmark.stringReplaceChain    hello world ok hello world ok hello world ok hello+world ok hello world ok hello world ok hello world ok  avgt    3  225.727 ±  5.096  ns/op | Try running the PlatformDependent clinit with many forks and OneShot measurement type if you have to measure ""cold"" init improvements | @franz1981 I did. The funniest thing is - PlatformDependent.logger initialization takes the most | @franz1981 @normanmaurer LoggerFactory.getILoggerFactory() in Slf4JLoggerFactory(boolean) constructor is the most time-consuming operation in the PlatformDependent init process. How do you think - can we remove that check? It doesn't make sense from my point of view.",remove regexp from HttpPostRequestEncoder | Merge branch '4.2' into remove_regeexp_fromhttpostencoder
netty/netty,15610,https://github.com/netty/netty/pull/15610,Improve Recycler's local pooling,"Motivation:

Existing Recycler has bad performance in the thread-local case

Modification:

Allow Recycler to drop fast-thread-local usage and enable fully unguarded recycles i.e. no wrapper handling

Result:

Fixes #15604

- [x] Implement correctly https://github.com/netty/netty/pull/15610#issuecomment-3258147192
- [x] Add tests for missing recycler types
- [x] Implementing a mechanism to allow pinned recyclers to gc their owner + tests
- [ ] Improve performance for the #15419 
",2025-10-02T06:50:05Z,normanmaurer,franz1981,easier to read,"FYI @georgebanasios | Some benchmark numbers:
Benchmark                                                                 (fastThreadLocal)  (unguarded)  Mode  Cnt    Score   Error      Units
RecyclerBenchmark.recyclerGetAndUnguardedRecycle                                      false         true  avgt   20    3.697 ± 0.031      ns/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:CPI                                  false         true  avgt    2    0.210          clks/insn
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:IPC                                  false         true  avgt    2    4.755          insns/clk
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:L1-dcache-load-misses                false         true  avgt    2    0.003               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:L1-dcache-loads                      false         true  avgt    2   32.647               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:L1-icache-load-misses                false         true  avgt    2   ≈ 10⁻⁴               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:L1-icache-loads                      false         true  avgt    2    0.043               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:branch-misses                        false         true  avgt    2    0.004               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:branches                             false         true  avgt    2   19.577               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:cycles                               false         true  avgt    2   19.990               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:dTLB-load-misses                     false         true  avgt    2   ≈ 10⁻⁵               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:dTLB-loads                           false         true  avgt    2   ≈ 10⁻⁴               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:iTLB-load-misses                     false         true  avgt    2   ≈ 10⁻⁴               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:iTLB-loads                           false         true  avgt    2   ≈ 10⁻⁴               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:instructions                         false         true  avgt    2   95.068               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:stalled-cycles-frontend              false         true  avgt    2    0.104               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle                                       true         true  avgt   20    4.430 ± 0.072      ns/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:CPI                                   true         true  avgt    2    0.218          clks/insn
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:IPC                                   true         true  avgt    2    4.587          insns/clk
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:L1-dcache-load-misses                 true         true  avgt    2    0.004               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:L1-dcache-loads                       true         true  avgt    2   38.730               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:L1-icache-load-misses                 true         true  avgt    2   ≈ 10⁻⁴               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:L1-icache-loads                       true         true  avgt    2    0.051               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:branch-misses                         true         true  avgt    2    0.005               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:branches                              true         true  avgt    2   23.635               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:cycles                                true         true  avgt    2   24.029               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:dTLB-load-misses                      true         true  avgt    2   ≈ 10⁻⁴               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:dTLB-loads                            true         true  avgt    2   ≈ 10⁻³               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:iTLB-load-misses                      true         true  avgt    2   ≈ 10⁻⁴               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:iTLB-loads                            true         true  avgt    2   ≈ 10⁻⁴               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:instructions                          true         true  avgt    2  110.240               #/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle:stalled-cycles-frontend               true         true  avgt    2    0.131               #/op

vs a fully local one:
Benchmark                                                             (unguarded)  Mode  Cnt   Score   Error      Units
RecyclerBenchmark.localRecyclerGetAndRecycle                                false  avgt   20   2.992 ± 0.011      ns/op
RecyclerBenchmark.localRecyclerGetAndRecycle:CPI                            false  avgt    2   0.221          clks/insn
RecyclerBenchmark.localRecyclerGetAndRecycle:IPC                            false  avgt    2   4.521          insns/clk
RecyclerBenchmark.localRecyclerGetAndRecycle:L1-dcache-load-misses          false  avgt    2   0.001               #/op
RecyclerBenchmark.localRecyclerGetAndRecycle:L1-dcache-loads                false  avgt    2  26.143               #/op
RecyclerBenchmark.localRecyclerGetAndRecycle:L1-icache-load-misses          false  avgt    2  ≈ 10⁻⁴               #/op
RecyclerBenchmark.localRecyclerGetAndRecycle:L1-icache-loads                false  avgt    2   0.021               #/op
RecyclerBenchmark.localRecyclerGetAndRecycle:branch-misses                  false  avgt    2   0.001               #/op
RecyclerBenchmark.localRecyclerGetAndRecycle:branches                       false  avgt    2  13.074               #/op
RecyclerBenchmark.localRecyclerGetAndRecycle:cycles                         false  avgt    2  16.008               #/op
RecyclerBenchmark.localRecyclerGetAndRecycle:dTLB-load-misses               false  avgt    2  ≈ 10⁻⁵               #/op
RecyclerBenchmark.localRecyclerGetAndRecycle:dTLB-loads                     false  avgt    2  ≈ 10⁻⁴               #/op
RecyclerBenchmark.localRecyclerGetAndRecycle:iTLB-load-misses               false  avgt    2  ≈ 10⁻⁵               #/op
RecyclerBenchmark.localRecyclerGetAndRecycle:iTLB-loads                     false  avgt    2  ≈ 10⁻⁴               #/op
RecyclerBenchmark.localRecyclerGetAndRecycle:instructions                   false  avgt    2  72.378               #/op
RecyclerBenchmark.localRecyclerGetAndRecycle:stalled-cycles-frontend        false  avgt    2   0.050               #/op

The number of instruction for the fastThreadLocal = false unguarded = true case is still higher than what we have in the local case, but I think that it depends by the additional checks and re-read of fields caused by the volatile fields in the LocalPool. I have to check if there's something we can do about it.
Let me know @georgebanasios if you can use the non fast thread local and unguarded version of Recycler for the Adaptive PR.
Using it it should be as simple as new Recycler(Thread ownerThread, boolean unguarded) stored in the magazine's field (or magazine group if we know is both pinned and uncontended): we're now ~25% slower than it. | The error at RecyclerTest it's because during the last refactoring I forgotten to add the max cached capacity and virtual thread checks in the recycler | Fixed test at f2726c8
I have to re-run the numbers of the fastThreadLocal = true case since we now add few more checks (I could simplify one of them). | @franz1981 This is what I get on the Adaptive PR:
Benchmark                                               (allocatorType)  (pollutionIterations)   Mode  Cnt         Score         Error  Units
ByteBufAllocatorAllocPatternBenchmark.directAllocation         ADAPTIVE                      0  thrpt   20  46162179.095 ±  537594.339  ops/s
ByteBufAllocatorAllocPatternBenchmark.directAllocation         ADAPTIVE                 200000  thrpt   20  41884913.064 ± 1195521.191  ops/s | Can you share the branch @georgebanasios ?
Just to make sure how you have allocated the recycler 🙏 | Yeah ofc, here: georgebanasios@736e4d0.
Is that wrong? per thread | It looks fine but there is a subtle difference with what you have
I.e. the local pool is used on release just till it won't exceed 512 which is the chunk size: there is a configured sys property to add to the benchmark via --jvmArgs to make it as large as what you got for yours | I expect that the baseline for #15610 (comment) before georgebanasios@736e4d0 is in the 62M op/s right? | I expect that the baseline for #15610 (comment) before georgebanasios@736e4d0 is in the 62M op/s right?

I changed the chunkSize to 512/1024, getting the same results. I'll test again.
Yes the baseline I would say is more closer to 64M ops/s atm. | It's rather suspicious, since the actual code should produce a similar assembly than the manual one, apart from chunk size, few volatile load and the indirection level.
I can fix the volatile load somehow, but need to profile your branch first

I changed the chunkSize to 512/1024

IDK how much you were allowing to cache in the existing version before my change.
In the original version I did it was unbounded and it was growing as much as needed... | In some previous comment I wrote that

we're now ~25% slower than it.

So actually 64×75% = 48
If we assume that the most relevant effect was the recycler, it is in the expected range, sigh.
But it is not what I was expecting tbh, to be THAT relevant - but just a part of the performance story
And indeed 64 M ops/sec is 15.625 ns: even if performance is not composable (you cannot just sum/diff cost of using the new recycler vs raw, which was 0.7 ns/op) the performance effect just look too large and requires some deeper analysis. | The previous tests I've run were on JDK 21 - with JDK 24:
Benchmark                                                                 (fastThreadLocal)  (unguarded)  Mode  Cnt    Score   Error      Units
RecyclerBenchmark.recyclerGetAndUnguardedRecycle                                      false         true  avgt   20    3.391 ± 0.084      ns/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle                                       true         true  avgt   20    5.263 ± 0.015      ns/op

the  latter is a much worse number than what reported at #15610 (comment) fir the fastThreadLocal case.
I have to investigate why... | So, to summarize
before these changes, JDK 21 (and JDK 24 too):
Benchmark                                         Mode  Cnt  Score   Error  Units
RecyclerBenchmark.recyclerGetAndUnguardedRecycle  avgt   20  5.510 ± 0.021  ns/op

at f2726c8 - with JDK 21:
Benchmark                                         (fastThreadLocal)  (unguarded)  Mode  Cnt  Score   Error  Units
# this can be compared to the original one
RecyclerBenchmark.recyclerGetAndUnguardedRecycle               true        false  avgt   20  5.189 ± 0.025  ns/op
# these are all additional ways
RecyclerBenchmark.recyclerGetAndUnguardedRecycle              false         true  avgt   20  3.622 ± 0.013  ns/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle              false        false  avgt   20  4.390 ± 0.124  ns/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle               true         true  avgt   20  4.458 ± 0.008  ns/op

Whilst with JDK 24:
Benchmark                                         (fastThreadLocal)  (unguarded)  Mode  Cnt  Score   Error  Units
RecyclerBenchmark.recyclerGetAndUnguardedRecycle               true        false  avgt   20  5.634 ± 0.206  ns/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle              false         true  avgt   20  3.317 ± 0.013  ns/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle              false        false  avgt   20  3.968 ± 0.007  ns/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle               true         true  avgt   20  5.622 ± 0.011  ns/op

The new version has a severe performance hit (still within original performance) regardless been unguarded - which is surprising - and which seems dependent by fastThreadLocal = true only. | The most relevant difference between JDK 21 and 25 for fastThreadLocal = true and JDK 25 contains many register spills:
mov    (%rsp),%r14
mov    0x78(%rsp),%rax
mov    0x10(%rsp),%rdx
mov    0x70(%rsp),%rcx
and
mov    %rbp,0x8(%rsp)
mov    %rcx,0x70(%rsp)
mov    %rdx,0x10(%rsp)
mov    %rax,0x78(%rsp)
mov    %r14,(%rsp)
which looks to delimit the benchmark loop body: this should (to verify) happen to pass state (invariant?) across loop iterations due to register pressure (after the spill)
            0x00007fb6a81e5e34:   mov    (%rsp),%r14
            0x00007fb6a81e5e38:   mov    0x78(%rsp),%rax
   0.03%    0x00007fb6a81e5e3d:   mov    0x10(%rsp),%rdx
            0x00007fb6a81e5e42:   mov    0x70(%rsp),%rcx              ;*aload_1 {reexecute=0 rethrow=0 return_oop=0}
            0x00007fb6a81e5e47:   mov    0x14(%rax),%r13d             ;*getfield recycler {reexecute=0 rethrow=0 return_oop=0}
   0.09%    0x00007fb6a81e5e4b:   mov    0x10(%r12,%r13,8),%r11d      ;*getfield localPool {reexecute=0 rethrow=0 return_oop=0}
   0.03%    0x00007fb6a81e5e50:   test   %r11d,%r11d
            0x00007fb6a81e5e53:   jne    0x00007fb6a81e62c4           ;*ifnull {reexecute=0 rethrow=0 return_oop=0}
shows that from
0x78(%rsp) -> %rax -> 0x14(%rax) -> %r13d -> 0x10(%r12,%r13,8) -> %r11d

which is the data dependency that read the recycler field in the JMH state and the localPool field spilling it from the top of the stack.
Profiling data doesn't suggest it to be a performance problem (store-to-load forwarding should make these pretty cheap hitting the store buffer and not even L1, so we are below 3 cycle), but it's relevant that this spilling happens, which means:

it is the cause of the performance difference but linux perf cycles doesn't show it
is an effect of the real cause which is making some method to consume more registers at the point that spilling them is necessary

I'll update this once I understand more.
For the purpose of this investigation we are not supposed to have any regression with the introduction of the thread unsafe (pinned) and unguarded recycler, so I cannot proceed till I fix it. | I've (half) solved the riddle at #15610 (comment): the operations of additiona/removal on ArrayList due to the check bound intrinsics uses some extra register (!), adding more pressure.
It is (let's say) a sort of microbenchmarking issue (at this point), because we assume fully inlining of everything, to make this to matter enough - but, at the same point, the number of instructions and loads to fill the local batch could be lowered further leveraging what the original and new logic suggest i.e. the local batch never grow past chunksSize so, why using a mutable structure at all?
The new numbers are so promising that the performance gap vs a bespoke array q + mpsc q is now zero (at least in this microbenchmark): running them now and sending a summary comment to describe them. | @chrisvest I have a strong feeling that this could be slightly further improved by removing the need for volatile owner and other stuff: will work on that one too, after a first round of new measurements to compare against baseline and what we want to achieve | before these changes, JDK 21 (and JDK 24 too):
Benchmark                                         Mode  Cnt  Score   Error  Units
RecyclerBenchmark.recyclerGetAndUnguardedRecycle  avgt   20  5.510 ± 0.021  ns/op

The same exact behaviour is, at 2a07a59 w JDK 21:
Benchmark                                         (fastThreadLocal)  (unguarded)  Mode  Cnt  Score   Error  Units
RecyclerBenchmark.recyclerGetAndUnguardedRecycle               true        false  avgt   20  4.398 ± 0.019  ns/op

w JDK 24:
Benchmark                                         (fastThreadLocal)  (unguarded)  Mode  Cnt  Score   Error  Units
RecyclerBenchmark.recyclerGetAndUnguardedRecycle               true        false  avgt   20  4.282 ± 0.036  ns/op

which means there's no regression and we are actually improved.
And, the cherry on the top, the comparison vs the fully local one (@georgebanasios ) - which was, w JDK 21:
Benchmark                                                             (unguarded)  Mode  Cnt   Score   Error      Units
RecyclerBenchmark.localRecyclerGetAndRecycle                                false  avgt   20   2.992 ± 0.011      ns/op

and is now:
Benchmark                                         (fastThreadLocal)  (unguarded)  Mode  Cnt  Score   Error  Units
RecyclerBenchmark.recyclerGetAndUnguardedRecycle              false         true  avgt   20  2.668 ± 0.010  ns/op

meaning that we're actually slightly faster compared to it (likely due to a simple local structure)
Now this is ready to be reviewed - and hope to try it w your branch @georgebanasios
The problem, as the new implementation make it even more clear, is that the Recyclers local batch really cannot grow past chunkSize - and if that happen, it's performance will drop a lot, especially on release side since it use a compare and set on the mpsc q. | I'll add the new tests and doc as separate commits during the review, to squash everything at the end.
While working on this I've found an ""interesting"" problem too, due to the Java's JMM and related JCtools (@nitsanw )
The pattern used to call drain on the mpsc queue is causing tons of wasteful bound checks, due to how mpsc works internally, see
@InternalAPI
public final class MessagePassingQueueUtil
{
    public static <E> int drain(MessagePassingQueue<E> queue, Consumer<E> c, int limit)
    {
        if (null == c)
            throw new IllegalArgumentException(""c is null"");
        if (limit < 0)
            throw new IllegalArgumentException(""limit is negative: "" + limit);
        if (limit == 0)
            return 0;
        E e;
        int i = 0;
        for (; i < limit && (e = queue.relaxedPoll()) != null; i++)
        {
            c.accept(e);
        }
        return i;
    }
The relaxedPoll uses Object e = lvRefElement(buffer, offset)  which is a volatile operation
which means that c.accept(e) assuming it to be something like batch.add(e) needs to:

read the batch fields
if backed by an array, read the arraylength of the array
perform a bound check per each element added

Things could improve slightly if we instruct the JIT that the array is in a stable field (e.g. by using a proper lambda for Consumer<E> and using a raw array captured by the lambda in its ""truly"" final fields) but what ever is used in the lambda to track the current array size, will still be read again and again, likely saving just the load to the array field and the array's length read (which still cause a data dependent load).
The best would be to have a int drain(E[] e, int start, int count) which will make able the JIT to lift the bound checks at the beginning of the loop, speeding up a lot the batch copy which will just care about storing in the array without any need to bound check and load it for each addition.
In some implementation(s) we could even use arrayCopy (spsc array q for example) and got even faster. | @He-Pin check this to see if it fits your need? c156ff6
@normanmaurer I admit this is a pretty sharp scissor in this form, but I've tried to be very clear on the docs when it has to be used. | See #15610 (comment) | I see that testSubmittingTaskWakesUpSuspendedExecutor has failed ""more"" now that I have removed volatile from the pooled Mpsc q, but I am not sure is related: it needs investigation | On the Adaptive branch, I'm getting more or less the same results as in this comment: #15610 (comment) | Did you used the constructor with the chunk size?
I will investigate today what's going on | Did you used the constructor with the chunk size?
I will investigate today what's going on

Yes | Late to the party, as always :-)
I think we have a couple of untapped wins to reduce loads (which hurt more than other instructions in non-benchmark workload):

Improve the Fast TL to use a special field for Recycler. Is this important enough to special case? I had at some point a variation on the Fast TL that leaned on a fixed number of fields (e.g. 128 refs, 128 longs) to serve as super fast TL refs and counters, with a fallback to the array if you exceed the field count. It makes all threads footprint a little bigger, but saves a lot of derefs.
Use the Recycler/LocalPool classes to extend the queue class (or some other field class). This will remove an interface check and a de-ref. An API issue for recycler probably, but should be an option for the internal pool class. | @georgebanasios
I'm going to run https://github.com/georgebanasios/netty/tree/adaptive-unshared-fast-path as baseline
and add the pinned unguarded Recycler there, is it fine? 🙏
IDK what's the state of such branch ATM - if it deliver the woopy 64 M rps?
MY initial tests on 32cc5cf would suggest it doesn't:
Benchmark                                                                   (allocatorType)  (pollutionIterations)   Mode  Cnt         Score         Error   Units
ByteBufAllocatorAllocPatternBenchmark.directAllocation                             ADAPTIVE                      0  thrpt   20  40982120.514 ± 1707408.724   ops/s
ByteBufAllocatorAllocPatternBenchmark.directAllocation                             ADAPTIVE                 200000  thrpt   20  34313104.332 ± 3071821.629   ops/s

and it has a lot of variability too...
Consider that this is running on my Ryzen which is probably not what you run it against...
So - in short - which branch I should use which deliver the best performance in your adaptive PR? 🙏 | @franz1981 Yes ofc.
I'm running on an M4
I'm using the same branch as the one you posted adaptive-unshared-fast-path (same one that's on the adaptive PR)
What're you getting on mimalloc? not getting the 64M ops/s is fine I guess as long as we're getting the same difference between those two probably | I'm running on an M4

Oki, still surprised such a big diff wow!

What're you getting on mimalloc?

I cannot see it in the benchmark list anymore - it has been removed? | I cannot see it in the benchmark list anymore - it has been removed?

Oh yeah, on the draft PR I have not pushed the mimalloc implementation at all. | @georgebanasios
I've created https://github.com/franz1981/netty/tree/adaptive-unshared-fast-path_w_recycler and the first results using this recycler are not good, as you pointed out in #15610 (comment)
Investigating what's going on - first results at 3c368f4 are:
Benchmark                                                      (allocatorType)  (pollutionIterations)   Mode  Cnt         Score         Error  Units
ByteBufAllocatorAllocPatternBenchmark.directAllocation                ADAPTIVE                      0  thrpt   20  36291075.307 ± 1407487.561  ops/s

vs baseline 32cc5cf:
Benchmark                                                                   (allocatorType)  (pollutionIterations)   Mode  Cnt         Score         Error   Units
ByteBufAllocatorAllocPatternBenchmark.directAllocation                             ADAPTIVE                      0  thrpt   20  40982120.514 ± 1707408.724   ops/s | Found something odd
          │   0x00007f7b1844873b:   lea    (%r12,%rbp,8),%r10           ;*invokeinterface recycle {reexecute=0 rethrow=0 return_oop=0}
          │                                                             ; - io.netty.buffer.AdaptivePoolingAllocator$AdaptiveByteBuf::deallocate@76 (line 2471)
          │                                                             ; - io.netty.buffer.AbstractReferenceCountedByteBuf::handleRelease@5 (line 149)
          │                                                             ; - io.netty.buffer.AbstractReferenceCountedByteBuf::release@8 (line 139)
          │                                                             ; - io.netty.microbench.buffer.ByteBufAllocatorAllocPatternBenchmark::performDirectAllocation@30 (line 533)
          │   0x00007f7b1844873f:   mov    0xc(%r10),%r10d              ;*getfield pool {reexecute=0 rethrow=0 return_oop=0}
          │                                                             ; - io.netty.util.Recycler$LocalPoolHandle::recycle@1 (line 59)
          │                                                             ; - io.netty.buffer.AdaptivePoolingAllocator$AdaptiveByteBuf::deallocate@76 (line 2471)
          │                                                             ; - io.netty.buffer.AbstractReferenceCountedByteBuf::handleRelease@5 (line 149)
          │                                                             ; - io.netty.buffer.AbstractReferenceCountedByteBuf::release@8 (line 139)
          │                                                             ; - io.netty.microbench.buffer.ByteBufAllocatorAllocPatternBenchmark::performDirectAllocation@30 (line 533)
   0.06%  │   0x00007f7b18448743:   test   %r10d,%r10d
          │╭  0x00007f7b18448746:   je     0x00007f7b18448767           ;*ifnull {reexecute=0 rethrow=0 return_oop=0}
          ││                                                            ; - io.netty.util.Recycler$LocalPoolHandle::recycle@6 (line 60)
          ││                                                            ; - io.netty.buffer.AdaptivePoolingAllocator$AdaptiveByteBuf::deallocate@76 (line 2471)
          ││                                                            ; - io.netty.buffer.AbstractReferenceCountedByteBuf::handleRelease@5 (line 149)
          ││                                                            ; - io.netty.buffer.AbstractReferenceCountedByteBuf::release@8 (line 139)
          ││                                                            ; - io.netty.microbench.buffer.ByteBufAllocatorAllocPatternBenchmark::performDirectAllocation@30 (line 533)
          ││  0x00007f7b18448748:   mov    %r13d,0x18(%rsp)
   0.23%  ││  0x00007f7b1844874d:   mov    %r14d,%ebp                   ;*synchronization entry
          ││                                                            ; - io.netty.util.Recycler$LocalPoolHandle::recycle@-1 (line 59)
          ││                                                            ; - io.netty.buffer.AdaptivePoolingAllocator$AdaptiveByteBuf::deallocate@76 (line 2471)
          ││                                                            ; - io.netty.buffer.AbstractReferenceCountedByteBuf::handleRelease@5 (line 149)
          ││                                                            ; - io.netty.buffer.AbstractReferenceCountedByteBuf::release@8 (line 139)
          ││                                                            ; - io.netty.microbench.buffer.ByteBufAllocatorAllocPatternBenchmark::performDirectAllocation@30 (line 533)
          ││  0x00007f7b18448750:   lea    (%r12,%r10,8),%rsi           ;*getfield pool {reexecute=0 rethrow=0 return_oop=0}
          ││                                                            ; - io.netty.util.Recycler$LocalPoolHandle::recycle@1 (line 59)
          ││                                                            ; - io.netty.buffer.AdaptivePoolingAllocator$AdaptiveByteBuf::deallocate@76 (line 2471)
          ││                                                            ; - io.netty.buffer.AbstractReferenceCountedByteBuf::handleRelease@5 (line 149)
          ││                                                            ; - io.netty.buffer.AbstractReferenceCountedByteBuf::release@8 (line 139)
          ││                                                            ; - io.netty.microbench.buffer.ByteBufAllocatorAllocPatternBenchmark::performDirectAllocation@30 (line 533)
          ││  0x00007f7b18448754:   mov    %rbx,%rdx
          ││  0x00007f7b18448757:   call   0x00007f7b17cad780           ; ImmutableOopMap {[0]=Oop [8]=Oop [16]=Oop [56]=Derived_oop_[16] }
          ││                                                            ;*invokevirtual release {reexecute=0 rethrow=0 return_oop=0}
          ││                                                            ; - io.netty.util.Recycler$LocalPoolHandle::recycle@11 (line 61)
          ││                                                            ; - io.netty.buffer.AdaptivePoolingAllocator$AdaptiveByteBuf::deallocate@76 (line 2471)
          ││                                                            ; - io.netty.buffer.AbstractReferenceCountedByteBuf::handleRelease@5 (line 149)
          ││                                                            ; - io.netty.buffer.AbstractReferenceCountedByteBuf::release@8 (line 139)
          ││                                                            ; - io.netty.microbench.buffer.ByteBufAllocatorAllocPatternBenchmark::performDirectAllocation@30 (line 533)
          ││                                                            ;   {optimized virtual_call}
   0.03%  ││  0x00007f7b1844875c:   nopl   0x44c(%rax,%rax,1)           ;*invokevirtual release {reexecute=0 rethrow=0 return_oop=0}
          ││                                                            ; - io.netty.util.Recycler$LocalPoolHandle::recycle@11 (line 61)
          ││                                                            ; - io.netty.buffer.AdaptivePoolingAllocator$AdaptiveByteBuf::deallocate@76 (line 2471)
          ││                                                            ; - io.netty.buffer.AbstractReferenceCountedByteBuf::handleRelease@5 (line 149)
          ││                                                            ; - io.netty.buffer.AbstractReferenceCountedByteBuf::release@8 (line 139)
          ││                                                            ; - io.netty.microbench.buffer.ByteBufAllocatorAllocPatternBenchmark::performDirectAllocation@30 (line 533)
          ││                                                            ;   {other}

which is:
   handle.recycle -> if pool != null : pool.release <--------- NOT INLINED !!!!!

LocalPool::release:
        protected final void release(H handle) {
            Thread owner = this.owner;
            if (owner != null && Thread.currentThread() == owner && batchSize < batch.length) {
                batch[batchSize] = handle;
                batchSize++;
            } else if (owner != null && isTerminated(owner)) {
                pooledHandles = null;
                this.owner = null;
            } else {
                MessagePassingQueue<H> handles = pooledHandles;
                if (handles != null) {
                    handles.relaxedOffer(handle);
                }
            }
        }
which is pretty consistent | If you want @georgebanasios run the benchmark for #15610 (comment) on your M4 you should have a similar ratio i.e. within 90% of the baseline commit.
I wish to solve the problem in #15610 (comment) and make performance on part, but doesn't look easy...
I don't like to miss the target of performance parity tbh | hey @franz1981 sorry for the delay. (Curious to see the commit for the release method inlining tbh)
Here are the results from your branch (I just used the same adaptive and recycler files as they are there):
Benchmark                                               (allocatorType)  (pollutionIterations)   Mode  Cnt         Score         Error  Units
ByteBufAllocatorAllocPatternBenchmark.directAllocation         ADAPTIVE                      0  thrpt   20  49015566.232 ±  875152.167  ops/s
ByteBufAllocatorAllocPatternBenchmark.directAllocation         ADAPTIVE                 200000  thrpt   20  42714508.430 ± 3070136.172  ops/s

You're getting way better results? | @georgebanasios

You're getting way better results?

Depends: if I assume that baseline for you was 64 M req/sec it means that you got 76.5% of the performance of baseline, while I got 90% of it - so yes, I got much better result.
That's why I wanted to make sure:

you run your benchmark vs the baseline at 32cc5cf  too?
your results are by running vs 3c368f4? | @georgebanasios

You're getting way better results?

Depends: if I assume that baseline for you was 64 M req/sec it means that you got 76.5% of the performance of baseline, while I got 90% of it - so yes, I got much better result. That's why I wanted to make sure:

you run your benchmark vs the baseline at 32cc5cf  too?
your results are by running vs 3c368f4?


Yes to both.
Maybe @chrisvest can give the new Recycler a try? | I don't have access to M4 assembly, or I would help, even to find if the problem is the call not inlinable, maybe killing performance on such big pipelined processor...
In any case I wasn't getting 100% the same performance either so...it means that there is work to do there...
I plan to do something easy for what I know about Aarch64 and is by removing any volatile field in the recycler. Even volatile loads can have subtle effects on aarch64 | I've verifie via my super old x86 i7 laptio and got consistent results w the Ryzen (although at a much different scale clearly): getting 90% of the perfmance of baseline.
I'll update the volatile and will ask you to retry at this point since is the easier way to help on Apple. | @georgebanasios on x86 the volatile load won't really matter (if not that force nearby fields to be re-read...) but I've sent a new commit at 18fb8bb: IIRC the ARM optimization guide, and how C2 emits load volatile (adding dmbish barriers) - it should matter ""enough""
NOTE:
The main change I've added to the recycler in the adaptive branch is to use a mpmc fixed size queue when ownerThread == null: this is clearly not right, because it will affect this code path
        LocalPool(int maxCapacity, int ratioInterval, int chunkSize) {
            this(!BATCH_FAST_TL_ONLY || FastThreadLocalThread.currentThreadHasFastThreadLocal()
                         ? Thread.currentThread() : null, maxCapacity, ratioInterval, chunkSize);
        }
Which is used by thread local's recycler(s) while interacting with non-fast-thread-local threads, making them to NOT use the local queue but just the mpsc queue, now mpmc (which is slower on the acquire path!).
Ideally we would like  this to be applied only if a local pool is used AND ownerThread = null. | @franz1981 Just to be sure. To test I'm using the recycler from here: 18fb8bb and the allocator from here: 712c4e6#diff-57ca23ebd0de1096dc91e73cb1f9c790c66494dd42f3960c71e6521a2b386e2f right? | @georgebanasios 18fb8bb is the last commit I've added on https://github.com/franz1981/netty/tree/adaptive-unshared-fast-path_w_recycler so if you checkout that branch and run its benchmark, you'll get both: you just need to compare the allocator w yours, in case you added more changes. If there's more I will update it | @franz1981 I checked out your branch to not have any diffs.
The results seem the same all around sadly. | So, like x86 (I got no diff) but still you won't got in 90% of baseline performance so, need further investigation 🙏 thanks to have tried | I will keep on working on this, but to isolate what's wrong w the adaptive allocator use case I need to write some more precise microbenchmark which isolate the same behaviour in the same way e.g.
I have noticed that If adaptive ByteBuf store the Handle in an EnhancedHandle field, when the code has to call recycle on it (moving from invokeinterface to invokevirtual) the JIT hasn't emitted the class check against the concrete class LocalPoolHandle, which surprised me a bit tbh. | This is getting very tedious: I've rewritten the recycler benchmark to isolate exactly the same usage pattern of the adaptive allocator and the difference between the local queue vs unguarded Recycler is negligible - and doesn't justify the performance difference observed by both me and @georgebanasios.
I'm still digging into it | I've futher improved the recycler (soon will add a new commit) to benefit the common use case i.e. maxCapacityPerThread > 0, but inspecting the ASM I didn't spotted that owner != null && Thread.currentThread == owner was that costly, see
  0x00007f7544231f96:   mov    0x1c(%r12,%r10,8),%ebx       ; localPool.owner -> ebx (it's compressed)
  0x00007f7544231f9b:   test   %ebx,%ebx                                
  0x00007f7544231f9d:   je     0x00007f7544232130           ; ebx != null
  0x00007f7544231fa3:   lea    (%r12,%rbx,8),%r11           ; uncompress(localPool.owner) -> r11
  0x00007f7544231fa7:   mov    0x390(%r15),%r9              ; &Thread::currentThread -> r9
  0x00007f7544231fae:   mov    (%r9),%rbp                   ; *r9 -> rbp
  0x00007f7544231fb1:   cmp    %r11,%rbp                             
  0x00007f7544231fb4:   jne    0x00007f75442320ec           ; rbp != r11
0x1c is the owner field, see
io.netty.util.Recycler.LocalPool object internals:
OFF  SZ                     TYPE DESCRIPTION               VALUE
  0   8                          (object header: mark)     N/A
  8   4                          (object header: class)    N/A
 12   4                      int LocalPool.ratioInterval   N/A
 16   4                      int LocalPool.batchSize       N/A
 20   4                      int LocalPool.ratioCounter    N/A
 24   4                      H[] LocalPool.batch           N/A
 28   4                   Thread LocalPool.owner           N/A
 32   4   MessagePassingQueue<H> LocalPool.pooledHandles   N/A
 36   4                          (object alignment gap)    
Instance size: 40 bytes
Space losses: 0 bytes internal + 4 bytes external = 4 bytes total

The assembly there is perfectly fine - but if the adaptive pool doesn't perform the same check (because can reuse Thread::currentThread obtained at the beginning...) clearly the performance will be different. | @georgebanasios I've added 66e14d0  to https://github.com/franz1981/netty/commits/adaptive-unshared-fast-path_w_recycler/ getting ~37.7 M rops vs 40.9 M rops. Not yet there, but still an improvement, if you wanna give it a shot | I have yet to implement #15610 (comment) and it's ready (will draft it again) | @franz1981 I'm seeing the same improvement as you. I'm getting ~50M ops/s now. | Nice @georgebanasios so 50M ops vs 64M ops is still 78%...will check if I can get more ;) | Nice @georgebanasios so 50M ops vs 64M ops is still 78%...will check if I can get more ;)

Not sure if it helps or not but in this commit I was getting around ~60M ops/s (ignore the PinnedRecycler class): georgebanasios@bef62fd#r164947435, if you wanna check it out. Looking back to it I see a couple of issues but yeah just in case it helps. | Thanks @georgebanasios I'm checking what I do differently from there and I saw georgebanasios@bef62fd#r165438322
Moreover I'm checking the way you used it in adaptive, because that could be another reason as well | @georgebanasios
The results I got here are much different from yours I see:
 $ git reset --hard bef62fd56f8cff7032948bdb36cb7fa19a9740ee
HEAD is now at bef62fd56f test recycler variant
and, on JDK 21:
$ numactl --localalloc -N 0 java -Djmh.executor=CUSTOM -Djmh.executor.class=io.netty.microbench.util.AbstractMicrobenchmark\$HarnessExecutor -jar microbench/target/microbenchmarks.jar io.netty.microbench.buffer.ByteBufAllocatorAllocPatternBenchmark.directAllocation -prof perfasm -pallocatoryType=ADAPTIVE -ppollutionIterations=

Produces:
Benchmark                                                    (allocatorType)  (pollutionIterations)   Mode  Cnt         Score         Error  Units
ByteBufAllocatorAllocPatternBenchmark.directAllocation              ADAPTIVE                      0  thrpt   20  32953862.731 ± 171944.509  ops/s

Consider that I see that bef62fd (mentioned by you at #15610 (comment)) is using ""guarded"" recycling, which I have avoided in my branch...are you use you got that high score there? | @franz1981 I also reset my branch in the same commit (I'm on JDK 21 too) and I got:
Benchmark                                               (allocatorType)  (pollutionIterations)   Mode  Cnt         Score         Error  Units
ByteBufAllocatorAllocPatternBenchmark.directAllocation         ADAPTIVE                      0  thrpt   20  59572755.696 ±  415589.283  ops/s
ByteBufAllocatorAllocPatternBenchmark.directAllocation         ADAPTIVE                 200000  thrpt   20  44272728.530 ± 1041772.088  ops/s | This is super interesting 🤔
It seems that data dependent loads are more harmful in M4 than volatile stores (the recycler on your commit still use them in toAvailable because is not using unguardedRecycle).
This come with a bit of shock, because as x86 user instead, these volatile store are performance killers.
I really need an apple machine.. | @chrisvest I remember you have something different from M4 (but still Apple), so I would like you to run
java -Djmh.executor=CUSTOM -Djmh.executor.class=io.netty.microbench.util.AbstractMicrobenchmark\$HarnessExecutor -jar microbench/target/microbenchmarks.jar io.netty.microbench.buffer.ByteBufAllocatorAllocPatternBenchmark.directAllocation -pallocatoryType=ADAPTIVE -ppollutionIterations=0

for these commits:

32cc5cf : @georgebanasios baseline
bef62fd :  @georgebanasios pinned recycler
66e14d0 : @franz1981 this Recycler w a small change to use an mpmc array q for large allocations (which are shared)

On AMD and Intel Xeon I'm getting w 66e14d0 about 90% of 32cc5cf throughput, whilst bef62fd is a bad performer.
Whilst Apple M4 seems the opposite 🙏
Same for @laosijikaichele if you got the chance...
I'm evaluating the existing recycler vs that adaptive changes because if it proves to be good enough could be used in other code paths in Netty (e.g. reusable array lists, write tasks, etc etc) @normanmaurer too
see https://github.com/search?q=repo%3Anetty%2Fnetty%20%22new%20Recycler%22&type=code | IDK at this point if I can make this better, perf wise
So, ready for reviews @normanmaurer @georgebanasios and @chrisvest | @franz1981 can you include a before / after benchmark ? There is so much churn in here :D | FYI @normanmaurer
#15610 (comment)
this is what you need?
i.e. we're now slightly better than what we was + we have some much more performant (and unsafe?) options i.e.

pin to thread without any fast thread local lookup
unguarded recycling (less garbage, indirection and validations)

I can re-run them with the latest commit too, wdyt? | on JDK 21 and AMD Ryzen 4
With this PR at ac6b915 and no fast thread local caller
via
java -jar microbench/target/microbenchmarks.jar io.netty.microbench.util.RecyclerBenchmark.recyclerGetAndUnguardedRecycle$

Benchmark                                         (fastThreadLocal)  (unguarded)  Mode  Cnt   Score   Error  Units
RecyclerBenchmark.recyclerGetAndUnguardedRecycle               true        false  avgt   20  11.006 ± 0.086  ns/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle               true         true  avgt   20   8.705 ± 0.102  ns/op

whilst with fast thread local caller, via
java -Djmh.executor=CUSTOM -Djmh.executor.class=io.netty.microbench.util.AbstractMicrobenchmark\$HarnessExecutor -jar microbench/target/microbenchmarks.jar io.netty.microbench.util.RecyclerBenchmark.recyclerGetAndUnguardedRecycle$

Benchmark                                         (fastThreadLocal)  (unguarded)  Mode  Cnt  Score   Error  Units
RecyclerBenchmark.recyclerGetAndUnguardedRecycle               true        false  avgt   20  4.096 ± 0.010  ns/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle               true         true  avgt   20  3.636 ± 0.012  ns/op

And with the new recycler options:
Benchmark                                         (fastThreadLocal)  (unguarded)  Mode  Cnt   Score   Error  Units
RecyclerBenchmark.recyclerGetAndUnguardedRecycle              false        false  avgt   20   3.252 ± 0.011  ns/op
RecyclerBenchmark.recyclerGetAndUnguardedRecycle              false         true  avgt   20   2.593 ± 0.008  ns/op

Whilst baseline at f922892 and no fast thread local caller:
Benchmark                                         Mode  Cnt   Score   Error  Units
RecyclerBenchmark.recyclerGetAndUnguardedRecycle  avgt   20  14.047 ± 0.943  ns/

baseline at f922892 and fast thread local caller:
Benchmark                                         Mode  Cnt  Score   Error  Units
RecyclerBenchmark.recyclerGetAndUnguardedRecycle  avgt   20  5.537 ± 0.024  ns/op

I know that's not very visual but the TLDR story is:

we're now faster in the same exact scenario (~30%)
we have the option(s) of been much faster (like +400%) | I don't have any Apple machine @normanmaurer so no idea on Apple MX | Uh bad news: I'm running io.netty.microbench.util.RecyclerBenchmark.producerConsumer and perf has regressed :""(
putting in draft again uff | Ok the comment on the benchmark is telling me that is not THAT important...
    // The allocation stats are the main thing interesting about this benchmark
    @Benchmark
    @Group(""producerConsumer"")
    public void producer(ProducerConsumerState state, Control control) throws Exception {
        Queue<DummyObject> queue = state.queue;
        DummyObject object = state.recycler.get();
        while (!control.stopMeasurement) {
            if (queue.offer(object)) {
                break;
            }
        }
    }
main thing doesn't mean that is the only one, I will still take a quick look... | I've implemented an alternative batchy drain as well:
        LocalPool(Thread owner, int maxCapacity, int ratioInterval, int chunkSize) {
            this.ratioInterval = ratioInterval;
            this.owner = owner;
            batch = owner != null? (H[]) new Object[chunkSize] : null;
            this.chunkSize = chunkSize;
            batchSize = 0;
            pooledHandles = createExternalScPool(chunkSize, maxCapacity);
            ratioCounter = ratioInterval; // Start at interval so the first one will be recycled.
            // funny way to avoid a cast on astore (if ever)
            final Object[] unsafeBatch = batch;
            drainQ = batch != null? h -> {
                unsafeBatch[batchSize++] = h;
            } : null;
        }

        protected final H acquire() {
            int size = batchSize;
            if (size == 0) {
                // it's ok to be racy; at worst we reuse something that won't return back to the pool
                final MessagePassingQueue<H> handles = pooledHandles;
                if (handles == null) {
                    return null;
                }
                if (drainQ == null) {
                    return handles.relaxedPoll();
                }
                size = handles.drain(drainQ, chunkSize);
                assert size == batchSize;
                if (size == 0) {
                    return null;
                }
            }
            int top = size - 1;
            final H h = batch[top];
            batchSize = top;
            batch[top] = null;
            return h;
        }
but TBH this is just making even more lighter the producer side of the producerConsumer benchmark - causing it to further slow down the consumer as well (since the mpmc array q used to communicate the pooled instances need to read the consumer index when the q is full); in short, an ""unbalanced"" implementation i.e. super fast Recycler::get and same Handle::recycle can make the implementation to not be good enough, with numbers, while is actually improved where is supposed to be better at (i.e. pool's acquire).
TLDR I would ignore the number of that benchmark; if we want to have an idea of the cost of performing these operations across threads, would be better a RTT-like JMH bench i.e.
// a background consumer thread spin wait till state.ready == true - recycle all the elements - and mark state.lazySetCompleted(true)
@Benchmark
@Thread(1)
public void acquireAndRecycleAcrossThreads(SharedState state) {
       Recycler<Foo> recycler = state.recycler;
       Foo[] pooled = state.pooled;
       for (int i = 0; i < state.batchSize; i++) {
             pooled[i] = recycler.get();
       }
       state.lazySetReady(true);
       while (!state.completed) {
             // burn burn burn
       }
       state.lazySetCompleted(false);
} | Ok I'm modifying the benchmarks in order to make sure that my changes are good. | Done, results on producer/consumer scenario
before:
Benchmark                                                Mode  Cnt   Score    Error   Units
RecyclerBenchmark.producerConsumer                      thrpt   20  32.893 ± 14.358  ops/us
RecyclerBenchmark.producerConsumer:consumer             thrpt   20  16.447 ±  7.179  ops/us
RecyclerBenchmark.producerConsumer:emptyQ               thrpt   20   0.673 ±  0.424  ops/us
RecyclerBenchmark.producerConsumer:fullQ                thrpt   20   3.514 ±  1.816  ops/us
RecyclerBenchmark.producerConsumer:producer             thrpt   20  16.447 ±  7.179  ops/us
RecyclerBenchmark.producerConsumer:·gc.alloc.rate       thrpt   20  20.381 ± 24.406  MB/sec
RecyclerBenchmark.producerConsumer:·gc.alloc.rate.norm  thrpt   20   0.470 ±  0.526    B/op
RecyclerBenchmark.producerConsumer:·gc.count            thrpt   20  37.000           counts
RecyclerBenchmark.producerConsumer:·gc.time             thrpt   20   9.000               ms

now:
Benchmark                                               (fastThreadLocal)  (unguarded)   Mode  Cnt    Score     Error   Units
RecyclerBenchmark.producerConsumer                                   true        false  thrpt   20   65.287 ±   2.120  ops/us
RecyclerBenchmark.producerConsumer:consumer                          true        false  thrpt   20   32.644 ±   1.060  ops/us
RecyclerBenchmark.producerConsumer:emptyQ                            true        false  thrpt   20    0.398 ±   0.273  ops/us
RecyclerBenchmark.producerConsumer:fullQ                             true        false  thrpt   20    5.624 ±   1.184  ops/us
RecyclerBenchmark.producerConsumer:producer                          true        false  thrpt   20   32.644 ±   1.060  ops/us
RecyclerBenchmark.producerConsumer:·gc.alloc.rate                    true        false  thrpt   20    0.499 ±   0.245  MB/sec
RecyclerBenchmark.producerConsumer:·gc.alloc.rate.norm               true        false  thrpt   20    0.008 ±   0.004    B/op
RecyclerBenchmark.producerConsumer:·gc.count                         true        false  thrpt   20      ≈ 0            counts

The metrics to read here are the gc.alloc.rate.norm which is slightly improved and the different counters
e.g. we have doubled the ""streaming"" throughput (producer -> consumer)
i.e. consumer counter
and we're still ""faster"" to recycler.get than to recycle i.e. fullQ means producer too fast and emptyQ means consumer too fast | @georgebanasios I've sent another small change using the latest recycler version at https://github.com/franz1981/netty/commits/adaptive-unshared-fast-path_w_recycler/ (last commit) - and in my machine I'm getting between 95% and 98% of the original performance - let me know if you can give it a shot and it's my last attempt. This task is sucking too much time already! | @franz1981 Nope, sadly I'm getting the same ~50M ops/s | At this point IDK how to squeeze more perf out of this recycler for Apple, but removing generics and save few check cast to happen.
I have even attempted to flatten the whole hierarchy, without any success.
What I don't get is that bef62fd is, on the paper, more costly:

it has the array deque level of indirection which perform more work (tail and head handling) than an hand rolled stack
it still uses DefaultHandle wrappers which requires to navigate

@georgebanasios the mentioned commit got just one thing which doesn't seem right i.e
bef62fd#diff-c4c0f035e26b5f8983f985f3e3368a0dfc8fc75e982268e53150af81da1d94acR389
We cannot return the handle to the local stack without marking it as available first; this is needed (for the guarded case), to make sure that the same handle hasn't been recycled concurrently in another non-owner thread.
In this form it looks like a bug, but to be fair, I have implemented the unguarded pool reducing the level of indirection and having the same limit..
The only critical difference I could spot in this commit is that AbstractMagazine::newBuffer has its own impl, whilst in my case I am sharing a single method; maybe I could split it too..
Feel free to play with the commit I have mentioned in the previous comment if you can, till you have an M4 and maybe we can find the best of the two worlds | @franz1981 I'm trying to integrate your changes in my commit to maybe somehow combine them, but unfortunately no luck so far | Thanks @georgebanasios
As I've said, I'm very surprised by the results, because the one I built is saving one level of indirection on the handles too... | @normanmaurer @chrisvest
As promised, before putting more work into creating builders (good suggestion from @georgebanasios ) I have some benchmark number re adaptive.
As you know, in #15530 we had lot of changes which lift many bottlenecks, but here I've reused the existing adaptive pool implementation without any of these changes, just replacing the Recycler used in the thread local magazines.
4.2. i.e. Recycler using guarded handles + thread local:
Benchmark                                                      (allocatorType)  (pollutionIterations)   Mode  Cnt         Score        Error  Units
ByteBufAllocatorAllocPatternBenchmark.directAllocation                ADAPTIVE                      0  thrpt   20  23512907.995 ± 268380.662  ops/s

The new Recycler using guarded handles + thread local:
Benchmark                                                      (allocatorType)  (pollutionIterations)   Mode  Cnt         Score        Error  Units
ByteBufAllocatorAllocPatternBenchmark.directAllocation                ADAPTIVE                      0  thrpt   20  25779610.634 ± 205116.179 

The new  Recycler using unguarded handles + pinned:
Benchmark                                               (allocatorType)  (pollutionIterations)   Mode  Cnt         Score        Error  Units
ByteBufAllocatorAllocPatternBenchmark.directAllocation         ADAPTIVE                      0  thrpt   20  28155150.995 ± 276509.279  ops/s

Which means +10% to 17% of performance improvement in the original adaptive allocator.
In the latest adaptive allocator changes (not yet merged), this translates in a bigger improvement too.
NOTE: both the guarded handles + thread local recyclers run shows a problem I've found while improving adaptive
i.e. the DefaultHandle's updater lazySet is not inlined.
But the benchmark is still fair with the old vs new Recycler since the inlining doesn't happen for both.
The profiling data reflect the improvement taken, in term of cycles spent on the Recycler, see
In 4.2. i.e. Recycler using guarded handles + thread local
the benchmark spend, in violet, 26.73% of its cpu-clock in the Recycler:

whilst the new  Recycler using unguarded handles + pinned
the benchmark spend, in violet, 10.31% of its cpu-clock in the Recycler:

which means saving 16.42% of cpu-clock.
Since the benchmark is saturating the cpu-clock resource, we get the claimed ~17% throughput improvement. | @chrisvest

It's your plan to use this to address #15419 in a separate PR?

I have commented on #15610 (comment) re the improvement on 4.2 only and I think I would add it separately only on top of 4.2.
I think that after we add these ""new"" recyclers, there are lot of existing code paths which could use them and get some very good performance improvement (or reduced memory footprint: the unguarded variant doesn't allocate any handle!).
Re adaptive improvements we have the work with @georgebanasios which could benefit of this if I got it fast enough - once the other improvements will be merged. | @franz1981 so are you ok with pulling this in or do you want to complete the last missing thing ? I suspect this should be a follow up tho. | I wanted to create a builder as suggested by @georgebanasios but can be done in a follow up pr, let me know 🙏 | @franz1981 whatever you prefer... just let me know | We can first merge this, and I can add the builder separately. This one is sitting for longprry for that 🙏
Hope this fit the case from @He-Pin","Improve Recycler's local pooling

Motivation:

Existing Recycler has bad performance in the thread-local case

Modification:

Allow Recycler to drop fast-thread-local usage and enable fully unguarded recycles i.e. no wrapper handling

Result:

Fixes #15604 | Address review comments"
netty/netty,15189,https://github.com/netty/netty/pull/15189,collapse identical 'catch' branches,"Simplifies code, easier to read, less bytecode",2025-05-15T19:27:21Z,normanmaurer,doom369,easier to read,@doom369 thanks,collapse identical 'catch' branches | Merge branch '4.2' into collapse_identical_catch_branches
netty/netty,14372,https://github.com/netty/netty/pull/14372,Allow MessageToMessageDecoder to take care of reading more data when …,"…needed

Motivation:

ByteToMessageDecoder already takes care of calling `read()` if we need to do so to make progress. We should do the same with MessageToMessageDecoder if possible so we make it easier for users and also ensure we do not stall

Modifications:

- Let MessageToMessageDecoder emit reads if needed when the decoder is not sharable
- Let MessageToMessageCodec emit reads if needed when the codec is not sharable

Result:

Ensure we never stall if AUTO_READ is disabled
",2024-11-04T07:31:58Z,normanmaurer,normanmaurer,easier to read,This was motivated by netty/netty-incubator-codec-ohttp#88 | @chrisvest WDYT ? | @chrisvest @thomdev PTAL again,"Allow MessageToMessageDecoder to take care of reading more data when needed

Motivation:

ByteToMessageDecoder already takes care of calling `read()` if we need to do so to make progress. We should do the same with MessageToMessageDecoder if possible so we make it easier for users and also ensure we do not stall

Modifications:

- Let MessageToMessageDecoder emit reads if needed when the decoder is not sharable
- Let MessageToMessageCodec emit reads if needed when the codec is not sharable

Result:

Ensure we never stall if AUTO_READ is disabled

Co-authored-by: Chris Vest <christianvest_hansen@apple.com> | Merge branch '4.1' into share | Merge branch '4.1' into share"
netty/netty,14690,https://github.com/netty/netty/pull/14690,IOUring: Add supported for provided buffers,"Motivation:

In scenarios with a large number of inactive connections, reduce memory usage and improve performance.

see https://lwn.net/Articles/815491/

Modification:

Implement IoUringBufferRing and IOSQE_BUFFER_SELECT flag for socket recv:

- Enable the use of IOSQE_BUFFER_SELECT in the socket recv when IoUringBufferRing is supported by the current Linux kernel version and IOUringSocketChannelConfig is configured to enable the feature
- IoUringBufferRing elements are not populated immediately; only when submitting recv ops, if there is available space in the current BufferRing, the allocator will be used to allocate a buffer.
- If the res in the recv CQE is -ERRNO_NO_BUFFER, mark the bufferRing as temporarily unavailable. The current and subsequent recv operations will fall back to the old recv path.
- When a ByteBuf allocated from IoUringBufferRing is released by the user, it will be returned to the IoUringBufferRing, and the buffer will be marked as available for reuse.

Result:

Fixes #14614


",2025-01-29T07:26:33Z,normanmaurer,dreamlike-ocean,easier to read,"Will check later today or over the weekend | I will fix it after work | Ah, it's already one o'clock in the middle of the night on my side. I will complete the remaining changes later on January 21, 2025 in the East Eighth Zone. | All the changes except those to the UserspaceIoUringBuffer have been completed. I don't have any ideas yet on how to modify the UserspaceIoUringBuffer. | All the changes except those to the UserspaceIoUringBuffer have been completed. I don't have any ideas yet on how to modify the UserspaceIoUringBuffer.

I can take care of it ... are you ok with me pushing changes to your branch ? | All the changes except those to the UserspaceIoUringBuffer have been completed. I don't have any ideas yet on how to modify the UserspaceIoUringBuffer.

I can take care of it ... are you ok with me pushing changes to your branch ?

No problem. Please do what you want. | @dreamlike-ocean @franz1981 @chrisvest I would like to pull this in as it is. I will open a few smaller PRs as followup for improvements. That said this works as it is and so I think we are ready to go.
@dreamlike-ocean great work btw! | @dreamlike-ocean @franz1981 @chrisvest I would like to pull this in as it is. I will open a few smaller PRs as followup for improvements. That said this works as it is and so I think we are ready to go.
@dreamlike-ocean great work btw!

Thank you all for your hard work.
let's strive to make netty io uring even better. 😁","Implement IoUringBufferRing and IOSQE_BUFFER_SELECT flag for socket recv:

- Enable the use of IOSQE_BUFFER_SELECT in the socket recv when IoUringBufferRing is supported by the current Linux kernel version and IOUringSocketChannelConfig is configured to enable the feature
- IoUringBufferRing elements are not populated immediately; only when submitting recv ops, if there is available space in the current BufferRing, the allocator will be used to allocate a buffer.
- If the res in the recv CQE is -ERRNO_NO_BUFFER, mark the bufferRing as temporarily unavailable. The current and subsequent recv operations will fall back to the old recv path.
- When a ByteBuf allocated from IoUringBufferRing is released by the user, it will be returned to the IoUringBufferRing, and the buffer will be marked as available for reuse. | add UT | The buffer ring is used only for the first time. | Fire the BufferRingExhaustedEvent to notify users when cqe return ERRNO_NO_BUFFER_NEGATIVE | Apply suggestions from code review | use `byteBuf.writerIndex(byteBuf.writerIndex() + res)` rather than `byteBuf.writerIndex(res)` | Apply suggestions from code review | Apply suggestions from code review | - Apply suggestions from code review
- Provide an additional initialization parameter to allow the user to pre-fill n buffers | - Apply pr 14700 for submitBeforeIO | Remove MPSC queue for recycling buffers | Precompute recycle task | Fix compat problem | Disable buffer ring by default as we need an explicit config | Cleanup | cleanup | Fix NPE | Simplify and add more docs | Enable buffer ring in testsuite when POLL_FIRST is disabled | Prefix classes with IoUring to be consistent with the rest | javadocs and checkstyle | Rename method and cleanup javadocs | cleanup | Add missing return | Fix buffer leaks in test"
netty/netty,14705,https://github.com/netty/netty/pull/14705,Reduce pipeline stack depth and improve performance,"Motivation:
Pipeline calls, such as `ctx.fireChannelRead` and `ctx.write`, currently go through multiple methods. This increases the stack depth in event loop threads, which makes it harder to debug (people have more ceremony mixed in with their code), and hurts performance (JIT inliner budget gets used up faster).

Furthermore, there are other places in this machinery which can be made faster.

Modification:

**1.**
In the `AbstractChannelHandlerContext` all inbound methods, e.g. `fireChannelRead`, have been manually inlined. This means that when a handler calls `ctx.fireChannelRead`, this method call in turn now directly call the `channelRead` method of the _next_ handler in the pipeline. Previously, we had two extra method calls here.

As a consequence of this inlining, we now have to re-compute the target context when we trampoline tasks onto the event-loop. This is presumably rare, and worth the cost.
This also means that some code now moves from the executor of the target context, to the executor of the calling context. This can create different behaviors from Netty 4.1, if the pipeline has multiple handlers, is modified by the handlers during the call, and the handlers use child-executors.

**2.**
A few outbound methods - `read`, `write`, `writeAndFlush` - are likewise inlined. Their inherent complexity and number of overloads means we can't realistically get them down to a single method call, but we can get them down to two. This is still a nice improvement.
The `flush` method is usually not implemented by handlers, so there's no point in inlining that further.

**3.**
In every such call, after finding the next context, we have to decide if we can call the handler directly, or need to trampoline onto a different event loop (due to the executor off-loading feature). This means we have to inspect the context and either pick its child executor, or load the channel event loop of the target context, and this latter part (which is the most common case) is a sequence of dependent loads. Dependent loads cause cache misses and CPU pipeline stalls, so to deal with this the `AbstractChannelHandlerContext` now has a `contextExecutor` field, which caches the result of computing the concrete executor. This means our executor is only one dependent (on the channel handler context) load away. The speedup from this is quite noticeable because it's such a common operation.



Results:
The `DefaultChannelPipelineBenchmark` on my M1 Pro, running JDK 17, tells an encouraging story:

```
Before:
Benchmark                                         (extraHandlers)  (pipelineArrayLength)   Mode  Cnt        Score        Error  Units
DefaultChannelPipelineBenchmark.propagateEvent                 16                   1024  thrpt   10  6515502.983 ± 253375.107  ops/s
DefaultChannelPipelineBenchmark.propagateVariety               16                   1024  thrpt   10  7628162.835 ±  40168.895  ops/s

After:
Benchmark                                         (extraHandlers)  (pipelineArrayLength)   Mode  Cnt        Score        Error  Units
DefaultChannelPipelineBenchmark.propagateEvent                 16                   1024  thrpt   50  6899710.275 ± 115305.284  ops/s
DefaultChannelPipelineBenchmark.propagateVariety               16                   1024  thrpt   50  7985208.748 ±  31532.793  ops/s
```
",2025-02-04T18:51:44Z,chrisvest,chrisvest,easier to read,"This is seems to be related to #13832 | executor, ctxCache seems the most relevant factor of the speedup | Here we go @chrisvest @normanmaurer
        private static long compilePattern(short maskToMatch) {
            return maskToMatch * 0x0001_0001_0001_0001L;
        }

        private static long packMasks(short mask1, short mask2) {
            // put the 3 masks into a single long every each 2 bytes
            return mask1 | (mask2 << 16);
        }

        private static long packMasks(short mask1, short mask2, short mask3) {
            // put the 3 masks into a single long every each 2 bytes
            return mask1 | (mask2 << 16) | ((long) mask3 << 32);
        }

        private static int indexOf(int mask, long masks, int containerSize) {
            short maskToMatch = (short) mask;
            long maskingMask = compilePattern(maskToMatch);
            long allMasksIncludingToFind = masks | ((long) maskToMatch << (containerSize * 16));
            long matchingMasks = allMasksIncludingToFind ^ maskingMask;
            // overflow one bit if there's a match
            long tmp = (matchingMasks & 0x7FFF_7FFF_7FFF_7FFFL) + 0x7FFF_7FFF_7FFF_7FFFL;
            long msbSetIfFound = ~(tmp | matchingMasks | 0x7FFF_7FFF_7FFF_7FFFL);
            int index = Long.numberOfTrailingZeros(msbSetIfFound) / 16;
            return index;
        }
It packs the masks ordered and place the one we search for in the last position i.e. containerSize
which can be second or third position.
We need to have AbstractChannelHandlerContext[] inbound/outbound arrays which contains an additional null slot to their size i.e. 3 and 4 capacity.
This will help the algorithm to return the last slot (which contain the mask we search for) and extracting null out of the array without any branch.
A different mechanism, which requires a 5 capacity array to store the contextes is
public static void main(String[] args) {
            long container = ContextCache.packMasks((short) 1, (short) 2, (short) 3);
            System.out.println(ContextCache.indexOf(1, container)); // print 0
            System.out.println(ContextCache.indexOf(2, container)); // print 1
            System.out.println(ContextCache.indexOf(3, container)); // print 2
            System.out.println(ContextCache.indexOf(0, container)); // print 3
            System.out.println(ContextCache.indexOf(4, container)); // print 4
            System.out.println(ContextCache.indexOf(5, container)); // print 4
            // we need a Object[] sized for 5 elements if we have 3 masks
            // we need a Object[] sized for 5 elements if we have 2 masks
        }

        private static long compilePattern(short maskToMatch) {
            return maskToMatch * 0x0001_0001_0001_0001L;
        }

        private static long packMasks(short mask1, short mask2, short mask3) {
            // put the 3 masks into a single long every each 2 bytes
            return mask1 | (mask2 << 16) | ((long) mask3 << 32);
        }

        private static int indexOf(int mask, long masks) {
            short maskToMatch = (short) mask;
            long maskingMask = compilePattern(maskToMatch);
            long matchingMasks = masks ^ maskingMask;
            // overflow one bit if there's a match
            long tmp = (matchingMasks & 0x7FFF_7FFF_7FFF_7FFFL) + 0x7FFF_7FFF_7FFF_7FFFL;
            long msbSetIfFound = ~(tmp | matchingMasks | 0x7FFF_7FFF_7FFF_7FFFL);
            int index = Long.numberOfTrailingZeros(msbSetIfFound) / 16;
            return index;
        }
And it save an additional couple of bit-tricks ops which can be costly in the hot path.
Beware comparing this with the previous mechanism using branches if the benchmark is not correctly making it as unpredictable as it would in the real world i.e. (each node is likely to share the same inlined version of the branches - meaning that based on the amount of jumps vs the requested mask, it can have mispredictions - which can cost hundreds of cycles, if hit. | @chrisvest let me know once I should review again in detail | @franz1981 Using the SWAR cache indexing approach (see 5150a04 for the changes) seems to have a fair bit of overhead, even when trying to hammer the branch predictor:
With branching:
Benchmark                                         (extraHandlers)  (pipelineArrayLength)   Mode  Cnt         Score        Error  Units
DefaultChannelPipelineBenchmark.propagateEvent                 16                   1024  thrpt   10   9359339.865 ± 279994.754  ops/s
DefaultChannelPipelineBenchmark.propagateVariety               16                   1024  thrpt   10  13078040.219 ± 285932.585  ops/s

With SWAR cache:
Benchmark                                         (extraHandlers)  (pipelineArrayLength)   Mode  Cnt         Score        Error  Units
DefaultChannelPipelineBenchmark.propagateEvent                 16                   1024  thrpt   50   7342312.864 ± 129490.288  ops/s
DefaultChannelPipelineBenchmark.propagateVariety               16                   1024  thrpt   50  10262412.530 ±  84772.980  ops/s | Reducing the context cache size to 2 entries per direction gets rid of the ugly divide. It's still behind compared to branching, though:
2-entry cache:
Benchmark                                         (extraHandlers)  (pipelineArrayLength)   Mode  Cnt         Score       Error  Units
DefaultChannelPipelineBenchmark.propagateEvent                 16                   1024  thrpt   50   7723965.179 ± 61180.335  ops/s
DefaultChannelPipelineBenchmark.propagateVariety               16                   1024  thrpt   50  11297468.759 ± 84817.476  ops/s | I will take a look there with perf to make sure it actually stress the branch predictor or not.
Did you get rid of the absent branch right? | @franz1981 By ""absent branch"" do you mean this?
https://github.com/netty/netty/pull/14705/files#diff-14b9f0e2bf3471c6dc6495bf8c2689e926ddc8f63b1a9608191497977f047a66R947-R949
We need to decide whether to scan the pipeline or not. We can't just return null. | I have to profile it to see what's going on | Another thing I can try is to remove the linked list structure of the handler context objects, and instead have arrays (plural, for the context and their mask) in the pipeline with each handler context knowing their own index. | Another thing I can try is to remove the linked list structure of the handler context objects, and instead have arrays (plural, for the context and their mask) in the pipeline with each handler context knowing their own index.

It should be the best thing to try IMO, and if lucky, the iteration of the mask array will be autovectorized too (no idea - I should check!) | These are the results for the benchmark and...
Benchmark                                                               (extraHandlers)  (pipelineArrayLength)   Mode  Cnt         Score         Error      Units
DefaultChannelPipelineBenchmark.propagateEvent                                       16                    128  thrpt   20  16006698.577 ± 1333560.526      ops/s
DefaultChannelPipelineBenchmark.propagateEvent:CPI                                   16                    128  thrpt    2         0.462                clks/insn
DefaultChannelPipelineBenchmark.propagateEvent:IPC                                   16                    128  thrpt    2         2.169                insns/clk
DefaultChannelPipelineBenchmark.propagateEvent:L1-dcache-load-misses                 16                    128  thrpt    2        23.758                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:L1-dcache-loads                       16                    128  thrpt    2       201.627                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:L1-dcache-stores                      16                    128  thrpt    2        47.412                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:L1-icache-load-misses                 16                    128  thrpt    2         0.094                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-load-misses                       16                    128  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-loads                             16                    128  thrpt    2         0.005                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-store-misses                      16                    128  thrpt    2         0.002                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-stores                            16                    128  thrpt    2         0.003                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:branch-misses                         16                    128  thrpt    2         0.949                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:branches                              16                    128  thrpt    2       124.401                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:cycles                                16                    128  thrpt    2       301.221                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-load-misses                      16                    128  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-loads                            16                    128  thrpt    2       201.422                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-store-misses                     16                    128  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-stores                           16                    128  thrpt    2        47.349                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:iTLB-load-misses                      16                    128  thrpt    2         0.005                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:instructions                          16                    128  thrpt    2       652.136                     #/op
DefaultChannelPipelineBenchmark.propagateEvent                                       16                   1024  thrpt   20   7816012.016 ±  131055.324      ops/s
DefaultChannelPipelineBenchmark.propagateEvent:CPI                                   16                   1024  thrpt    2         0.927                clks/insn
DefaultChannelPipelineBenchmark.propagateEvent:IPC                                   16                   1024  thrpt    2         1.079                insns/clk
DefaultChannelPipelineBenchmark.propagateEvent:L1-dcache-load-misses                 16                   1024  thrpt    2        25.781                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:L1-dcache-loads                       16                   1024  thrpt    2       205.366                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:L1-dcache-stores                      16                   1024  thrpt    2        44.440                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:L1-icache-load-misses                 16                   1024  thrpt    2         0.226                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-load-misses                       16                   1024  thrpt    2         0.004                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-loads                             16                   1024  thrpt    2         9.708                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-store-misses                      16                   1024  thrpt    2         0.007                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-stores                            16                   1024  thrpt    2         0.029                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:branch-misses                         16                   1024  thrpt    2         4.105                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:branches                              16                   1024  thrpt    2       133.905                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:cycles                                16                   1024  thrpt    2       616.374                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-load-misses                      16                   1024  thrpt    2         0.008                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-loads                            16                   1024  thrpt    2       204.257                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-store-misses                     16                   1024  thrpt    2         0.002                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-stores                           16                   1024  thrpt    2        44.061                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:iTLB-load-misses                      16                   1024  thrpt    2         0.049                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:instructions                          16                   1024  thrpt    2       665.049                     #/op
DefaultChannelPipelineBenchmark.propagateVariety                                     16                    128  thrpt   20  14064388.529 ±  231481.420      ops/s
DefaultChannelPipelineBenchmark.propagateVariety:CPI                                 16                    128  thrpt    2         0.579                clks/insn
DefaultChannelPipelineBenchmark.propagateVariety:IPC                                 16                    128  thrpt    2         1.726                insns/clk
DefaultChannelPipelineBenchmark.propagateVariety:L1-dcache-load-misses               16                    128  thrpt    2         5.341                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:L1-dcache-loads                     16                    128  thrpt    2       177.111                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:L1-dcache-stores                    16                    128  thrpt    2        45.098                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:L1-icache-load-misses               16                    128  thrpt    2         0.441                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-load-misses                     16                    128  thrpt    2         0.002                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-loads                           16                    128  thrpt    2         0.008                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-store-misses                    16                    128  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-stores                          16                    128  thrpt    2         0.003                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:branch-misses                       16                    128  thrpt    2         2.879                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:branches                            16                    128  thrpt    2       114.198                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:cycles                              16                    128  thrpt    2       335.541                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-load-misses                    16                    128  thrpt    2         0.004                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-loads                          16                    128  thrpt    2       177.801                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-store-misses                   16                    128  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-stores                         16                    128  thrpt    2        45.140                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:iTLB-load-misses                    16                    128  thrpt    2         0.070                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:instructions                        16                    128  thrpt    2       579.047                     #/op
DefaultChannelPipelineBenchmark.propagateVariety                                     16                   1024  thrpt   20  11382360.696 ±  121130.268      ops/s
DefaultChannelPipelineBenchmark.propagateVariety:CPI                                 16                   1024  thrpt    2         0.696                clks/insn
DefaultChannelPipelineBenchmark.propagateVariety:IPC                                 16                   1024  thrpt    2         1.437                insns/clk
DefaultChannelPipelineBenchmark.propagateVariety:L1-dcache-load-misses               16                   1024  thrpt    2         5.188                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:L1-dcache-loads                     16                   1024  thrpt    2       184.606                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:L1-dcache-stores                    16                   1024  thrpt    2        49.352                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:L1-icache-load-misses               16                   1024  thrpt    2         0.204                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-load-misses                     16                   1024  thrpt    2         0.002                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-loads                           16                   1024  thrpt    2         0.906                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-store-misses                    16                   1024  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-stores                          16                   1024  thrpt    2         0.008                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:branch-misses                       16                   1024  thrpt    2         4.304                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:branches                            16                   1024  thrpt    2       125.317                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:cycles                              16                   1024  thrpt    2       417.662                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-load-misses                    16                   1024  thrpt    2         0.009                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-loads                          16                   1024  thrpt    2       184.713                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-store-misses                   16                   1024  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-stores                         16                   1024  thrpt    2        49.284                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:iTLB-load-misses                    16                   1024  thrpt    2         0.076                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:instructions                        16                   1024  thrpt    2       600.236                     #/op

This is for the SWAR case, branch-misses for the 128 case are twice the ones for 1024 (which is good), but I have to compare this against the non-SWAR to see what's going on (see branches too). | This instead is
377b4b119e07409fc5f375eb8ff91cb519077c5c
Benchmark                                                               (extraHandlers)  (pipelineArrayLength)   Mode  Cnt         Score         Error      Units
DefaultChannelPipelineBenchmark.propagateEvent                                       16                    128  thrpt   20  15670466.633 ± 1064987.960      ops/s
DefaultChannelPipelineBenchmark.propagateEvent:CPI                                   16                    128  thrpt    2         0.541                clks/insn
DefaultChannelPipelineBenchmark.propagateEvent:IPC                                   16                    128  thrpt    2         1.851                insns/clk
DefaultChannelPipelineBenchmark.propagateEvent:L1-dcache-load-misses                 16                    128  thrpt    2        24.631                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:L1-dcache-loads                       16                    128  thrpt    2       192.844                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:L1-dcache-stores                      16                    128  thrpt    2        40.886                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:L1-icache-load-misses                 16                    128  thrpt    2         0.108                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-load-misses                       16                    128  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-loads                             16                    128  thrpt    2         0.007                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-store-misses                      16                    128  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-stores                            16                    128  thrpt    2         0.003                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:branch-misses                         16                    128  thrpt    2         1.761                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:branches                              16                    128  thrpt    2       127.584                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:cycles                                16                    128  thrpt    2       309.245                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-load-misses                      16                    128  thrpt    2         0.008                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-loads                            16                    128  thrpt    2       193.229                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-store-misses                     16                    128  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-stores                           16                    128  thrpt    2        40.915                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:iTLB-load-misses                      16                    128  thrpt    2         0.025                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:instructions                          16                    128  thrpt    2       570.841                     #/op
DefaultChannelPipelineBenchmark.propagateEvent                                       16                   1024  thrpt   20   8731116.667 ±  165242.286      ops/s
DefaultChannelPipelineBenchmark.propagateEvent:CPI                                   16                   1024  thrpt    2         0.941                clks/insn
DefaultChannelPipelineBenchmark.propagateEvent:IPC                                   16                   1024  thrpt    2         1.062                insns/clk
DefaultChannelPipelineBenchmark.propagateEvent:L1-dcache-load-misses                 16                   1024  thrpt    2        25.659                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:L1-dcache-loads                       16                   1024  thrpt    2       195.697                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:L1-dcache-stores                      16                   1024  thrpt    2        42.075                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:L1-icache-load-misses                 16                   1024  thrpt    2         0.198                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-load-misses                       16                   1024  thrpt    2         0.003                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-loads                             16                   1024  thrpt    2         7.554                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-store-misses                      16                   1024  thrpt    2         0.004                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:LLC-stores                            16                   1024  thrpt    2         0.022                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:branch-misses                         16                   1024  thrpt    2         3.995                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:branches                              16                   1024  thrpt    2       136.987                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:cycles                                16                   1024  thrpt    2       548.005                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-load-misses                      16                   1024  thrpt    2         0.005                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-loads                            16                   1024  thrpt    2       194.440                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-store-misses                     16                   1024  thrpt    2         0.002                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:dTLB-stores                           16                   1024  thrpt    2        41.412                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:iTLB-load-misses                      16                   1024  thrpt    2         0.068                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:instructions                          16                   1024  thrpt    2       582.113                     #/op
DefaultChannelPipelineBenchmark.propagateEvent:·async                                16                   1024  thrpt                NaN                      ---
DefaultChannelPipelineBenchmark.propagateVariety                                     16                    128  thrpt   20  14727534.875 ±  143356.129      ops/s
DefaultChannelPipelineBenchmark.propagateVariety:CPI                                 16                    128  thrpt    2         0.592                clks/insn
DefaultChannelPipelineBenchmark.propagateVariety:IPC                                 16                    128  thrpt    2         1.689                insns/clk
DefaultChannelPipelineBenchmark.propagateVariety:L1-dcache-load-misses               16                    128  thrpt    2         5.281                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:L1-dcache-loads                     16                    128  thrpt    2       188.000                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:L1-dcache-stores                    16                    128  thrpt    2        54.575                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:L1-icache-load-misses               16                    128  thrpt    2         0.245                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-load-misses                     16                    128  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-loads                           16                    128  thrpt    2         0.008                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-store-misses                    16                    128  thrpt    2         0.002                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-stores                          16                    128  thrpt    2         0.003                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:branch-misses                       16                    128  thrpt    2         2.767                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:branches                            16                    128  thrpt    2       121.349                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:cycles                              16                    128  thrpt    2       316.889                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-load-misses                    16                    128  thrpt    2         0.011                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-loads                          16                    128  thrpt    2       187.304                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-store-misses                   16                    128  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-stores                         16                    128  thrpt    2        54.352                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:iTLB-load-misses                    16                    128  thrpt    2         0.002                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:instructions                        16                    128  thrpt    2       535.174                     #/op
DefaultChannelPipelineBenchmark.propagateVariety                                     16                   1024  thrpt   20  12449750.020 ±  167364.111      ops/s
DefaultChannelPipelineBenchmark.propagateVariety:CPI                                 16                   1024  thrpt    2         0.701                clks/insn
DefaultChannelPipelineBenchmark.propagateVariety:IPC                                 16                   1024  thrpt    2         1.427                insns/clk
DefaultChannelPipelineBenchmark.propagateVariety:L1-dcache-load-misses               16                   1024  thrpt    2         5.252                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:L1-dcache-loads                     16                   1024  thrpt    2       182.632                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:L1-dcache-stores                    16                   1024  thrpt    2        47.230                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:L1-icache-load-misses               16                   1024  thrpt    2         0.303                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-load-misses                     16                   1024  thrpt    2         0.007                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-loads                           16                   1024  thrpt    2         0.669                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-store-misses                    16                   1024  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:LLC-stores                          16                   1024  thrpt    2         0.007                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:branch-misses                       16                   1024  thrpt    2         4.393                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:branches                            16                   1024  thrpt    2       128.163                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:cycles                              16                   1024  thrpt    2       378.239                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-load-misses                    16                   1024  thrpt    2         0.009                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-loads                          16                   1024  thrpt    2       182.740                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-store-misses                   16                   1024  thrpt    2         0.001                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:dTLB-stores                         16                   1024  thrpt    2        47.154                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:iTLB-load-misses                    16                   1024  thrpt    2         0.044                     #/op
DefaultChannelPipelineBenchmark.propagateVariety:instructions                        16                   1024  thrpt    2       539.857                     #/op

and similar to your findings @chrisvest the SWAR method is not improving things, in particular for the propagateEvent: case and indeed the number of mispredicts is quite similar - and so the number of branches performed.
Still, it's evident that the misprediction rate affect results, but is unclear yet where (i.e. is to investigate) it's happening. | @franz1981 I'm working on converting the linked list traversal to an int array scan. Maybe then we won't need to do any caching at all. It's a bit of a bigger change as all the pipeline mutation code needs to change. | The fact that I got
....[Hottest Regions]...............................................................................
   9.08%          c2, level 4  io.netty.microbench.channel.DefaultChannelPipelineBenchmark::propagateEvent, version 2, compile id 1320 
   8.88%                       <unknown> 

and

shows that itable_stub account for a large part of the cost of the benchmark (this is using a 4096 sized array!) - and since the itable has to find the implementation of channelReadComplete among few, I can see why the branch misses there could be relevant - with more unpredictable samples.
At the same time it means that, since the dispatch cost is larger as we perform more virtual calls - being able to ""skip"" in the right handler is the most important thing: the more we perform unpredictable virtual calls, the worse - to not mention some data deps to climb the chain.
So, TLDR what's going on?

it looks like the branches we care the most are the virtual calls
since the very first find ctx seems relevant, but is well predicted (need to investigate why - it could be that the benchmark doesn't stress it enough?) with the non-SWAR method, it can deliver a good speed up because it saves more data dependent loads (1 more?) | The int array scan is the way to go imo | Pushed the int array scan, which also removes the caching entirely. The pipeline is now maintained as two copy-on-write arrays, which makes pipeline modifications more expensive.
Unfortunately, the results are not that encouraging:. This PR with 11d0f0b:
Benchmark                                         (extraHandlers)  (pipelineArrayLength)   Mode  Cnt        Score        Error  Units
DefaultChannelPipelineBenchmark.propagateEvent                 16                   1024  thrpt   50  6906835.242 ± 126943.659  ops/s
DefaultChannelPipelineBenchmark.propagateVariety               16                   1024  thrpt   50  8662131.451 ±  55463.381  ops/s

Compared to #14724:
Benchmark                                         (extraHandlers)  (pipelineArrayLength)   Mode  Cnt        Score        Error  Units
DefaultChannelPipelineBenchmark.propagateEvent                 16                   1024  thrpt   10  6515502.983 ± 253375.107  ops/s
DefaultChannelPipelineBenchmark.propagateVariety               16                   1024  thrpt   10  7628162.835 ±  40168.895  ops/s

It turns out as part of the skipContext check, we need to inspect the executor field of each context we iterate.
This makes this more involved than just iterating the int array and checking the mask, as we still have some dependent loads through the handlerContexts array and its entries.
So I think we are not getting any benefit from using arrays, and the performance boost seen is likely just the inlining and the executor field caching.
The array will probably do better in Netty 5 where the executor() has no bearing on the execution mask. | What you can do is to bring the executor's identity hash code (which I know, is costly to install) packed in the int with the mask as a long and you can just search for the right hash + mask, falling back to actual reading the executor field only if the hash is the same.
Apple M1 have some monstrous paralleilsm and just having an addition int[] for the identities shouldn't hurt AFAIK, but if we want to play safe, a packed pair of ints as long should work decently since that using the lowest 32bits in x86 is pretty cheap.
In theory you can have an additional array of Executor[] to search for...but maybe there are good reasons which prevent to use it.
Said that, this will still suffer a bit because of branch misses, unless searching the long[] array won't be vectorized - in such case we would perform much less branches and reducing a bit the misprediction chance. | What you can do is to bring the executor's identity hash code (which I know, is costly to install) packed in the int with the mask as a long and you can just search for the right hash + mask, falling back to actual reading the executor field only if the hash is the same.

We can't do this because channels need to be registered before they get an executor.
It's too hacky to try to patch that up, so I think we should just do the inlining and contextExecutor changes for now. | With the reduced changes we get
Benchmark                                         (extraHandlers)  (pipelineArrayLength)   Mode  Cnt        Score        Error  Units
DefaultChannelPipelineBenchmark.propagateEvent                 16                   1024  thrpt   50  6899710.275 ± 115305.284  ops/s
DefaultChannelPipelineBenchmark.propagateVariety               16                   1024  thrpt   50  7985208.748 ±  31532.793  ops/s

Which is still nice. | @normanmaurer addressed your comments | Test failures are related to #14773","Reduce pipeline stack depth and improve performance

Motivation:
Pipeline calls, such as `ctx.fireChannelRead` and `ctx.write`, currently go through multiple methods.
This increases the stack depth in event loop threads, which makes it harder to debug (people have more ceremony mixed in with their code), and hurts performance (JIT inliner budget gets used up faster).

Furthermore, there are other places in this machinery which can be made faster.

Modification:

**1.**
In the `AbstractChannelHandlerContext` all inbound methods, e.g. `fireChannelRead`, have been manually inlined.
This means that when a handler calls `ctx.fireChannelRead`, this method call in turn now directly call the `channelRead` method of the _next_ handler in the pipeline.
Previously, we had two extra method calls here.

As a consequence of this inlining, we now have to re-compute the target context when we trampoline tasks onto the event-loop.
This is presumably rare, and worth the cost.
This also means that some code now moves from the executor of the target context, to the executor of the calling context.
This can create different behaviors from Netty 4.1, if the pipeline has multiple handlers, is modified by the handlers during the call, and the handlers use child-executors.

**2.**
A few outbound methods - `read`, `write`, `writeAndFlush` - are likewise inlined.
Their inherent complexity and number of overloads means we can't realistically get them down to a single method call, but we can get them down to two.
This is still a nice improvement.
The `flush` method is usually not implemented by handlers, so there's no point in inlining that further.

**3.**
In every such call, after finding the next context, we have to decide if we can call the handler directly, or need to trampoline onto a different event loop (due to the executor off-loading feature).
This means we have to inspect the context and either pick its child executor, or load the channel event loop of the target context, and this latter part (which is the most common case) is a sequence of dependent loads.
Dependent loads cause cache misses and CPU pipeline stalls, so to deal with this the `AbstractChannelHandlerContext` now has a `contextExecutor` field, which caches the result of computing the concrete executor.
This means our executor is only one dependent (on the channel handler context) load away.
The speedup from this is quite noticeable because it's such a common operation.

**4.**
The final hot-spot fixed here, is that we were iterating the pipeline context objects on each call, in order to find the next context that actually implements the method we want to call.
The skip and execution mask mechanism is itself a very profitable optimisation, but we can do better.
Iterating the pipeline is another sequence of dependent loads, and it involves volatile loads in each iteration.

A new `ContextCache` object is added to the `AbstractChannelHandlerContext`, which caches 2 outbound lookups, e.g. `writeAndFlush` or `read`, and it caches 3 inbound methods, e.g. `fireChannelRead`, `fireChannelReadComplete`, `fireChannelWritabilityChanged`.
The `ContextCache` object is stored in a volatile field, and all pipeline modifying operations, like `pipeline.addLast()`, will iterate the pipeline and clear these caches.

The cache significantly improves the performance in microbenchmarks.

Results:
The `DefaultChannelPipelineBenchmark` on my M1 Pro, running JDK 17, tells an encouraging story:

```
Before:
Benchmark                                       (extraHandlers)   Mode  Cnt       Score      Error  Units
DefaultChannelPipelineBenchmark.propagateEvent                4  thrpt   25  214516.225 ± 7724.391  ops/s

After (inline-inbound):
Benchmark                                       (extraHandlers)   Mode  Cnt       Score      Error  Units
DefaultChannelPipelineBenchmark.propagateEvent                4  thrpt   25  222729.436 ± 6440.818  ops/s
After (inline-inbound, inline-outbound):
Benchmark                                       (extraHandlers)   Mode  Cnt       Score      Error  Units
DefaultChannelPipelineBenchmark.propagateEvent                4  thrpt   25  225200.265 ± 6325.982  ops/s
After (inline-inbound, inline-outbound, executor):
Benchmark                                       (extraHandlers)   Mode  Cnt       Score       Error  Units
DefaultChannelPipelineBenchmark.propagateEvent                4  thrpt   25  300374.903 ± 17051.143  ops/s
After (inline-inbound, inline-outbound, executor, ctxCache):
Benchmark                                       (extraHandlers)   Mode  Cnt       Score      Error  Units
DefaultChannelPipelineBenchmark.propagateEvent                4  thrpt   25  475873.309 ± 7018.985  ops/s
``` | Fix bug with propagating read() on the wrong handler | Try to make the DefaultChannelPipelineBenchmark more realistic | Remove a bit of unnecessary indirection | Add comment explaining `contextExecutor` | Use SWAR cache indexing | Reduce context cache size to 2 entries per direction | Replace pipeline linked list with arrays | Revert context caching and array pipeline | Apply review comments | Properly implement inEventLoop for DefaultChannelPipelineTest"
netty/netty,14314,https://github.com/netty/netty/pull/14314,AdaptiveByteBufAllocator: Make code more robust in terms of possible …,"…leaks

Motivation:

The allocation code still had some cases where in theory it would have been possible to leak Chunks. Beside this it was a bit hard to read.

Modifications:

- Make handling more robust
- Add comments to make code easier to understand for future readers / contributors

Result:

More robust handling, easier to understand",2024-09-11T17:47:04Z,normanmaurer,normanmaurer,easier to read,,"AdaptiveByteBufAllocator: Make code more robust in terms of possible leaks

Motivation:

The allocation code still had some cases where in theory it would have been possible to leak Chunks. Beside this it was a bit hard to read.

Modifications:

- Make handling more robust
- Add comments to make code easier to understand for future readers / contributors

Result:

More robust handling, easier to understand"
netty/netty,14193,https://github.com/netty/netty/pull/14193,Rectify socket half-closed behavior with 4.1,"Motivation:

For certain IO implementations the behavior of the socket-half-closed
signals has changed since 4.1 where even without a read set a channel
would finish reading and emit the ChannelInputShutdownReadComplete
event. In 4.2 this has regressed for both epoll and io_uring
implementations.

Modifications:

- Change the half-closed logic for epoll and io_uring to properly emit the
  signals.
- Also had to fix a few bugs in io_uring.",2024-07-30T15:59:10Z,bryce-anderson,bryce-anderson,easier to read,"@normanmaurer, a question I have is whether we want to fix this. It might actually be better behavior since it doesn't cause the pipeline to spontaneously emit data if there isn't a read pending. I have what might be a fix for ST here although some other tests are flaky and I'm not sure yet if it's fallout or just other unrelated tests that are flaky: apple/servicetalk#3015 | @bryce-anderson I think we should pull in the change to not change behavior between 4.1 and 4.2 | Good thing that you pointed out that the io_uring tests were broken, because the builds are passing and, turns out, all those tests were being skipped in our builds!
Opened a PR to find out why: #14202 | Good thing that you pointed out that the io_uring tests were broken, because the builds are passing and, turns out, all those tests were being skipped in our builds! Opened a PR to find out why: #14202

I was surprised by the green check as well when they are failing locally for me. | I made some local changes to AbstractIoUringChannel to get the test to progress until the point you previously had marked as ""failing here"".
Maybe with this, and changes mirroring what you did to Epoll, it'll work?
Index: transport-classes-io_uring/src/main/java/io/netty/channel/uring/AbstractIoUringChannel.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/transport-classes-io_uring/src/main/java/io/netty/channel/uring/AbstractIoUringChannel.java b/transport-classes-io_uring/src/main/java/io/netty/channel/uring/AbstractIoUringChannel.java
--- a/transport-classes-io_uring/src/main/java/io/netty/channel/uring/AbstractIoUringChannel.java	(revision ba096a57eb7b4d747db09076779f285d4190a212)
+++ b/transport-classes-io_uring/src/main/java/io/netty/channel/uring/AbstractIoUringChannel.java	(date 1722038376739)
@@ -34,6 +34,7 @@
 import io.netty.channel.IoEventLoop;
 import io.netty.channel.IoRegistration;
 import io.netty.channel.RecvByteBufAllocator;
+import io.netty.channel.ServerChannel;
 import io.netty.channel.socket.ChannelInputShutdownEvent;
 import io.netty.channel.socket.ChannelInputShutdownReadComplete;
 import io.netty.channel.socket.SocketChannelConfig;
@@ -126,10 +127,10 @@
             this.remote = socket.remoteAddress();
         }
 
-        if (parent != null) {
-            logger.trace(""Create Channel Socket: {}"", socket.intValue());
+        if (this instanceof ServerChannel) {
+            logger.trace(""Create Server Socket: {}"", socket.intValue());
         } else {
-            logger.trace(""Create Server Socket: {}"", socket.intValue());
+            logger.trace(""Create Channel Socket: {}"", socket.intValue());
         }
     }
 
@@ -277,7 +278,7 @@
             // We already have a read pending.
             return;
         }
-        if (inReadComplete) {
+        if (inReadComplete || !isActive()) {
             // We are currently in the readComplete(...) callback which might issue more reads by itself. Just
             // mark it as a pending read. If readComplete(...) will not issue more reads itself it will pick up
             // the readPending flag, reset it and call doBeginReadNow().
@@ -878,6 +879,9 @@
                 try {
                     if (res == 0) {
                         fulfillConnectPromise(connectPromise, active);
+                        if (readPending) {
+                            doBeginReadNow();
+                        }
                     } else if (res != Native.ERRNO_ECANCELED_NEGATIVE) {
                         try {
                             Errors.throwConnectException(""io_uring connect"", res); | I made some local changes to AbstractIoUringChannel to get the test to progress until the point you previously had marked as ""failing here"".
Maybe with this, and changes mirroring what you did to Epoll, it'll work?

This seems to have done the trick. Thanks @chrisvest. 🙏","This seems like a repro | Cleanup | fix checkstyle | This might be a slightly cleaner fix | Some cleanup, but now its broken with io_uring | Remove unused import | Also dont test OIO socket channels | Chris feedback | Chris suggestions | Less janky skipping of nio and oio clients"
netty/netty,14045,https://github.com/netty/netty/pull/14045,Integrate io_uring transport into 4.2,"Motivation:

We should include our io_uring transport as part of 4.2

Modifications:

Add io_uring transport based on our incubator version and main version

Result:

Fixes https://github.com/netty/netty/issues/14010",2024-05-15T08:23:48Z,normanmaurer,normanmaurer,easier to read,"This ""version"" is a combination of what we had in netty-incubator-transport-native-io_uring and our main branch.
You might wonder why not pick one or the other, the reason is that both had its pros and cons and so I tried to merge these to also work with our new IoHandler design.
The good news is that with this implementation it should be trivial to support things like file IO, as all it would take is to write a FileIoHandle that you can register and then use the returned IOUringIoRegistration to submit the relevant IOUringIoOps and react to the IOUringIoEvent.
Beside this I also want to take another stab on the IOUringDatagramChannel implementation and use a blocking fd there as well with fastpoll. I will work on this while already give you some chance to have a look at the code.
We should also remove all the methods on the SubmissionQueue that are now not used anymore as Channel implementations are using the IOUringIoRegistration to submit work to the SubmissionQueue. | Also in the future we should look into using provided buffers: https://github.com/axboe/liburing/wiki/io_uring-and-networking-in-2023#provided-buffers | And I think we also should move from IOUring* to IoUring* for class / interface names. This is more inline with others like Nio etc. | And before I forget shout out to @axboe for all the help in answering my questions and also for working on future changes for io_uring based on some of my feedback. OSS rocks :) | First round seems ok, but wanna run it and see if I am convinced the same :P

For sure :) Keep me posted.

Related the dead code, that's why is not yet approved: need to delete quite a few no longer used methods!

Yes will do more cleanup etc. Just wanted to show something first | Need to investigate why I see some AssertionError on the CI while I never see it here... | Need to investigate why I see some AssertionError on the CI while I never see it here...

Nevermind I did mess up while cleanup ... should be fixed now | @franz1981 @chrisvest I think this is ready to go now. | I will rename everything from IOUring* to IoUring* before pulling it in. I just wanted to keep the name like it is for now to make it  easier to compare to main and the incubator version. | It's a lot to review, but I'm having a look now. | Had some comments. Mostly small stuff.

@chrisvest addressed and also did some more simplification... PTAL again | Thanks a lot for everyone involved!","Integrate io_uring transport into 4.2

Motivation:

We should include our io_uring transport as part of 4.2

Modifications:

Add io_uring transport based on our incubator version and main version

Result:

Fixes https://github.com/netty/netty/issues/14010 | Rebase and make it work on latest 4.2 branch | Cleanup | Correctly fix sendmsg used for TCP_FAST_OPEN and also fix a double submit that ca
used assert error | Remove unused methods / classes and reduce visibility of config implementations | finish impl for datagram | Ensure io_uring can also be used when we dont have a fd yet | move ops id generation to AbstractIoUringChannel as its an implementation detail | Let IoRegistration.submit(...) return a long | Fix typo in comment | Use IORING_CQE_F_SOCK_NONEMPTY with datagrams as well | cleanup | remove link method | Remove unused property | Apply suggestions from code review

Co-authored-by: Chris Vest <christianvest_hansen@apple.com> | Add comments | Use handle | Remove id from IoUringIoOps and IOUringIoRegistration as we can just make this an implementation detail | Add asserts to clarify | Tighten up visibility | More asserts for the win | Fix copyright year | Last cleanup | Rename to be more inline with other namings"
netty/netty,12133,https://github.com/netty/netty/pull/12133,Add BufferConversionHandler,"Motivation:
Migrating code from one buffer API to the next will require a transition period.
For this purpose, it is useful to have a handler that can convert between the buffer types, so new and old handlers alike can participate in the same pipeline.

Modification:
Add a BufferConversionHandler that can be configured to convert reads, writes, and user events, between ByteBuf and Buffer.
Messages that are not either ByteBufs or Buffers will be passed through unchanged.

Result:
It is now easier to build pipelines that make use of both kinds of buffers.
Note that higher-level messages, such as HttpRequest or ByteBufHolder implementers, cannot be automatically converted in this way.
These objects pass through this handler unchanged.",2022-02-28T21:50:23Z,chrisvest,chrisvest,easier to read,,"Add BufferConversionHandler

Motivation:
Migrating code from one buffer API to the next will require a transition period.
For this purpose, it is useful to have a handler that can convert between the buffer types, so new and old handlers alike can participate in the same pipeline.

Modification:
Add a BufferConversionHandler that can be configured to convert reads, writes, and user events, between ByteBuf and Buffer.
Messages that are not either ByteBufs or Buffers will be passed through unchanged.

Result:
It is now easier to build pipelines that make use of both kinds of buffers.
Note that higher-level messages, such as HttpRequest or ByteBufHolder implementers, cannot be automatically converted in this way.
These objects pass through this handler unchanged."
netty/netty,13857,https://github.com/netty/netty/pull/13857,Save array bound-checks on AsciiString::contentEqualsIgnoreCase,"Motivation:

Http headers decoding cache known header names (eg Connection) with first letter upper-case, while users would lookup the same, while using the HttpHeaderNames's form, which is fully lower-case, causing frequent AsciiString::contentEqualsIgnoreCase calls

Modification:

Speedup containEqualsIgnoreCase by caching the compared AsciiString underlying's byte[] as local variable, to save bound-checks during the comparison

Result:

Faster HTTP 1.1 header names lookups on the decoded HTTP requests",2024-02-21T09:00:30Z,normanmaurer,franz1981,easier to read,"This is not a huge win, but is a free-lunch, still
this PR:
Benchmark                                   (size)   Mode  Cnt          Score         Error  Units
AsciiStringBenchmark.equalsIgnoreCaseBench       3  thrpt   10  162444616.617 ± 1327.324  ops/s

while 4.1:
Benchmark                                   (size)   Mode  Cnt          Score          Error  Units
AsciiStringBenchmark.equalsIgnoreCaseBench       3  thrpt   10  142999129.476 ± 1021.660  ops/s | I've further improved performance, as usual, by making the common path easier for the JIT
now getting
Benchmark                                   (size)   Mode  Cnt          Score         Error  Units
AsciiStringBenchmark.equalsIgnoreCaseBench       3  thrpt   10  187713249.718 ± 2702.056  ops/s

And I think it could be made even faster by using AsciiString::cache to perform a quick isLowerCase  on the cached AsciiString saving a byte to know if is a full lowercase String aready (which is the most common use case for cached ones).
This will enable super quick equality checks for known HTTP 1.1 lower-case header names and faster ones for cases like the one used in the benchmark (eg Accept vs accept). wdyt @normanmaurer @idelpivnitskiy ? | Fore reference, I've added 81fbc5ac486d4c7554f041f5baedd17d5f8a9b2b which remove the array bound-checks almost completely for this array, but not the other, and getting
Benchmark                                   (size)   Mode  Cnt          Score          Error  Units
AsciiStringBenchmark.equalsIgnoreCaseBench       3  thrpt   10  197791068.769 ±  1767.922  ops/s

And if I uses PlatformDependent::getByte for the other array I can get  another bump till ~210 M ops/sec, but i'm trying hard to avoid using PlatformDependent::getByte because, if we're unlucky with inlining, could make a single-byte access to be paid a LOT. | @franz1981 let me know once it is ready for review | @normanmaurer I'm still digging into the JIT code because I'm surprised the JIT is not doing the ""right thing"" (TM) here, really | ok @normanmaurer , given that using PlatformDependent seems too much here. IMO, let's go with this as it is (for the review) | PTAL @bryce-anderson @normanmaurer | LGTM | @franz1981 amazing perf work as usual <3","Save array bound-checks on AsciiString::contentEqualsIgnoreCase

Motivation:

Http headers decoding cache known header names (eg Connection) with first letter upper-case, while users would lookup the same, while using the HttpHeaderNames's form, which is fully lower-case, causing frequent AsciiString::contentEqualsIgnoreCase calls

Modification:

Speedup containEqualsIgnoreCase by caching the compared AsciiString underlying's byte[] as local variable, to save bound-checks during the comparison

Result:

Faster HTTP 1.1 header names lookups on the decoded HTTP requests | Adding aligned/misaligned versions | Further reduce bound-checks | addressing Bryce comments"
netty/netty,13440,https://github.com/netty/netty/pull/13440,Rewrite DnsQueryContextManager to simplify and make it easier to debug,"Motivation:

We are currently investigate an issue where it seems that we receive un unexpected hostname in the response. To make it easier to debug let us simplify the DnsQueryContextManager

Modifications:

- Move extra logic to internal class
- Add asserts

Result:

Cleanup
",2023-06-14T06:31:17Z,normanmaurer,normanmaurer,easier to read,"Seems like a common ground in dns querying. I have not read the code, have the questions in response been validated？","Rewrite DnsQueryContextManager to simplify and make it easier to debug

Motivation:

We are currently investigate an issue where it seems that we receive un unexpected hostname in the response. To make it easier to debug let us simplify the DnsQueryContextManager

Modifications:

- Move extra logic to internal class
- Add asserts

Result:

Cleanup | Add assert messages"
netty/netty,13436,https://github.com/netty/netty/pull/13436,Always enable leak tracking for derived buffers if parent is tracked,"Motivation:

Currently, when leak tracking is set to SIMPLE or ADVANCED, and a derived buffer is created, the buffer is selected for leak tracking if the parent buffer is tracked and the derived buffer is also selected by an independent roll of the dice. As a result, deeply derived buffers are exponentially less likely to be selected for leak tracking. This makes it difficult to find and diagnose leaks related to such buffers.

Modifications:

When a derived buffer is created, and the parent buffer has leak tracking, the derived buffer now always has leak tracking applied.

Result:

Leaks of derived buffers are easier to find. Fixes #13414.",2023-06-13T08:04:45Z,normanmaurer,rdicroce,easier to read,"@normanmaurer I added a trackForcibly() method in ResourceLeakDetector and marked it final since track() is also final. But the verify-pr job is complaining about adding a final method to a non-final class. What do you want me to do about this? Just remove the final modifier? | @rdicroce yep | The failure for windows-x86_64-java11-boringssl is unrelated to this change:

2023-06-12T16:21:47.3979582Z [ERROR] Failed to execute goal on project netty-codec-http: Could not resolve dependencies for project io.netty:netty-codec-http:jar:4.1.94.Final-SNAPSHOT: Failed to collect dependencies at commons-io:commons-io:jar:2.8.0: Failed to read artifact descriptor for commons-io:commons-io:jar:2.8.0: The following artifacts could not be resolved: commons-io:commons-io:pom:2.8.0 (absent): Could not transfer artifact commons-io:commons-io:pom:2.8.0 from/to central (https://repo.maven.apache.org/maven2): Connection reset -> [Help 1] | @chrisvest can you check if this applies to main as well ? | Thanks for the quick review! | @normanmaurer The main branch does not suffer from the problem fixed here.","Always enable leak tracking for derived buffers if parent is tracked

Motivation:

Currently, when leak tracking is set to SIMPLE or ADVANCED, and a
derived buffer is created, the buffer is selected for leak tracking if
the parent buffer is tracked and the derived buffer is also selected by
an independent roll of the dice. As a result, deeply derived buffers are
exponentially less likely to be selected for leak tracking. This makes
it difficult to find and diagnose leaks related to such buffers.

Modifications:

When a derived buffer is created, and the parent buffer has leak
tracking, the derived buffer now always has leak tracking applied.

Result:

Leaks of derived buffers are easier to find. | Fix trailing whitespace | Remove final modified from trackForcibly()"
netty/netty,13186,https://github.com/netty/netty/pull/13186,Add handling of inflight lookups to reduce real queries when lookup s…,"…ame hostname

Motivation:

To reduce the overhead we can shortcut queries for the same hostname until the first query comes back. This will reduce queries and so overhead

Modifications:

- Add new builder option that allows to shortcut queries to the same hostname (default this is disabled)

Result:

Be able to reduce queries
",2023-02-08T15:39:28Z,normanmaurer,normanmaurer,easier to read,Needs some tests ... | PTAL again...also added a test case | @idelpivnitskiy @chrisvest @Scottmitch any idea for a better name of the builder method ?,"Add handling of inflight lookups to reduce real queries when lookup same hostname

Motivation:

To reduce the overhead we can shortcut queries for the same hostname until the first query comes back. This will reduce queries and so overhead

Modifications:

- Add new builder option that allows to shortcut queries to the same hostname (default this is disabled)

Result:

Be able to reduce queries"
netty/netty,12939,https://github.com/netty/netty/pull/12939,Make it possible to complete and compact bytes in the `IovecArray`,"Motivation:
When we experience incomplete reads or writes, it can be useful to compact and reuse the same `IovecArray` that the first IO was submitted with. Like a smaller, more local version of what the `OutboundBuffer` is doing. This makes it easier to drive certain system calls to ""completion"" in some cases.

Modification:
Remove the unused `writtenMessages` method, and put a `completeBytes` method in its place. The `completeBytes` method update length and address of partially completed iovecs, and move the remaining incomplete iovecs to the start of the array, allowing more to be added if need be.

Result:
The `IovecArray` is easier to reuse when faced with incomplete IO.",2022-10-31T19:01:28Z,chrisvest,chrisvest,easier to read,,"Make it possible to complete and compact bytes in the `IovecArray`

Motivation:
When we experience incomplete reads or writes, it can be useful to compact and reuse the same `IovecArray` that the first IO was submitted with.
Like a smaller, more local version of what the `OutboundBuffer` is doing.
This makes it easier to drive certain system calls to ""completion"" in some cases.

Modification:
Remove the unused `writtenMessages` method, and put a `completeBytes` method in its place.
The `completeBytes` method update length and address of partially completed iovecs, and move the remaining incomplete iovecs to the start of the array, allowing more to be added if need be.

Result:
The `IovecArray` is easier to reuse when faced with incomplete IO. | Unwrap else-clauses

Co-authored-by: Norman Maurer <norman_maurer@apple.com> | Fix compilation error"
netty/netty,12693,https://github.com/netty/netty/pull/12693,Move logic for read handling into AbstractChannel,"Motivation:

There was a lot of duplication in the different Channel implementation when it comes to handling reads. To make things more consistent and also easier for transport implementators we should move all the complexity into AbstractChannel.

Modifications:

- Introduce AbstractChannel.readNow(...) which should be called when there is data to read from the underlying transport (like a socket).
- Add writeFlushedNow() which should be called when the underlying transport becomes writable again
- Move readPending tracking and autoRead logic into AbstractChannel
- Thighten up access and visibility
- Remove a lot of code duplication

Result:

Cleaner and more consistent code. Beside this also easier to write a custom Channel / Transport implementation
",2022-08-13T09:10:11Z,normanmaurer,normanmaurer,easier to read,,"Move logic for read handling into AbstractChannel

Motivation:

There was a lot of duplication in the different Channel implementation when it comes to handling reads. To make things more consistent and also easier for transport implementators we should move all the complexity into AbstractChannel.

Modifications:

- Introduce AbstractChannel.readNow(...) which should be called when there is data to read from the underlying transport (like a socket).
- Add writeFlushedNow() which should be called when the underlying transport becomes writable again
- Move readPending tracking and autoRead logic into AbstractChannel
- Thighten up access and visibility
- Remove a lot of code duplication

Result:

Cleaner and more consistent code. Beside this also easier to write a custom Channel / Transport implementation | Rename method"
netty/netty,12735,https://github.com/netty/netty/pull/12735,Migrate HTTP/1 and HTTP/2 to the new headers/cookies API,"Motivation:

We ported over a simpler API for HTTP headers and cookies from ServiceTalk, but we didn't put that API to use.
This PR rectifies this.

Modification:

- HTTP/1 has been changed to use the new headers API.
- HTTP/2 has also been changed to use the new headers API.
- The old HTTP/1 and HTTP/2 headers APIs have been removed, but the generic old headers API remain because it still has a number of niche uses.
- Tests have been updated.
- Two tests have been disabled; one appears to have a chaotic relationship with header validation, requiring it in some places but not in others, in the same pipeline; another relied on header iteration order being stable, to verify HPACK encoding against binary images.

Result:

Cleaner headers and cookies API for HTTP/1 and HTTP/2.

Draft for now because I think there's an issue with HPACK encoding, where some headers are supposed to be filtered out (!?) but aren't.
",2022-09-07T23:59:51Z,chrisvest,chrisvest,easier to read,"Hi there,
Do you have some performance numbers to share to compare the new Cookie API vs the old one wrt parsing and serialization? | I ran the HeaderBenchmark, but only for 30 headers to keep the run-time reasonable.
Will need to make a new benchmark for cookie parsing, since the API is so different.
Iterating HTTP/2 headers is worse. The rest is improved or on-par: | @slandelle I brought back and migrated ClientCookieDecoderBenchmark to this new API. With the old parsing code I was getting 2.690 ± 0.024  ops/us, and with a few adjustments I got the new code on par with 2.715 ± 0.208  ops/us. A bit more variability, though. | All comments addressed. Please take a look, guys. | @normanmaurer Can I get another review?","Work in progress new HttpHeaders and Cookies API | Remove old cookie API | Make HTTP/2 compile and tests pass | Fix checkstyle issues, and compilation in examples and benchmarks | Address first round of comments | Revert Http2Exception back to a checked exception | Address more review comments | Avoid allocating a processor for each delimiter when parsing cookies | Fix test failures | Bring back ClientCookieDecodeerBenchmark

And make small adjustments to DefaultHttpSetCookie to bring decoding performance back on par. | Remove unused code | Add more tests for set-cookie parsing | Fix compilation after rebase | Fix review comment"
netty/netty,12526,https://github.com/netty/netty/pull/12526,Add methods to override address inference in PcapWriteHandler,"Motivation:

The PcapWriteHandler has requires either a SocketChannel or a DatagramChannel to infer TCP / UDP connection details. This makes it useless for debugging an EmbeddedChannel.

Modification:

- Add `forceTcpChannel` and `forceUdpChannel` methods to allow users to bypass the channel metadata inference and provide their own information.
- Move handler initialization from `channelActive` into a separate method that is also called on any write or read. This ensures the new inference code runs even if channelActive isn't called. With the old inference code, this would have broken further down the line because `handlerAddr` and `initiatiorAddr` are null and the writer code can't deal with that, except the udp DatagramPacket branch. The change is necessary to maintain compatibility with that DatagramPacket branch, and to make use with EmbeddedChannel easier (EmbeddedChannel does not fire channelActive unless requested).
- Add tests for EmbeddedChannel based on the existing tests. This also changes the test case for normal udp/tcp a bit, because the test case mixed up the remote and local address (this was previously not an issue because both were 127.0.0.1).
- Improve error message for incompatible channel type, and elevate it to a warning. It should never appear with proper use of PcapWriteHandler.

Result:

PcapWriteHandler can now be used to debug test cases that use EmbeddedChannel.",2022-06-30T20:19:43Z,chrisvest,yawkat,easier to read,"oops, checkstyle. | /cc @hyperxpro | Let's Deprecate constructors in favor of Builder. | @yawkat For Netty 5, we moved these classes to a contrib repository: https://github.com/netty-contrib/handler-pcap - feel free to update that as well. | netty-contrib/handler-pcap#1","Add methods to override address inference in PcapWriteHandler
Motivation:
The PcapWriteHandler has requires either a SocketChannel or a DatagramChannel to infer TCP / UDP connection details. This makes it useless for debugging an EmbeddedChannel.

Modification:

- Add `forceTcpChannel` and `forceUdpChannel` methods to allow users to bypass the channel metadata inference and provide their own information.
- Move handler initialization from `channelActive` into a separate method that is also called on any write or read. This ensures the new inference code runs even if channelActive isn't called. With the old inference code, this would have broken further down the line because `handlerAddr` and `initiatiorAddr` are null and the writer code can't deal with that, except the udp DatagramPacket branch. The change is necessary to maintain compatibility with that DatagramPacket branch, and to make use with EmbeddedChannel easier (EmbeddedChannel does not fire channelActive unless requested).
- Add tests for EmbeddedChannel based on the existing tests. This also changes the test case for normal udp/tcp a bit, because the test case mixed up the remote and local address (this was previously not an issue because both were 127.0.0.1).
- Improve error message for incompatible channel type, and elevate it to a warning. It should never appear with proper use of PcapWriteHandler.

Result:
PcapWriteHandler can now be used to debug test cases that use EmbeddedChannel. | checkstyle | move to builder | private | deprecate old constructors"
netty/netty,11644,https://github.com/netty/netty/pull/11644,Include number of maximum active streams in exception message,"Motivation:

When users receive ""Maximum active streams violated for this endpoint""
exception, it's useful to know what is the current max streams limit on
HTTP/2 connection.

Modifications:

- Include current number of maximum active streams in exception message;

Result:

Easier debugging of HTTP/2 connections.",2021-09-03T06:53:36Z,normanmaurer,idelpivnitskiy,easier to read,,"Include number of maximum active streams in exception message

Motivation:

When users receive ""Maximum active streams violated for this endpoint""
exception, it's useful to know what is the current max streams limit on
HTTP/2 connection.

Modifications:

- Include current number of maximum active streams in exception message;

Result:

Easier debugging of HTTP/2 connections. | revert conditional"
netty/netty,11512,https://github.com/netty/netty/pull/11512,Add some size checks to make code more robust and more clear,"Motivation:

While its technical impossible that a chunk is larger than 64kb it still makes things easier to read and more robust to add some size checks to LzfDecoder.

Modifications:

Check the maximum length

Result:

More robust and easier to reason about code
",2021-07-26T15:11:19Z,normanmaurer,normanmaurer,easier to read,,"Add some size checks to make code more robust and more clear

Motivation:

While its technical impossible that a chunk is larger than 64kb it still makes things easier to read and more robust to add some size checks to LzfDecoder.

Modifications:

Check the maximum length

Result:

More robust and easier to reason about code"
netty/netty,11680,https://github.com/netty/netty/pull/11680,Fix ByteBufUtil indexOf ClassCastException,"Motivation:

In this issue(https://github.com/netty/netty/issues/11678) , it is proposed that `ByteBufUtil.indexOf()` may throw `ClassCastException`, so type checking on `haystack` is required

Modification:

Use `ByteBuf.indexOf()` instead of `firstIndexOf()`, and add test case

Result:

Fixes https://github.com/netty/netty/issues/11678

",2021-09-20T13:15:36Z,normanmaurer,skyguard1,easier to read,"The boringssl build got cancelled after 6 hours. Not clear why. | Maybe ci build can be triggered again? | Done
…
 Am 16.09.2021 um 03:48 schrieb skyguard1 ***@***.***>:

 ﻿
 Maybe ci build can be triggered again?

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub, or unsubscribe. | @skyguard1 thanks a lot! | @normanmaurer It is my pleasure","Fix ByteBufUtil indexOf ClassCastException

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | Update buffer/src/main/java/io/netty/buffer/ByteBufUtil.java

Co-authored-by: Aayush Atharva <hyperx.pro@outlook.com> | Update by review

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | Update by review

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | Use ByteBuf.indexOf() when haystack is not AbstractByteBuf

Signed-off-by: xingrufei <xingrufei@sogou-inc.com> | Use ByteBuf.indexOf() instead of firstIndexOf()

Signed-off-by: xingrufei <xingrufei@sogou-inc.com>"
netty/netty,11594,https://github.com/netty/netty/pull/11594,Future methods getNow() and cause() now throw on incomplete futures,"Motivation:
Since most futures in Netty are of the `Void` type, methods like `getNow()` and `cause()` cannot distinguish if the future has finished or not.
This can cause data race bugs which, in the case of `Void` futures, can be silent.

Modification:
The methods `getNow()` and `cause()` now throw an `IllegalStateException` if the future has not yet completed.
Most use of these methods are inside listeners, and so are not impacted.
One place in `AbstractBootstrap` was doing a racy read and has been adjusted.

Result:
Data race bugs around `getNow()` and `cause()` are no longer silent.",2021-08-24T13:47:27Z,chrisvest,chrisvest,easier to read,@chrisvest I only reviewed the last commit... This makes a lot of sense to me as discussed offline! | This makes sense to me. | huge +1 as discussed offline! | Converted to draft until #11575 is merged. | Rebased | Hopefully the build goes green now. | Makes sense. The old API was easier to make a mistake for sure. 👍,"Future methods getNow() and cause() now throw on incomplete futures

Motivation:
Since most futures in Netty are of the Void type, methods like getNow() and cause() cannot distinguish if the future has finished or not.
This can cause data race bugs which, in the case of Void futures, can be silent.

Modification:
The methods getNow() and cause() now throw an IllegalStateException if the future has not yet completed.
Most use of these methods are inside listeners, and so are not impacted.
One place in AbstractBootstrap was doing a racy read and has been adjusted.

Result:
Data race bugs around getNow() and cause() are no longer silent. | Add a convenient Future.isFailed to replace the `isDone() && cause() != null` pattern | Fix more racy tests"
netty/netty,11725,https://github.com/netty/netty/pull/11725,New leak detector for the new Buffer APIs,"This is still in a rough shape, but close enough for an initial review.",2021-10-12T14:44:21Z,chrisvest,chrisvest,easier to read,"Is there any performance penalty for tracking Buffer? | @hyperxpro Yeah, capturing the stack traces has some noticeable overhead. You can kind of get a feel for it wrt. the old leak detector by looking at the builds; some have leak detection enabled and some have not, and there's a non-trivial difference in the build times. I haven't measured the impact of this new leak detector yet but I expect it would be similar to the old one. | Is there any way to completely disable Buffer tracking? | Yes, it's off by default. | Thanks a lot! :) | @normanmaurer I think this is ready for another round of review. | 🥳","New leak detection for new buffer APIs | Clean up code and address review comments | Make uninstalling leak callbacks unable to throw any exceptions | Remove unnecessary static getAcquires bypass method | Fix data race between leak-producing threads and BufferAllocator.close in BufferLeakDetectionTest | Both cleanup and more aggressive garbage production in BufferLeakDetectionTest | Address more review comments | Avoid modifying LeakReport object while it is being constructed | Try making BufferLeakDetectionTest more robust | Make the leak detector robust to when the same callback instances are registered multiple times | Add some logging to help debug the Windows test failures | Improve the leak report logging message | Improve debugging of BufferLeakDetectionTest further | Only allocate one potentially leaked buffer per test | More info on failed tests, and fix a few drop ordering issues for PoolChunk | Leak detection also needs to work when a pool allocates unpooled memory | Remove logging leak callbacks since we're not debugging this test anymore | LeakInfo should talk about generic ""resources"", not buffers

Co-authored-by: Norman Maurer <norman_maurer@apple.com> | Address comment - make lifecycleTracingEnabled a constant | Small comment clarification | Fix NPE after disabling stack walking by default in LifecycleTracer"
netty/netty,11623,https://github.com/netty/netty/pull/11623,Add cascadeTo methods to Future,"Motivation:

The need of cascade from a Future to a Promise exists. We should add some default implementation for it.

Modifications:

- Merge PromiseNotifier into Futures
- Add default cascadeTo(...) methods to Future
- Add tests to FuturesTest
- Replace usage of PromiseNotifier with Future.cascadeTo
- Use combination of map(...) and cascadeTo(...) in *Bootstrap to reduce code duplication

Result:

Provide default implementation of cascadeTo.

",2021-08-29T13:44:34Z,normanmaurer,normanmaurer,easier to read,,"Add cascadeTo methods to Future

Motivation:

The need of cascade from a Future to a Promise exists. We should add some default implementation for it.

Modifications:

- Merge PromiseNotifier into Futures
- Add default cascadeTo(...) methods to Future
- Add tests to FuturesTest
- Replace usage of PromiseNotifier with Future.cascadeTo
- Use combination of map(...) and cascadeTo(...) in *Bootstrap to reduce code duplication

Result:

Provide default implementation of cascadeTo. | Address comments | Fix null check message"
netty/netty,11346,https://github.com/netty/netty/pull/11346,Don't take Promise as argument in Channel API.,"Motivation:

At the moment the outbound operations of ChannelHandler take a Promise as argument. This Promise needs to be carried forward to the next handler in the pipeline until it hits the transport. This is API choice has a few quirks which we should aim to remove:

 - There is a difference between if you add a FutureListener to the Promise or the Future that is returned by the outbound method in terms of the ordering of execution of the listeners. Sometimes we add the listener to the promise while in reality we usually always want to add it to the future to ensure the listerns are executed in the ""correct order"".
- It is quite easy to ""loose"" a promise by forgetting to use the right method which also takes a promise
- We have no idea what EventExecutor is used for the passed in Promise which may invalid our assumption of threading.

While changing the method signature of the outbound operations of the ChannelHandler is a good step forward we should also take care of just remove all the methods from ChannelOutboundInvoker (and its sub-types) that take a Promise and just always use the methods that return a Future only.

Modifications:

- Change the signature of the methods that took a Promise to not take one anymore and just return a Future
- Remove all operations for ChannelOutboundInvoker that take a Promise.
- Adjust all code to cope with the API changes

Result:

Cleaner API which is easier to reason about and easier to use.
",2021-08-25T12:12:33Z,normanmaurer,normanmaurer,easier to read,"I would be very interested to hear opinions related to this change... Especially from @ejona86 @bryce-anderson @mosesn @slandelle @Scottmitch @NiteshKant @vietj @violetagg | And @idelpivnitskiy | And @trustin | We could reduce some boiler-plate code by adding default methods to Future. Like:

    /**
     * Cascade the result of this {@link Future} to the given {@link Promise}s.
     * @param promises the promises to notify
     * @return itself.
     */
    default Future<V> cascadeTo(Promise<V>... promises) {
        return addListener(new PromiseNotifier<>(promises));
    }

    /**
     * Cascade the result of this {@link Future} to the given {@link Promise}s.
     * @param logNotifyFailure  {@code true} if logging should be done in case notification fails.
     * @param promises          the promises to notify
     * @return                  itself.
     */
    default Future<V> cascadeTo(boolean logNotifyFailure, Promise<V>... promises) {
        return addListener(new PromiseNotifier<>(logNotifyFailure, promises));
    } | Also please note that this one is currently aimed on the disable_master branch but I will re-aim it once #11344 is merged | I like this change all in all, but I think it's still worth adding throws Exception to the handler methods, because it'd be just simpler to let an exception is propagated to Netty's pipeline implementation so it creates a failed future for me. An exception can be raised regardless whether a handler method has it or not anyway.
Regarding Future.cascadeTo(), it's probably time to review our future API from scratch? There are quite a few nice future implementations out - JDK, Scala, Kotlin, etc. I find .map and .flatMap super useful when writing non-trivial asynchronous transformation for example. | @trustin i disagree with the throws :) imho a method that returns a Future should never have a throws ... | I'm sure throws Exception reduces quite a bit of annoyances when you deal with any exception throwing APIs, such as crypto or I/O API. It's also worth noting that exceptions are an inevitable part of the language we cannot prevent our users from throwing them. I insist adding it back. 😄 | @trustin still disagree but let's see what others think ;) Imho this is also a good thing as people will not do easily forget to release the message before returning an error | this is also a good thing as people will not do easily forget to release the message before returning an error

I'm not sure if this is relevant to throws Exception given that most leaks occur due to a missing finally block and an uncaught unchecked exception, not due to an uncaught checked exception. | I think it is as if there would not be a throws it would be harder for people to miss … let’s just wait for others to see what or other opinions here. Maybe it’s just me but whenever I see an API that returns a Future and also throws I can only thing of an API smell
…
 Am 01.06.2021 um 13:34 schrieb Trustin Lee ***@***.***>:

 ﻿
 this is also a good thing as people will not do easily forget to release the message before returning an error

 I'm not sure if this is relevant to throws Exception given that most leaks occur due to a missing finally block, not due to an uncaught checked exception.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub, or unsubscribe. | Regarding Future.cascadeTo(), it's probably time to review our future API from scratch? There are quite a few nice future implementations out - JDK, Scala, Kotlin, etc. I find .map and .flatMap super useful when writing non-trivial asynchronous transformation for example.

@trustin I agree... @chrisvest  will do some investigation here. | I'm in favor of this change. I poked at the changes a bit and didn't see anything surprising/annoying, but it was just a small sampling.
Regarding throws Exception, I do agree it would generally be better to return a failed future instead of throwing an exception, but the signature does make it clear that it is the responsibility of the pipeline to handle RuntimeExceptions. I feel like most exceptions are not explicit in a handler code but instead being thrown from random utilities/libraries and I don't want to litter ChannelHandlers with catch (RuntimeException)s due to fear of how it will be handled. Since these methods aren't called directly by arbitrary code (it is either called by the pipeline, or your handler calling itself), I don't see it as being all that bad to return a Future and throw as if you don't like it just don't use it in your own code.
I think I am most in favor of removing throws Exception and just putting clear documentation that runtime exceptions are handled by the pipeline, making it very clear there is a strong distinction between using ctx and calling methods directly. I see little reason for checked exceptions to be thrown instead of failing the future and that aligns better with ""Java style."" But I also don't think it matters much and would be fine either way. | There are two test-failures that I need to debug:
true)})) + Bootstrap(BootstrapConfig(group: MultithreadEventLoopGroup, channelFactory: ReflectiveChannelFactory(NioSocketChannel.class), options: {ALLOCATOR=PooledByteBufAllocator(directByDefault: true)}, resolver: io.netty.resolver.DefaultAddressResolverGroup@3f2ef586))) with PooledByteBufAllocator
2021-06-01T13:45:21.4455149Z [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.015 s - in io.netty.testsuite.transport.socket.SocketAutoReadTest
2021-06-01T13:45:21.8596004Z [INFO] 
2021-06-01T13:45:21.8596683Z [INFO] Results:
2021-06-01T13:45:21.8597109Z [INFO] 
2021-06-01T13:45:21.8597626Z [ERROR] Failures: 
2021-06-01T13:45:21.8599592Z [ERROR]   SocketCancelWriteTest.testCancelWrite:42->AbstractComboTestsuiteTest.run:52->testCancelWrite:98 expected: <UnpooledHeapByteBuf(ridx: 0, widx: 3, cap: 3/3)> but was: <UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeHeapByteBuf(ridx: 0, widx: 5, cap: 256)>
2021-06-01T13:45:21.8602607Z [ERROR]   SocketConnectionAttemptTest.testConnectCancellation:124->AbstractTestsuiteTest.run:49->testConnectCancellation:140 
2021-06-01T13:45:21.8603990Z Expected: is <true>
2021-06-01T13:45:21.8604430Z      but: was <false>
2021-06-01T13:45:21.8604827Z [INFO] 
2021-06-01T13:45:21.8605315Z [ERROR] Tests run: 374, Failures: 2, Errors: 0, Skipped: 72

That said please check the API changes in the meantime :) | Ok I know why this happened.... The problem is that basically Future.cancel(...) does not really cascade to the promises in all cases. This was also true before but now is just more visible. That said I think cancellation of IO is quite fragile anyway so I think it is ok and we should just ""relax"" the tests a bit. @chrisvest @ejona86 @NiteshKant WDYT ? | @trustin ^^ | I think cancellation of IO is quite fragile anyway so I think it is ok and we should just ""relax"" the tests a bit.

Cancellation could in principle be allowed up until the moment we do a system call. It's hard to compose, though. Imagine a future that is composed of multiple other futures (completion of the outer future implies completion of all inner futures). If the outer future is cancelled, it'll have to cancel all inner futures, but some could have already completed. And likewise, if an inner future is cancelled, it should propagate to the outer future, but the outer one might not be known, and even if it was, the cancellation would be just as racy as the first example.
As an interesting data point, Scala futures are not cancellable, but their promises are: https://viktorklang.com/blog/Futures-in-Scala-protips-6.html
That might be an example to follow. | As an interesting data point, Scala futures are not cancellable, but their promises are: viktorklang.com/blog/Futures-in-Scala-protips-6.html That might be an example to follow.

I'm not sure if it's a good idea to disallow a user from cancelling a future. I'm sure someone who requested I/O changes his or her mind later for many different reasons, including an end user clicking the cancel button or refreshing the page eventually being propagated to I/O layer. | Async methods should only return the result (success or error) via the returned async primitive(Future). Having different semantics just adds confusion for a user as the question would be if a ChannelHandler#write() can throw would channel.write() also throw?

Even if we don't have throws Exception on the handler the discrepancy here would remain. Unless you are suggesting that the pipeline should not catch RuntimeExceptions and so channel.write() should be able to throw unchecked exceptions.

Having a consistent messaging that any API that returns a Future MUST never throw reduces such confusion.

How is that enforced? Clearly the pipeline cannot prevent a Handler from throwing. So if a Handler does throw, how should the pipeline behave?

Internal netty code can be defensive by assuming runtime exceptions and converting to promise failures as we do today but that does not change the general guidance.

That is quite different than the ""MUST"" mentioned previously. With this behavior, applications can throw unchecked exceptions and nothing bad happens. It is a ""MUST NOT; or do. I'm a sign, not a cop"". We can put in our docs ""don't do that,"" but I think it would be obvious that applications would be relying on the catch-all behavior of the pipeline, either on purpose or accident.

I hear the argument that removing exceptions will potentially litter some handler code with try-catch blocks which will cause distraction but such is the cost of writing an async API in java.

I agree that as a callee you shouldn't be actively throwing. But I also don't want every handler to put a ""useless"" try-catch (RuntimeException) (or even worse, Error, as I don't trust it would always be handled correctly) in every handler method just comply with a MUST requirement not to throw that doesn't matter anyway because the pipeline is going to try-catch anyway.
When writing code with Netty initially I wondered how exceptions were handled. And throws Exception makes it very clear the caller handles exceptions. I didn't use throws Exception on my own methods (not that it matters, because I also didn't deal with checked exceptions).
I don't know. I'm just thrown off by the thought that removing throws Exception helps anything if no behavior actually changes. I really don't mind removing it, but the arguments for removing it seem too strong. The only argument I see is ""to encourage returning a failed future instead of throwing, which is the normal async way of things."" ""Encourage"" is very weak, yet I'm quite willing to remove throws Exception for it. But if we are removing it because we think we actually truly gain something, then I think we are lying to ourselves. | @ejona86 my suggestion is surely in the ""I am a sign; not a cop"" camp. From where I stand it is a guidance for handler implementations to use Future as the sole way to pass the result of the method.
There will be handlers who will not follow the guidance and we can perhaps log and warn when that happens. It will make the handlers that throw on purpose annoying enough to change and the handlers that throw by accident aware. We can perhaps also provide a Safe*Handler base classes that will give a less cumbersome way to try-catch their way through the change.

The only argument I see is ""to encourage returning a failed future instead of throwing, which is the normal async way of things."" ""Encourage"" is very weak, yet I'm quite willing to remove throws Exception for it. But if we are removing it because we think we actually truly gain something, then I think we are lying to ourselves.

IMHO clarifying expectations clearly and encouraging the same, is a gain. | Regarding cancellation:
I'm growing concerned by the number of changes in this PR especially if we also bring death-to-cancellation into it. Every feature we remove is a potential problem for migration, and this PR seems a very silly reason to remove cancellation. (Nothing in this PR changes the idea of whether cancellation is a necessary feature, and we hadn't previously talked of killing cancellation.) That said, it will take thought to accidentally nerf cancellation in this PR. If we need to kill cancellation, then it should probably be a separate PR that receives thorough review of its own and  that gets merged first.
It looks like PromiseNotifier is the primary source of non-propagating cancellation. Is that right? It looks like instead of using PromiseNotifier, we should be returning a custom Promise that can be informed later of the child future, in similar style to Guava's SettableFuture.setFuture(ListenableFuture). In principal, it could also be similar to PromiseCombiner (modulo threading differences), while although that class currently fails to propagate cancellations it has all the information necessary to do the propagation and would just be an extra for loop. That isn't really any more inefficient or painful compared to the PromiseNotifier solution from the code I looked at. Except...
I do think it opens up an important gap in cancellation handling: previously operations can be cancelled before they start, and thus never start, with no thought from middle-men code. So setFuture() by itself wouldn't be quite enough; the code must also check promise.isCancelled() before starting delegating work. Without that, I think the practical result of cancellation is the same as it being disabled, as any cancellation arrives after the operation is processed. The only possible exception is due to interplay with flush(), since that naturally delays the start of operations. But I don't believe flush() ""saves us"" from the issue; I expect we still have trouble if SslHandler is in play, for example.

Imagine a future that is composed of multiple other futures (completion of the outer future implies completion of all inner futures). If the outer future is cancelled, it'll have to cancel all inner futures, but some could have already completed.

Even if you have a single Future, cancellation could half-complete the operation. The cancellation API in Java is pretty horrible as even if the operation is atomic, you have no way to know if the operation completed or was cancelled if you request cancellation. But that has been true for years; it isn't something that changed as part of this PR.

And likewise, if an inner future is cancelled, it should propagate to the outer future, but the outer one might not be known, and even if it was, the cancellation would be just as racy as the first example.

I see this as more of an example of ""don't do that."" If you don't ""own"" a Future, don't go around cancelling it. Otherwise, this will be the least of the problems you'll introduce. I don't think there is any need to propagate cancellation ""outward"", other than propagating a failure outward.

Regarding throws Exception:

There will be handlers who will not follow the guidance and we can perhaps log and warn when that happens.

I agree that logging is a deterrent. It gets us into ""cop"" territory where the cop just slaps your wrist. Unfortunately I'm not wild about log+propagate approaches, as they tend to produce confusing results. Maybe with a well-worded message it could be made less confusing.
I do think going that route would mean we'd really need to audit the Netty APIs to make sure they use checked exceptions for remotely-triggerable exceptions. It would also mean that users should audit their own code base. And you are simply going to have continued bugs if using a library (e.g., the JDK) that ""chooses poorly"" between checked and unchecked exceptions (based on what we define ""best practices"", which actually differs from other parties' beliefs)... unless you duplicate the try-catch which is exactly the behavior we have today.

We can perhaps also provide a Safe*Handler base classes that will give a less cumbersome way to try-catch their way through the change.

I doubt it. In the past when I've seen such things they were more confusing to use than just doing without. OO is pretty poor for many ""base class calls extending class"" use cases (and those that it isn't poor at could be solved well by composition). We could use composition to wrap a Handler into a SafeHandler which would be trivial and easy, but in no way do I think the world would be a better place because of it. There's no real harm to a handler throwing an exception today and a handler can trivially ""opt-out"" of throwing checked exceptions if it doesn't like the throws Exception, so I feel like this change would be mostly to give ourselves warm fuzzies that things ""feel clean"" and not as much to fix a problem (e.g., that causes bugs, performance issues, etc).
I'm fine removing throws Exception (or keeping it), but I'm currently against the idea of logging+SafeHandler approach simply because it adds pain for no perceivable benefit to users that I can tell. It seems handler behavior is right or wrong and if using SafeHandler converts wrong behavior to right then what harm occurs to use the SafeHandler behavior all the time (i.e., the current behavior)? | if we also bring death-to-cancellation into it.

+1, I don't think we should take this PR into a remove/unsupport cancellation territory.

I don't think there is any need to propagate cancellation ""outward""

+1

I'm fine removing throws Exception (or keeping it), but I'm currently against the idea of logging+SafeHandler approach simply because it adds pain for no perceivable benefit to users that I can tell.

Fair enough, looks like removing throws Exception is the minimum most of us are agreeing on so seems like a good place to stop for this PR. | @ejona86 @trustin @NiteshKant @chrisvest See #11378 for another approach. I tried to limit the changes in #11378 to make it easier to discuss the idea. | Removed the change to disallow cancellation | Rebased this one... | I like this PR because it greatly simplifies the API. The number of moving parts, and the number of method overloads are decreased. The drawback is that every operation now allocates at least one Future/Promise object, and this allocation can hurt performance quite a lot in high-load scenarios. The #11378 PR tries to fix that, but from an API perspective it mostly just replace the Promise with a callback, and the API isn't that much simpler.
What I notice in this PR is that, since we no longer have any handler/context methods that take a promise, we are assured that promises only move up the pipeline. In the reverse direction of the call stack:
handler -> handler -> handler \
                           *newPromise(): Allocation*
                              /
future  <- future  <----------

By default, we want these promises and futures to be fully featured, which requires allocation. However, we also want some kind of opt-in optimisation that allow us to skip the allocation when we know it's not needed. This optimisation also needs to compose cleanly, so if some handlers don't need any return values, cancellation or listeners on the future, but some other handlers do, and we combine these two in a pipeline, then we should get fully featured promise/future objects. And we'd also like to not be able to break the expectations of handlers, like is currently possible with passing a void promise to a handler that needs a real promise.
There is precedent for a solution to a similar problem already in the code base. The @Skip annotation can bypass layers of handlers down the pipeline, if they claim that all they do is just delegate anyway.
We could add another annotation that modify the behaviour of ChannelHandlerContext.newPromise such that if the current, and all prior handlers in the pipeline, claim they don't need any features of the promise or the future, then we just return a constant - the equivalent of a void promise:
@NoFut     @NoFut     @NoFut
handler -> handler -> handler \
                           *newPromise(): voidPromise*
                              /
future  <- future  <----------

That would keep the API simple; the futures will work by default; you can't mistakenly use a void promise if you needed a real one (not sure how big this problem is in practice, but whatever); handlers will compose without breaking on an optimisation; but an optimisation will exist for advanced use cases and people who know what they're doing; and even if people mess up, things will still work, just slower.
@normanmaurer What do you think? | One thing I don't have a handy answer for is how to deal with calls via Bootstrap and Channel from outside the event loop and pipeline.
Maybe it's not a big problem if these always allocate real promise/future objects. Otherwise we could perhaps have a state on the channel or pipeline that the next call don't need a real future, but only expose this outside of the pipeline, for channel interactions that are driven externally. | @chrisvest Meta opinion: annotations indicating the implementation traits of a method usually scares me as they tend to drift over time.
Specifically for this proposal; will this annotation be at the handler or for each method?  If at the handler, how do we handle cases where close() needs a real promise but write does not? If for each method, is this state stored at the pipeline level? | @NiteshKant It would be a method annotation, similar to @Skip. | So I rebased this one on the latest changes and I think after thinking more about it I think this is the way to go. While this also means we loose the ability to reduce allocations (like we did with VoidPromise) I think the win in terms of API cleanup and guards against common problems outweights the downsides a lot.
So to make it short I am in favor to merge this and not care about my other proposed alternatives | Overall I like this change and how it cleans up the API. There's a lot of promise notifiers around, but hopefully we can make that look nicer with some convenient methods on Future. Had some comments that needs to be looked at.

Yes... I didn't want to mix this into this PR but I think the cascade methods should be merged into Future and provided as default impls. This should remove some ""boilerplate"" code | Rebased... | I'm in favour of async code not throwing an exception, the future returned should be handled correctly by the caller. | And merged.. Thanks for all the feedback! | I like this changes, the only one thing what we lost in performance without ability to pass a VoidPromise? Giant work done, @normanmaurer! | @amizurov I am still thunking about if we can do and want do for this.","Don't take ChannelPromise as argument in Channel API.

Motivation:

At the moment the outbound operations of ChannelHandler take a ChannelPromise as argument. This ChannelPromise needs to be carried forward to the next handler in the pipeline until it hits the transport. This is API choice has a few quirks which we should aim to remove:

 - There is a difference between if you add a ChannelFutureListener to the ChannelPromise or the ChannelFuture that is returned by the outbound method in terms of the ordering of execution of the listeners. Sometimes we add the listener to the promise while in reality we usually always want to add it to the future to ensure the listerns are executed in the ""correct order"".
- It is quite easy to ""loose"" a promise by forgetting to use the right method which also takes a promise
- Outbound operations of a ChannelHandler currently have a signature which allws these to throw an Exception. This is odd as we really should fail the promise / future to signal an exception.
- We need to explicit handle the VoidChannelPromise
- We have no idea what EventExecutor is used for the passed in ChannelPromise which may invalid our assumption of threading.

While changing the method signature of the outbound operations of the ChannelHandler is a good step forward we should also take care of just remove all the methods from ChannelOutboundInvoker (and its sub-types) that take a ChannelPromise and just always use the methods that return a ChannelFuture only.

Modifications:

- Remove throws Exception from the outbound operations of ChannelHandler
- Change the signature of the methods that took a ChannelPromise to not take one anymore and just return a ChannelFuture
- Remove all operations for ChannelOutboundInvoker that take a ChannelPromise.
- Adjust all code to cope with the API changes

Result:

Cleaner API which is easier to reason about and easier to use. | Address comments of chris | more comments | Address comments of nitesh"
